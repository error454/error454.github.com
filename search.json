{"entries":[{"title":404,"baseurl":"","url":"/404.html","date":null,"categories":[],"body":""},{"title":"About Me","baseurl":"","url":"/about.html","date":null,"categories":[],"body":" Name: Zachary Burke Email: zachary.burke@gmail.com Github: error454 Flickr: flickr.com/error454 YouTube: www.youtube.com/user/Error454 "},{"title":"Archive","baseurl":"","url":"/archive.html","date":null,"categories":[],"body":""},{"title":"Categories","baseurl":"","url":"/categories.html","date":null,"categories":[],"body":""},{"title":null,"baseurl":"","url":"/index.html","date":null,"categories":[],"body":" error454 The Blog of a Compulsive Creator, Game Developer and Photographer. Load More… "},{"title":"Tags","baseurl":"","url":"/tags.html","date":null,"categories":[],"body":" ares 2 webos 17 palm 6 pdk 6 pixi 2 pre 4 sdl 4 event 1 input 3 collision detection 1 sdl_blitsurface 1 sdl_fillrect 1 ffmpeg 4 ffplay 1 ffserver 1 hulu 1 rtsp 1 sdl_hwsurface 1 vlc 4 360 1 driver 1 linux 2 mass 1 storage 2 xbox 3 mass storage 4 usb 4 xbox 360 2 android 9 droid 2 droid2 1 evo 1 sd 1 xbox360 1 example 1 tutorial 3 view menu 1 viewmenu 1 widget 1 palm pre 1 proc 1 ps3 1 sysfs 1 wii 1 camera 3 flash 1 led 1 light sensor 1 torch 1 3d 4 art 1 daz 1 graphics 1 icons 1 mobile 1 vue 1 engine 2 shiva 28 unity 1 unreal 2 benchmark 1 vertex count 1 fail 1 hp 2 framework 1 homebrew 4 js 1 node 2 node.js 1 root 1 service 1 file_storage.c 1 hacking 1 kernel 1 2d 3 effect 1 laser 1 lazer 1 shiva 3d 2 shiva3d 9 angry satellites 2 diary 1 games 1 indie 2 ai 1 avoid 1 dynamics 3 flee 1 navigation 1 offsetpursuit 1 pursuit 1 seek 1 translation 2 2d in 3d 1 fit objects 1 shake 1 burst 1 patch 1 timer 1 fps 1 onSensorCollision 1 onSensorCollisionBegin 1 performance 2 sensors 2 audio 1 design 2 openal 1 pools 1 slowdown 1 texture 1 textures 1 game 1 screen boundary 1 avi 3 mkv 3 mplayer 3 draw_frame 1 draw_slice 1 fragment shader 1 shader 1 stride 1 yuv 1 flv 1 touchpad 1 touchplayer 1 childhood 1 geek 1 hex editing 1 learning 1 retrospect 1 tutorials 1 amazon 1 app store 1 apple 1 appstore 1 comments 1 developer 1 george washington 1 marketplace 1 negative 1 richard feynman 1 segregated 1 uninstall 1 wtf 1 C++ 1 java 1 native 1 lighting 1 particles 1 ball joint 1 cradle 1 joints 1 newton 1 extensions 1 sdk 2 smart 1 smartextensions 1 smartwatch 1 sony 1 watch 1 jumpman 1 platform 2 platformer 4 poc 2 arabic 1 disappear 1 listview 1 joystick 2 keyboard 1 keyboard handler 1 all in one 1 open source 1 play market 1 retrospective 1 wallpaper 1 hud 1 gravity 1 jump 1 jump termination 1 physics 1 directed tests 1 ouya 2 perfhud 2 tegra 2 gpu 1 flux 1 gauge 1 gsm 1 jtag 1 jtaging 1 lift 1 pad 1 pins 1 solder 1 stormtrooper 1 torx 1 wire 1 api 1 changes 1 obfuscation 1 proguard 1 aiming 1 animation 1 quicksort 1 gamestorming 1 note 3 1 samsung 1 template 1 csv 1 i18n 1 localization 1 lua 1 parser 1 spreadsheet 1 xml 1 packaging 2 packs 1 stk 1 cloth 1 debug 1 eRIGID_STATIC 1 fetchResults 1 filter shader 1 jitter 1 nvidia 1 physx 1 PxClothMeshDesc 1 PxFilterFlags 1 PxGetFilterObjectType 1 PxPairFlag 1 rigid body physics 1 simulate 1 simulation 1 sub-stepping 1 UE4 1 editor 1 ue4 7 c++ 3 hue 1 philips 1 python 1 analysis 2 source control 2 repo 1 perforce 2 azure 1 git 1 philanthropy 1 gamedev 1 barbie 1 nebula 1 star 1 perlin 1 simplex 1 material 1 matinee 1 anim 1 bp 1 network 1 replication 1 character movement 1 tv 1 mr robot 1 misc 1 astsu 1 astu 1 elpscrk 1 chmod 1 gamepad 1 controller 1 mapping 1 geometry wars 1 pacman 1 child of light 1 assault android cactus 1 tomb raider 1 lara croft 1 ares An Ares View Menu Example for WebOS The 10-minute guide to Ares for WebOS webos TouchPlayer Documentation Note to self: Port mplayer to Touchpad Play mkv, avi, flv and wmv files on webOS Touchpad WebOS Camera Patch: Advanced Camera Configuration Lab Notes Hacking file_storage.c Using the Homebrew Javascript Service Framework Stories of Warning for HP (Palm) Shiva 3D for Mobile Games An Ares View Menu Example for WebOS Using the Palm Pre as Xbox 360 Storage (the finale) Using the Palm Pre as Xbox 360 Storage (part 2) Lab Notes: Compiling ffmpeg for WebOS Collision Detection for WebOS SDL/PDK Input The Event Loop for WebOS SDL/PDK Drawing Images for WebOS SDL/PDK Barebones SDL For WebOS PDK The 10-minute guide to Ares for WebOS palm Stories of Warning for HP (Palm) Shiva 3D Benchmarking for Mobile Performance Using the Palm Pre as Xbox 360 storage Input The Event Loop for WebOS SDL/PDK Drawing Images for WebOS SDL/PDK Barebones SDL For WebOS PDK pdk Using the Palm Pre as Xbox 360 Storage (part 2) Lab Notes: Compiling ffmpeg for WebOS Collision Detection for WebOS SDL/PDK Input The Event Loop for WebOS SDL/PDK Drawing Images for WebOS SDL/PDK Barebones SDL For WebOS PDK pixi Drawing Images for WebOS SDL/PDK Barebones SDL For WebOS PDK pre Using the Palm Pre as Xbox 360 storage Input The Event Loop for WebOS SDL/PDK Drawing Images for WebOS SDL/PDK Barebones SDL For WebOS PDK sdl Collision Detection for WebOS SDL/PDK Input The Event Loop for WebOS SDL/PDK Drawing Images for WebOS SDL/PDK Barebones SDL For WebOS PDK event Input The Event Loop for WebOS SDL/PDK input The 5 Golden Rules of Input ShiVa3D Flexible Keyboard/Joystick Input Architecture Input The Event Loop for WebOS SDL/PDK collision detection Collision Detection for WebOS SDL/PDK sdl_blitsurface Collision Detection for WebOS SDL/PDK sdl_fillrect Collision Detection for WebOS SDL/PDK ffmpeg TouchPlayer Documentation Note to self: Port mplayer to Touchpad Play mkv, avi, flv and wmv files on webOS Touchpad Lab Notes: Compiling ffmpeg for WebOS ffplay Lab Notes: Compiling ffmpeg for WebOS ffserver Lab Notes: Compiling ffmpeg for WebOS hulu Lab Notes: Compiling ffmpeg for WebOS rtsp Lab Notes: Compiling ffmpeg for WebOS sdl_hwsurface Lab Notes: Compiling ffmpeg for WebOS vlc TouchPlayer Documentation Note to self: Port mplayer to Touchpad Play mkv, avi, flv and wmv files on webOS Touchpad Lab Notes: Compiling ffmpeg for WebOS 360 Using the Palm Pre as Xbox 360 storage driver Using the Palm Pre as Xbox 360 storage linux Setting up Perforce on Azure for UE4 Using the Palm Pre as Xbox 360 storage mass Using the Palm Pre as Xbox 360 storage storage Using the Palm Pre as Xbox 360 Storage (the finale) Using the Palm Pre as Xbox 360 storage xbox The Path of Most Resistance Using the Palm Pre as Xbox 360 Storage (the finale) Using the Palm Pre as Xbox 360 storage mass storage Lab Notes Hacking file_storage.c Stories of Warning for HP (Palm) Using the Palm Pre as Xbox 360 Storage (the finale) Using the Palm Pre as Xbox 360 Storage (part 2) usb Stories of Warning for HP (Palm) The Path of Most Resistance Using the Palm Pre as Xbox 360 Storage (the finale) Using the Palm Pre as Xbox 360 Storage (part 2) xbox 360 Using the Palm Pre as Xbox 360 Storage (the finale) Using the Palm Pre as Xbox 360 Storage (part 2) android Stormtrooper's Guide to Droid JTAG'ing Jelly Bean LWP Retrospective Introducing ShiVa Android All In One (AAIO) Android ListView Contents Disappearing Sony Smartwatch SDK Impressions ShiVa Native Projects for Android A Guide for the Forgetful Why App Stores SUCK Segregation Android Torch LED Using the Palm Pre as Xbox 360 Storage (the finale) droid Stormtrooper's Guide to Droid JTAG'ing Using the Palm Pre as Xbox 360 Storage (the finale) droid2 Using the Palm Pre as Xbox 360 Storage (the finale) evo Using the Palm Pre as Xbox 360 Storage (the finale) sd Using the Palm Pre as Xbox 360 Storage (the finale) xbox360 Using the Palm Pre as Xbox 360 Storage (the finale) example An Ares View Menu Example for WebOS tutorial Stormtrooper's Guide to Droid JTAG'ing ShiVa PoC: Box Particle Lighting An Ares View Menu Example for WebOS view menu An Ares View Menu Example for WebOS viewmenu An Ares View Menu Example for WebOS widget An Ares View Menu Example for WebOS palm pre The Path of Most Resistance proc The Path of Most Resistance ps3 The Path of Most Resistance sysfs The Path of Most Resistance wii The Path of Most Resistance camera WebOS Camera Patch: Advanced Camera Configuration Design Diary: Shiva 3D Camera Implementation for 2D in 3D Android Torch LED flash Android Torch LED led Android Torch LED light sensor Android Torch LED torch Android Torch LED 3d Using STK Packs in ShiVa 3D Platformer Character Animation Notes Design Diary Shiva3D Screen Boundary Detection in 2 Steps How To Make Graphics Without Being An Artist art How To Make Graphics Without Being An Artist daz How To Make Graphics Without Being An Artist graphics How To Make Graphics Without Being An Artist icons How To Make Graphics Without Being An Artist mobile How To Make Graphics Without Being An Artist vue How To Make Graphics Without Being An Artist engine Packaging Unreal Engine Editor Shiva 3D for Mobile Games shiva 11 Random ShiVa Projects PhysX 3.3.1 Integration Notebook Using STK Packs in ShiVa ShiVa Localization Quicksort for ShiVa3D 3D Platformer Character Animation Notes Obfuscating ShiVa Games with Proguard ShiVa 1.9.2 New API Calls Make Your Ouya Games Run at 60 FPS with This Sneaky Trick! Profiling OUYA (Tegra 3) Games using Nvidia PerfHUD ES Platformer Physics 101 and The 3 Fundamental Equations of Platformers ShiVa3D HUD Interpolators Test Jelly Bean LWP Retrospective Introducing ShiVa Android All In One (AAIO) ShiVa3D Flexible Keyboard/Joystick Input Architecture ShiVa PoC: 2D Platformer ShiVa PoC: Newtons Cradle ShiVa PoC: Box Particle Lighting ShiVa Native Projects for Android A Guide for the Forgetful Design Diary Shiva3D Screen Boundary Detection in 2 Steps Design Diary Shiva 3D Mobile Optimization Design Diary Shiva 3D Sensor Performance Design Diary: Shiva 3D Camera Implementation for 2D in 3D Designer Diary: Shiva AI Navigation Dynamics vs Translation Diary of an Indie Game Developer ShiVa 3D Making a 2D Laser Shiva 3D Benchmarking for Mobile Performance Shiva 3D for Mobile Games unity Shiva 3D for Mobile Games unreal Packaging Unreal Engine Editor Shiva 3D for Mobile Games benchmark Shiva 3D Benchmarking for Mobile Performance vertex count Shiva 3D Benchmarking for Mobile Performance fail Stories of Warning for HP (Palm) hp Why App Stores SUCK Segregation Stories of Warning for HP (Palm) framework Using the Homebrew Javascript Service Framework homebrew TouchPlayer Documentation Note to self: Port mplayer to Touchpad Play mkv, avi, flv and wmv files on webOS Touchpad Using the Homebrew Javascript Service Framework js Using the Homebrew Javascript Service Framework node Geeking Out with Philips Hue Using the Homebrew Javascript Service Framework node.js Using the Homebrew Javascript Service Framework root Using the Homebrew Javascript Service Framework service Using the Homebrew Javascript Service Framework file_storage.c Lab Notes Hacking file_storage.c hacking Lab Notes Hacking file_storage.c kernel Lab Notes Hacking file_storage.c 2d ShiVa PoC: 2D Platformer Design Diary Shiva3D Screen Boundary Detection in 2 Steps ShiVa 3D Making a 2D Laser effect ShiVa 3D Making a 2D Laser laser ShiVa 3D Making a 2D Laser lazer ShiVa 3D Making a 2D Laser shiva 3d Design Diary: Shiva 3D Camera Implementation for 2D in 3D ShiVa 3D Making a 2D Laser shiva3d Using STK Packs in ShiVa Jelly Bean LWP Retrospective Design Diary Shiva3D Screen Boundary Detection in 2 Steps Design Diary Shiva 3D Mobile Optimization Design Diary Shiva 3D Sensor Performance Design Diary: Shiva 3D Camera Implementation for 2D in 3D Designer Diary: Shiva AI Navigation Dynamics vs Translation Diary of an Indie Game Developer ShiVa 3D Making a 2D Laser angry satellites Design Diary: Shiva 3D Camera Implementation for 2D in 3D Diary of an Indie Game Developer diary Diary of an Indie Game Developer games Diary of an Indie Game Developer indie Design Diary Shiva3D Screen Boundary Detection in 2 Steps Diary of an Indie Game Developer ai Designer Diary: Shiva AI Navigation Dynamics vs Translation avoid Designer Diary: Shiva AI Navigation Dynamics vs Translation dynamics ShiVa PoC: Newtons Cradle ShiVa PoC: Box Particle Lighting Designer Diary: Shiva AI Navigation Dynamics vs Translation flee Designer Diary: Shiva AI Navigation Dynamics vs Translation navigation Designer Diary: Shiva AI Navigation Dynamics vs Translation offsetpursuit Designer Diary: Shiva AI Navigation Dynamics vs Translation pursuit Designer Diary: Shiva AI Navigation Dynamics vs Translation seek Designer Diary: Shiva AI Navigation Dynamics vs Translation translation ShiVa Localization Designer Diary: Shiva AI Navigation Dynamics vs Translation 2d in 3d Design Diary: Shiva 3D Camera Implementation for 2D in 3D fit objects Design Diary: Shiva 3D Camera Implementation for 2D in 3D shake Design Diary: Shiva 3D Camera Implementation for 2D in 3D burst WebOS Camera Patch: Advanced Camera Configuration patch WebOS Camera Patch: Advanced Camera Configuration timer WebOS Camera Patch: Advanced Camera Configuration fps Design Diary Shiva 3D Sensor Performance onSensorCollision Design Diary Shiva 3D Sensor Performance onSensorCollisionBegin Design Diary Shiva 3D Sensor Performance performance Design Diary Shiva 3D Mobile Optimization Design Diary Shiva 3D Sensor Performance sensors Design Diary Shiva 3D Mobile Optimization Design Diary Shiva 3D Sensor Performance audio Design Diary Shiva 3D Mobile Optimization design Design Diary Shiva3D Screen Boundary Detection in 2 Steps Design Diary Shiva 3D Mobile Optimization openal Design Diary Shiva 3D Mobile Optimization pools Design Diary Shiva 3D Mobile Optimization slowdown Design Diary Shiva 3D Mobile Optimization texture Design Diary Shiva 3D Mobile Optimization textures Design Diary Shiva 3D Mobile Optimization game Design Diary Shiva3D Screen Boundary Detection in 2 Steps screen boundary Design Diary Shiva3D Screen Boundary Detection in 2 Steps avi TouchPlayer Documentation Note to self: Port mplayer to Touchpad Play mkv, avi, flv and wmv files on webOS Touchpad mkv TouchPlayer Documentation Note to self: Port mplayer to Touchpad Play mkv, avi, flv and wmv files on webOS Touchpad mplayer TouchPlayer Documentation Note to self: Port mplayer to Touchpad Play mkv, avi, flv and wmv files on webOS Touchpad draw_frame Note to self: Port mplayer to Touchpad draw_slice Note to self: Port mplayer to Touchpad fragment shader Note to self: Port mplayer to Touchpad shader Note to self: Port mplayer to Touchpad stride Note to self: Port mplayer to Touchpad yuv Note to self: Port mplayer to Touchpad flv TouchPlayer Documentation touchpad TouchPlayer Documentation touchplayer TouchPlayer Documentation childhood Promote Cheating for a Better Future (a retrospect of a geeky childhood) geek Promote Cheating for a Better Future (a retrospect of a geeky childhood) hex editing Promote Cheating for a Better Future (a retrospect of a geeky childhood) learning Promote Cheating for a Better Future (a retrospect of a geeky childhood) retrospect Promote Cheating for a Better Future (a retrospect of a geeky childhood) tutorials Promote Cheating for a Better Future (a retrospect of a geeky childhood) amazon Why App Stores SUCK Segregation app store Why App Stores SUCK Segregation apple Why App Stores SUCK Segregation appstore Why App Stores SUCK Segregation comments Why App Stores SUCK Segregation developer Why App Stores SUCK Segregation george washington Why App Stores SUCK Segregation marketplace Why App Stores SUCK Segregation negative Why App Stores SUCK Segregation richard feynman Why App Stores SUCK Segregation segregated Why App Stores SUCK Segregation uninstall Why App Stores SUCK Segregation wtf Why App Stores SUCK Segregation C++ ShiVa Native Projects for Android A Guide for the Forgetful java ShiVa Native Projects for Android A Guide for the Forgetful native ShiVa Native Projects for Android A Guide for the Forgetful lighting ShiVa PoC: Box Particle Lighting particles ShiVa PoC: Box Particle Lighting ball joint ShiVa PoC: Newtons Cradle cradle ShiVa PoC: Newtons Cradle joints ShiVa PoC: Newtons Cradle newton ShiVa PoC: Newtons Cradle extensions Sony Smartwatch SDK Impressions sdk Introducing ShiVa Android All In One (AAIO) Sony Smartwatch SDK Impressions smart Sony Smartwatch SDK Impressions smartextensions Sony Smartwatch SDK Impressions smartwatch Sony Smartwatch SDK Impressions sony Sony Smartwatch SDK Impressions watch Sony Smartwatch SDK Impressions jumpman ShiVa PoC: 2D Platformer platform Platformer Physics 101 and The 3 Fundamental Equations of Platformers ShiVa PoC: 2D Platformer platformer UE4 Platformer Game Analysis 3D Platformer Character Animation Notes Platformer Physics 101 and The 3 Fundamental Equations of Platformers ShiVa PoC: 2D Platformer poc 11 Random ShiVa Projects ShiVa PoC: 2D Platformer arabic Android ListView Contents Disappearing disappear Android ListView Contents Disappearing listview Android ListView Contents Disappearing joystick The 5 Golden Rules of Input ShiVa3D Flexible Keyboard/Joystick Input Architecture keyboard ShiVa3D Flexible Keyboard/Joystick Input Architecture keyboard handler ShiVa3D Flexible Keyboard/Joystick Input Architecture all in one Introducing ShiVa Android All In One (AAIO) open source Introducing ShiVa Android All In One (AAIO) play market Jelly Bean LWP Retrospective retrospective Jelly Bean LWP Retrospective wallpaper Jelly Bean LWP Retrospective hud ShiVa3D HUD Interpolators Test gravity Platformer Physics 101 and The 3 Fundamental Equations of Platformers jump Platformer Physics 101 and The 3 Fundamental Equations of Platformers jump termination Platformer Physics 101 and The 3 Fundamental Equations of Platformers physics Platformer Physics 101 and The 3 Fundamental Equations of Platformers directed tests Profiling OUYA (Tegra 3) Games using Nvidia PerfHUD ES ouya Make Your Ouya Games Run at 60 FPS with This Sneaky Trick! Profiling OUYA (Tegra 3) Games using Nvidia PerfHUD ES perfhud Make Your Ouya Games Run at 60 FPS with This Sneaky Trick! Profiling OUYA (Tegra 3) Games using Nvidia PerfHUD ES tegra Make Your Ouya Games Run at 60 FPS with This Sneaky Trick! Profiling OUYA (Tegra 3) Games using Nvidia PerfHUD ES gpu Make Your Ouya Games Run at 60 FPS with This Sneaky Trick! flux Stormtrooper's Guide to Droid JTAG'ing gauge Stormtrooper's Guide to Droid JTAG'ing gsm Stormtrooper's Guide to Droid JTAG'ing jtag Stormtrooper's Guide to Droid JTAG'ing jtaging Stormtrooper's Guide to Droid JTAG'ing lift Stormtrooper's Guide to Droid JTAG'ing pad Stormtrooper's Guide to Droid JTAG'ing pins Stormtrooper's Guide to Droid JTAG'ing solder Stormtrooper's Guide to Droid JTAG'ing stormtrooper Stormtrooper's Guide to Droid JTAG'ing torx Stormtrooper's Guide to Droid JTAG'ing wire Stormtrooper's Guide to Droid JTAG'ing api ShiVa 1.9.2 New API Calls changes ShiVa 1.9.2 New API Calls obfuscation Obfuscating ShiVa Games with Proguard proguard Obfuscating ShiVa Games with Proguard aiming 3D Platformer Character Animation Notes animation 3D Platformer Character Animation Notes quicksort Quicksort for ShiVa3D gamestorming Game Design Template for Galaxy Note note 3 Game Design Template for Galaxy Note samsung Game Design Template for Galaxy Note template Game Design Template for Galaxy Note csv ShiVa Localization i18n ShiVa Localization localization ShiVa Localization lua ShiVa Localization parser ShiVa Localization spreadsheet ShiVa Localization xml ShiVa Localization packaging Packaging Unreal Engine Editor Using STK Packs in ShiVa packs Using STK Packs in ShiVa stk Using STK Packs in ShiVa cloth PhysX 3.3.1 Integration Notebook debug PhysX 3.3.1 Integration Notebook eRIGID_STATIC PhysX 3.3.1 Integration Notebook fetchResults PhysX 3.3.1 Integration Notebook filter shader PhysX 3.3.1 Integration Notebook jitter PhysX 3.3.1 Integration Notebook nvidia PhysX 3.3.1 Integration Notebook physx PhysX 3.3.1 Integration Notebook PxClothMeshDesc PhysX 3.3.1 Integration Notebook PxFilterFlags PhysX 3.3.1 Integration Notebook PxGetFilterObjectType PhysX 3.3.1 Integration Notebook PxPairFlag PhysX 3.3.1 Integration Notebook rigid body physics PhysX 3.3.1 Integration Notebook simulate PhysX 3.3.1 Integration Notebook simulation PhysX 3.3.1 Integration Notebook sub-stepping PhysX 3.3.1 Integration Notebook UE4 Packaging Unreal Engine Editor editor Packaging Unreal Engine Editor ue4 Character Movement Replication in UE4 Using Animation Blueprints with Matinee Nebulas, Stars, Random Numbers and UE4 How to use Perforce in UE4 Setting up Perforce on Azure for UE4 UE4 Platformer Game Analysis 1,000,000 Stupid Questions for UE4 Developers c++ Character Movement Replication in UE4 UE4 Platformer Game Analysis 1,000,000 Stupid Questions for UE4 Developers hue Geeking Out with Philips Hue philips Geeking Out with Philips Hue python Geeking Out with Philips Hue analysis Mr. Robot Episode 1 Analysis UE4 Platformer Game Analysis source control How to use Perforce in UE4 Setting up Perforce on Azure for UE4 repo Setting up Perforce on Azure for UE4 perforce How to use Perforce in UE4 Setting up Perforce on Azure for UE4 azure Setting up Perforce on Azure for UE4 git Setting up Perforce on Azure for UE4 philanthropy 11 Random ShiVa Projects gamedev GameDev Barbie barbie GameDev Barbie nebula Nebulas, Stars, Random Numbers and UE4 star Nebulas, Stars, Random Numbers and UE4 perlin Nebulas, Stars, Random Numbers and UE4 simplex Nebulas, Stars, Random Numbers and UE4 material Nebulas, Stars, Random Numbers and UE4 matinee Using Animation Blueprints with Matinee anim Using Animation Blueprints with Matinee bp Using Animation Blueprints with Matinee network Character Movement Replication in UE4 replication Character Movement Replication in UE4 character movement Character Movement Replication in UE4 tv Mr. Robot Episode 1 Analysis mr robot Mr. Robot Episode 1 Analysis misc Mr. Robot Episode 1 Analysis astsu Mr. Robot Episode 1 Analysis astu Mr. Robot Episode 1 Analysis elpscrk Mr. Robot Episode 1 Analysis chmod Mr. Robot Episode 1 Analysis gamepad The 5 Golden Rules of Input controller The 5 Golden Rules of Input mapping The 5 Golden Rules of Input geometry wars The 5 Golden Rules of Input pacman The 5 Golden Rules of Input child of light The 5 Golden Rules of Input assault android cactus The 5 Golden Rules of Input tomb raider The 5 Golden Rules of Input lara croft The 5 Golden Rules of Input "},{"title":"The 10-minute guide to Ares for WebOS","baseurl":"","url":"/2010/08/06/the-10-minute-guide-to-ares-for-webos/","date":"2010-08-06 00:00:00 -0700","categories":["webos"],"body":"This is the 10-minute reference that I was looking for when I started using Ares for my WebOS projects.  Topics covered: UI Basics Widget Fundamentals Using Widgets the Ares Way UI Basics After starting a new project, you will have a fresh canvas full of potential!  Your screen will look like this. If you haven't familiarized yourself with the UI then let's take a quick tour together. Left Panel On the left-hand side, you have a menu that toggles the left pane between 3 views.  The 3 views are: Palette This view contains all of the pre-built widgets that can be used in your app.  Buttons, check boxes and text boxes are just some of the choices here.  You will notice that this panel is organized by widget type, with visible widgets listed first and then things like system services coming later.  Everything under the Palette panel can be added to your application by dragging the widget onto the scene. View Don't dismiss this panel, it is incredibly useful!  The view panel shows you the hierarchical layout of your widgets.  If you are wondering why your widgets don't quite look right, this panel can be invaluable.  For instance, you can quickly see whether a button is contained inside of a horizontal scroll panel. Files This panel allows you to browse your projects and files, double clicking a file will open it in the viewer. Right Panel On the right-hand side, you have a menu that toggles the right pane between 4 views. The 4 views are: Settings This panel displays the settings for the selected widget.  Each widget has a unique set of properties. Styles This panel displays the style for the selected widget, allowing you to change padding, background color and opacity on a per-widget basis. Events This panel displays a list of events that can be hooked for the given widget.  Events are used as a way to perform an action based on user interaction with a widget.  For instance, a Button has the following events. Clicking on the icon that looks like a piece of paper will automatically add a new event handler for a given event and will create an empty event handler method in your code! Help This panel display help for a selected widget.  As of this time, only non-visible widgets contain help entries. Bottom Panel At the bottom of the screen you'll see the Non-Visual Components section.  When you drag services and other non-visual widgets to the canvas, this is where they show up. UI Navigation Tips When a widget is selected, the ESC key will select the parent widget The View tab makes selecting the correct widget infinitely easier When dragging a widget across the UI, the dragging pop-up shows you which container you are currently hovering over. This makes it much easier to place widgets Dragging/dropping in the View menu does nothing Widget Fundamentals Scrollers One thing you will almost always want to do is add a Scroller widget (Palette -> Layout -> Scroller) as the first item in your scene.   If you do use a scroller widget then you can simply use your mouse-wheel to scroll the scene up/down while designing.  If you don't use a scroller widget and your scene extends beyond the height of the screen, you will have to hide widgets to be able to reach the widgets further down on the screen. Once you add the Scroller, you may notice that it does not fill the scene.  To easily make the Scroller fill the scene, you can use the right-hand panel (Settings -> Sizing Tools -> Maximize). Widget Names Even if you never plan on sharing your code, it is good practice to give useful names to the widgets that you will be referencing often (Buttons, labels, services).  It is important that you do this before you start adding event handlers to your widgets, otherwise you may have to go and change the names of any event handlers attached to the widget.  Many people use the objectVerbNoun naming scheme for things that perform an action (buttonCloseAlert or buttonSendData) and the objectNoun scheme for things like labels (labelAddress). Note: I usually don't rename my scrollers and headers since I never refer to them in the actual code. Using Widgets the Ares Way Ares makes widgets easy to use.  To illustrate how to interact with widgets in Ares, I have created a simple project.  You should have no trouble creating the same project by looking at this screenshot. Here we have a very simple scene, a scroller, header, label and button.  The top right panel under the Common section is where you make your basic modifications like changing the name and label of a widget.  For instance, on my header, I changed the  label property to Widget Fundies.  I also changed the label property of labelForce so that it is empty and buttonUpdateForce to The Force. Updating Widgets Programatically Select buttonUpdateForce and add an event handler for ontap: After clicking, you will be whisked away to the code view where you will find this event handler: buttonUpdateForceTap: function(inSender, event) { } Let's make our button do something by modifying line 2 as shown below. buttonUpdateForceTap: function(inSender, event) { this.$.labelForce.setLabel(Is Strong!); } Because the buttonUpdateForceTap event handler is defined in the MainAssistant.prototype, this refers to MainAssistant.  Adding the dollar sign this.$ refers to the collection of objects inside MainAssistant, meaning that every widget contained in main-chrome.js is available through this.$.  Although you cannot see the code generated, when you switch to the GUI view of main-assistant.js, you are actually modifying main-chrome.js. You can modify any common property using this method, here are a couple examples. //Get the current value of the label var currentValue = this.$.labelForce.getLabel(); //Change the label of our button this.$.buttonUpdateForce.setLabel(Join the Dark Side); //Change the class of our button this.$.buttonUpdateForce.setButtonClass(secondary); //Make our button disappear this.$.buttonUpdateForce.setShowing(false); //Get the value of showing var isShowing = this.$.buttonUpdateForce.getShowing(); That is all for this brief introduction, go forth and program! "},{"title":"Barebones SDL For WebOS PDK","baseurl":"","url":"/2010/08/16/barebones-sdl-for-webos-pdk/","date":"2010-08-16 00:00:00 -0700","categories":["pdk","webos"],"body":"This article covers creating barebones SDL apps for the WebOS PDK.  The following topics are covered: How to write a minimalist SDL app How to set screen resolution based on device Ok, let’s dust off those C skills and get to work.  First, of course, you need to setup your environment.  The Palm folks did such a good job explaining how to setup your development environment that I would be silly to try any better.  This tutorial is written from a windows-centric perspective, I love my Mac friends too, I just don’t have the privilege of owning one! After you’ve installed the PDK, configured your environment and all that, it’s time to start a project.  I find it easiest to copy the palm included simple sample project found at C:Program Files (x86)PalmPDKsharesamplecodesimple into a new directory.  When you load up the sample project you might be shocked to see how much code there is, don’t panic, pretty much all this code is for OpenGL stuff. A Minimalist SDL/PDK App I’m all about keeping things simple, so I chopped down the the sample app to the barest essentials.  Here is my barebones version of hello world. #include stdio.h #include SDL.h int main(int argc, char** argv) { /* Even though we aren't using SDL for graphics * the PDK requires that Video be initialized */ SDL_Init(SDL_INIT_VIDEO); /* printf is deprecated, stdout is buffered * -use stderr so we can see output */ fprintf(stderr, Hello worldn); // Cleanup SDL_Quit(); return 0; } Palm includes 3 scripts in the sample that help you build the plugin executable and upload it to the device.  If you’ve renamed your project then you’ll need to modify the highlighted references below so that sample and Sample.cpp point to the correct place location. As you may have guessed, this app prints output to the console.  The only way to see this output is to ssh into your device and execute the app from there.  Not very practical for an app that runs on your phone, but now we have a barebones base to build on. buildit.cmd @echo off @rem Set the device you want to build for to 1 set PRE=1 set PIXI=0 set DEBUG=0 @rem List your source files here set SRC=Sample.cpp @rem List the libraries needed set LIBS=-lSDL -lGLESv2 @rem Name your output executable set OUTFILE=sample ... runit.cmd @echo off call buildit.cmd call uploadit.cmd plink -P 10022 root@localhost -pw /media/internal/sample uploadit.cmd @echo off plink -P 10022 root@localhost -pw killall -9 gdbserver sample pscp -scp -P 10022 -pw sample root@localhost:/media/internal Windows Libraries Just a quick note, if you are testing the following code on your windows machine, you will start getting linker errors once we start using the PDL functions.  The errors you might see are unresolved symbols for _WSACleanup _gethostbyname _gethostname and _WSAStartup.  The solution here is to include a dependency for ws2_32.lib in your project linker dependencies. Alternatively, you could add the following pragma to the code. #pragma comment(lib, ws2_32.lib) Setting Screen Resolution Is the user running a Pre, a Pixi or maybe even a RoadRunner?  Let’s find out and set the screen resolution appropriately.  Because I’m not really interested in printing to the console from here on out, I am going to remove the include file along with the fprintf call.  There are 3 new data types in the following code: SDL_Surface represents an area of memory that can be drawn to. PDL_ScreenMetrics is a struct that holds information about the device’s screen (look at the return types for this call, we will use them below) PDL_Err is a struct that holds failure/success information for PDL calls Bookmark the PDL libraries , you’ll reference them frequently. #include SDL.h #include PDL.h SDL_Surface* screen; PDL_ScreenMetrics screenMetrics; int main(int argc, char** argv) { //Initialize SDL SDL_Init(SDL_INIT_VIDEO); //Initialize the PDL library, this gives us access to the Plug-in API's PDL_Init(0); //Use a PDL call to collect screen metrics PDL_Err err = PDL_GetScreenMetrics(screenMetrics); //If the screen metric call failed, bail out if(err == PDL_INVALIDINPUT) return -1; //Fall back to 320x480 if not Pre or Pixi if(err == PDL_EOTHER) screen = SDL_SetVideoMode(320, 480, 0, SDL_SWSURFACE); //Else if the device IS a pre/pixi, set the resolution based on the screen metrics else screen = SDL_SetVideoMode(screenMetrics.horizontalPixels, screenMetrics.verticalPixels, 0, SDL_SWSURFACE); //Pause for 5 seconds before exiting SDL_Delay(5000); // Cleanup PDL_Quit(); SDL_Quit(); return 0; } I believe this code is fairly self-documenting, but let me point out a few lines.  Line 16, if you haven’t been faithful to your C for awhile you may have forgotten about notation which references the address of the variable, remember to look at function parameters to see whether they take a pointer. On line 24, the SDL_SetVideoMode call takes 4 arguments (width, height, bits per pixel (bpp), flags).  In our case, we are setting bpp to 0, this causes the bpp to be set to whatever the current bpp of the screen is.  We are also using the flag SDL_SWSURFACE.  Often times in the SDL world you will see people calling SDL_DOUBLEBUF SDL_HWSURFACE to enable double buffering and write directly to video memory.  This doesn’t appear to be implemented in the PDK, so if you aren’t using SDL_OPENGL here then you have to use SDL_SWSURFACE. "},{"title":"Drawing Images for WebOS SDL/PDK","baseurl":"","url":"/2010/08/16/drawing-images-for-webos-sdlpdk/","date":"2010-08-16 00:00:00 -0700","categories":["pdk","webos"],"body":"Drawing Images Half of making a game is drawing the images to the screen, it’s not too late to enter into the PDK contest folks!  Thankfully, drawing an image is simple with the functions provided by SDL_image.h. Coincidently, you will need to add SDL_image.lib to your dependencies (or a pragma) for this to run on the desktop.  Before drawing an image, we first need to load it.  All images in SDL are of type SDL_Surface, this should look familiar because our screen object is also a surface, it just happens to be the surface that we draw everything else on to. Did you ever play with  felts as a kid?  You know, where you had a big felt board that you stuck smaller felts to to make various scenes and stories?  Annoyingly, the small cut-out felts always faced the same direction, a fact that probably made budding young directors nervous.  So, surfaces are like felts, the screen is a giant SDL_Surface upon which we place (blit) smaller SDL_Surface objects. Here is how to draw an image.  If a section of code is not commented below, it means that I covered it earlier . #include SDL.h #include PDL.h #include SDL_image.h SDL_Surface* screen; SDL_Surface* image; SDL_Rect imageLocation; PDL_ScreenMetrics screenMetrics; SDL_Event sdlEvent; int main(int argc, char** argv) { SDL_Init(SDL_INIT_VIDEO); PDL_Init(0); PDL_Err err = PDL_GetScreenMetrics(screenMetrics); if(err == PDL_INVALIDINPUT) return -1; if(err == PDL_EOTHER) screen = SDL_SetVideoMode(320, 480, 0, SDL_SWSURFACE); else screen = SDL_SetVideoMode(screenMetrics.horizontalPixels, screenMetrics.verticalPixels, 0, SDL_SWSURFACE); //Load the image image = IMG_Load(moon.png); if(image == NULL) return -2; imageLocation.x = screen-w/2 - image-w/2; imageLocation.y = screen-h/2 - image-h/2; //Blit the image to the screen SDL_BlitSurface(image, NULL, screen, imageLocation); //Draw the screen SDL_Flip(screen); SDL_Delay(10000); PDL_Quit(); SDL_Quit(); return 0; } First we have 2 definitions, an SDL_Surface pointer for our image and an SDL_Rect to hold the coordinates where the image will be displayed.  Line 26 is where we actually load the image using the IMG_Load function provided by SDL_image.h, the only parameter that this function takes is the file location, you might want to check the supported image formats .  Make sure to put your image in your project directory so it can be found.  After checking to make sure that the image loaded successfully, we set the position of the image on the screen. I’ve you never seen screen coordinates calculated, I’ll explain the simple formula in lines 29/30.  Bitmaps are drawn based on the coordinates of a rectangle, the position of the rectangle is specified by a single vertex, which just happens to be the top-left vertex. So you can see that the formula (screen width / 2) would put you directly in the center of the horizontal.  But if you draw the image at this coordinate, the top-left vertex of the image would be placed dead center, which isn’t what we want.  So we need to offset the center by (image width / 2) which will properly center the image horizontally.  The same thing is done vertically. Line 33 is where we blit the image onto the screen using SDL_BlitSurface .  Blit or Bit Blit is a fun word/phrase meaning that we are taking multiple bitmaps and turning them into a single bitmap.  This command literally copies our image onto the screen SDL_Surface at imageLocation.  The 2nd parameter is the source rectangle, setting it to NULL copies the entire source image. Updating the Screen Finally on line 36 we display the screen object using SDL_Flip .  Two things to note here, first, you only call SDL_Flip on a surface that has been allocated with SDL_SetVideoMode.  Finally, in the absence of double buffering, SDL_Flip is functionally equivalent to SDL_UpdateRec .  I prefer SDL_Flip because it only takes 1 parameter. In SDL, you draw the entire scene starting at the background and working to the foreground.  So say you have a starfield background, a planet and a spaceship and you draw them in that same order.  Anytime the spaceship moves, you have to redraw the background, the planet and the spaceship, not just the spaceship.  In a game, this means you redraw every bitmap every frame! There are basically two things you can do to optimize this.  The first is to reduce the number of blits.  For instance if you had a HUD that contained 20 separate bitmaps that don’t change every single frame, it would be more efficient to blit the 20 bitmaps to a new surface, update this surface only when it changes and then blit this master surface to the screen.  Second, there is an SDL function that allows you to  set a clipping rectangle on a surface to prevent certain areas of the surface from being written to.  Depending on the type of app you are writing, some creative clipping rectangles could save you from redrawing. "},{"title":"Input  The Event Loop for WebOS SDL/PDK","baseurl":"","url":"/2010/08/16/input-the-event-loop-for-webos-sdlpdk/","date":"2010-08-16 00:00:00 -0700","categories":["pdk","webos"],"body":"Reading Keyboard & Touch Events SDL provides everything we need to read keyboard input and screen taps.  There is only 1 new data type that hasn’t been covered previously,  SDL_Event , the SDL wiki says it best: > The SDL_Event union is the core to all event handling in SDL; it’s probably the most important structure after SDL_Surface. What we are about to do here is lay down the skeleton for event processing, the loop that keeps the app running, responds to input and displays output.  If you have an engineering background in feedback and control systems, we are about to define our transfer functions.  SDL_Event is a key player here since it allows us to get keyboard and tap events. If you see code that isn’t commented below, it is because I covered it in the barebones article or in Drawing Images . #include SDL.h #include PDL.h SDL_Surface* screen; PDL_ScreenMetrics screenMetrics; SDL_Event sdlEvent; int main(int argc, char** argv) { SDL_Init(SDL_INIT_VIDEO); PDL_Init(0); PDL_Err err = PDL_GetScreenMetrics(screenMetrics); if(err == PDL_INVALIDINPUT) return -1; if(err == PDL_EOTHER) screen = SDL_SetVideoMode(320, 480, 0, SDL_SWSURFACE); else screen = SDL_SetVideoMode(screenMetrics.horizontalPixels, screenMetrics.verticalPixels, 0, SDL_SWSURFACE); //The Event processing loop do { //Loop through all of the current events while(SDL_PollEvent(sdlEvent)){ //Tap event if(sdlEvent.type == SDL_MOUSEBUTTONDOWN){ //Do something with mouse input //sdlEvent.button.x, sdlEvent.button.y contain x,y of tap } //Keyboard Event else if(sdlEvent.type == SDL_KEYDOWN){ //Read the actual key that is down switch(sdlEvent.key.keysym.sym) { //Handle the escape key by exiting the program case SDLK_ESCAPE: sdlEvent.type = SDL_QUIT; break; default: break; } } } //Don't be a cpu hog SDL_Delay(1); } while (sdlEvent.type != SDL_QUIT); PDL_Quit(); SDL_Quit(); return 0; } The first thing we do is setup the do/while loop.  The exit condition is for sdlEvent.type to be SDL_QUIT, this is a condition we will set manually when a user hits the escape key.  The inner while loop calls SDL_PollEvent and passes in the sdlEvent variable created in line 6.  The sdlEvent is populated with the current event and we can then make decisions based on the event.  We handle two event types in this example. Tap events We don’t do anything here but we will soon Keyboard events We detect if there are any keys down (SDL_KEYDOWN), if so then we read the actual key being pressed.  If that key is the escape key, we set sdlEvent.type to SDL_QUIT, our exit condition. The last thing to note is line 49.  This loop will run continuously until the user exits, this means that when your app is completely idle it will still be using a healthy amount of CPU.  The solution is to put a small delay in the processing loop.  If you google around you will find a couple threads on this topic, along with debate between SDL_Delay(0) and SDL_Delay(1) as a minimum.  Specifying a 0 millisecond delay works on some systems because it ends up relinquishing a single timeslice, I typically use 1 as a minimum just to be safe. When you begin polishing your app to be a good WebOS citizen, you will want to revisit SDL_Delay in your event loop and look to adjust the amount of delay based on whether your app is in the foreground or background. "},{"title":"Collision Detection for WebOS SDL/PDK","baseurl":"","url":"/2010/08/19/collision-detection-in-webos-sdlpdk/","date":"2010-08-19 00:00:00 -0700","categories":["pdk","webos"],"body":"Collision Detection An SDL padawan you are. Drawn images to the screen using SDL_BlitSurface you have. How to detect button presses you know not! mMMmm, much to learn you still have! Collision detection isn’t just for video games!  Imagine, your finger colliding with an image and then something epic happening.  This post is going to explore a simple way to detect when things collide. What I will cover in this post is a very simple form of collision detection.  Quite simply, we will look at an X,Y coordinate for a tap event and then figure out if it fits inside a specified rectangle.  Before we begin, I should mention something that is very important about SDL_Rect.  The diligent student may notice that I never set the width/height of a rectangle in the below code.  Well, it just happens that when you call SDL_BlitSurface and specify a destination rectangle, the destination rectangle is modified to contain the width/height information of the bitmap you are blitting.  Keep this in mind. Is this lazy coding?  Should we manually set rectangle dimensions so that we don’t have to worry about whether they’ve been blit’d?  Please comment if you have an opinion.  Now to the code. #include SDL.h #include PDL.h #include SDL_image.h //Surfaces SDL_Surface* screen; SDL_Surface* moon; SDL_Surface* ship; SDL_Surface* explosion; //Rectangles SDL_Rect moonLocation; SDL_Rect shipLocation; //Misc PDL_ScreenMetrics screenMetrics; SDL_Event sdlEvent; bool isPointInRect(SDL_Rect r, Uint16 x, Uint16 y){ return (r.x = x) (x = r.x + r.w) (r.y = y) (y = r.y + r.h); } int main(int argc, char** argv) { //Init SDL_Init(SDL_INIT_VIDEO); PDL_Init(0); //Set screen resolution and check for errors PDL_Err err = PDL_GetScreenMetrics(screenMetrics); if(err == PDL_INVALIDINPUT) return -1; if(err == PDL_EOTHER) screen = SDL_SetVideoMode(320, 480, 0, SDL_SWSURFACE); else screen = SDL_SetVideoMode(screenMetrics.horizontalPixels, screenMetrics.verticalPixels, 0, SDL_SWSURFACE); //Load the images moon = IMG_Load(moon.png); ship = IMG_Load(ship.png); explosion = IMG_Load(explosion.png); if(moon == NULL || ship == NULL || explosion == NULL) return -2; moonLocation.x = screen-w/2 - moon-w/2; moonLocation.y = screen-h/2 - moon-h/2; //Draw the moon to the screen SDL_BlitSurface(moon, NULL, screen, moonLocation); SDL_Flip(screen); //The Event processing loop do { //Loop through all of the current events while(SDL_PollEvent(sdlEvent)){ //Tap event if(sdlEvent.type == SDL_MOUSEBUTTONDOWN){ //Move the ship to the clicked location shipLocation.x = sdlEvent.button.x - ship-w/2; shipLocation.y = sdlEvent.button.y - ship-h/2; //Clear the entire screen SDL_FillRect(screen, NULL, SDL_MapRGB(screen-format, 0, 0, 0)); //Draw the moon SDL_BlitSurface(moon, NULL, screen, moonLocation); //If the location is inside the moon, draw an explosion if(isPointInRect(moonLocation, sdlEvent.button.x, sdlEvent.button.y)) SDL_BlitSurface(explosion, NULL, screen, shipLocation); else //Draw the ship SDL_BlitSurface(ship, NULL, screen, shipLocation); //Draw the scene SDL_Flip(screen); } //Keyboard Event else if(sdlEvent.type == SDL_KEYDOWN){ //Read the actual key that is down switch(sdlEvent.key.keysym.sym) { //Handle the escape key by exiting the program case SDLK_ESCAPE: sdlEvent.type = SDL_QUIT; break; default: break; } } } //Don't be a cpu hog SDL_Delay(1); } while (sdlEvent.type != SDL_QUIT); PDL_Quit(); SDL_Quit(); return 0; } The majority of this code has been covered in previous WebOS PDK articles, please have a look under the PDK Category on the right. Collision Function Let us begin with lines 19-24.  This function returns true if a point is contained inside a rectangle.  The function takes 3 parameters, an SDL_Rect along with an x,y coordinate. bool isPointInRect(SDL_Rect r, Uint16 x, Uint16 y){ return (r.x = x) (x = r.x + r.w) (r.y = y) (y = r.y + r.h); } The condition for a point to be inside a box is quite easy to describe and can be done using 2 inequalities.  As an aid, it may help to see a visual representation of this. Ok, just look at the rectangle for a moment.  Notice what the minimums and maximums for X and Y are: X Min: x X Max: x+w Y Min: y Y Max: y+h Given a point P that has component values for x and y, we can determine if the point is in the rectangle by comparing it to the min and max values of x and y.  First, the x component: x <= P.x <= x+w Then the y component: y <= P.y <= y+h If both statements are true then we are inside the rectangle.  The code in lines 20-23 just breaks up the above 2 inequalities into 4 inequalities, left side first, then right side.  It’s quite simple once you can visualize the min/max of the rectangle. Blowing Things Up 62-63 set the location of the ship based on where the user taps.  The ship is centered on the tap.  Note that we are using shipLocation for both the location of the ship and the explosion. shipLocation.x = sdlEvent.button.x - ship-w/2; shipLocation.y = sdlEvent.button.y - ship-h/2; What we do next is clear the screen (66). Note that this call to SDL_FillRect can be troublesome if you attempt to take shortcuts like I did. You see, I figured that I would save some time by throwing in 0000000 as the color value. This worked fine in Visual Studio but resulted in a permanent black screen on the device. You can read through my misadventures if you are curious.  The important lesson is to use SDL_MapRGB to get the resulting Uint32 for the color. SDL_FillRect(screen, NULL, SDL_MapRGB(screen-format, 0, 0, 0)); 72-75 is where we call the collision detection function and draw either an explosion or a ship, based on the results. //If the location is inside the moon, draw an explosion if(isPointInRect(moonLocation, sdlEvent.button.x, sdlEvent.button.y)) SDL_BlitSurface(explosion, NULL, screen, shipLocation); else //Draw the ship SDL_BlitSurface(ship, NULL, screen, shipLocation); That’s it, here is a short video proving that it works! Credits: Explosion png was found at Bothel University of Washington . Moon png is from Nasa gallery. Ship png created by myself. "},{"title":"Lab Notes: Compiling ffmpeg for WebOS","baseurl":"","url":"/2010/08/25/lab-notes-compiling-ffmpeg-for-webos/","date":"2010-08-25 00:00:00 -0700","categories":["pdk","webos"],"body":"It is a dream of many to have VLC running on WebOS.  I thought that with the release of the PDK, I would start exploring this.  Since VLC uses many of the codecs provided by ffmpeg’s libavcodec library, it made sense to start by compiling this library and verifying basic functionality. In the end, I got ffplay to play a video on WebOS using libavcodec, the result was anti-climactic for large videos due to the lack of HW acceleration, one of the Palm PDK guys, unwiredBen provided information for further research .  Overall, performance for smaller files is acceptable, and the ability to grab rtsp streams could lead to many applications.  The lab notes are below. This isn’t meant to be a tutorial but I will expound on things as  necessary.  I have a cross-compile configuration setup as outlined by the fine folks at WebOS Internals .  If you configure Scratchbox 2 as outlined in the link, it makes these cross-compile tasks incredibly trivial. Getting the Source git clone git://git.ffmpeg.org/ffmpeg/ cd ffmpeg git clone git://git.ffmpeg.org/libswscale/ Config Options If you’re like me, the first thing I do when compiling source is to run ./configure help and copy/paste that output to notepad++/vi/gedit.  I then go through, line-by-line and delete options that aren’t of interest to me.  At the end, I am left with a list of relevant config options.  Here is the list I came up with.  I use a + to designate that I want this option and a - to designate that I specifically want to make sure this option is disabled.  You may think that the **- **is redundant, it is not because it lets me later check default config options to make sure that the default for the option has not changed. Configuration options: - --disable-static do not build static libraries [no] - --enable-shared build shared libraries [no] - --enable-gpl allow use of GPL code, the resulting libs and binaries will be under GPL [no] - --enable-nonfree allow use of nonfree code, the resulting libs and binaries will be unredistributable [no] + --disable-doc do not build documentation + --enable-runtime-cpudetect detect cpu capabilities at runtime (bigger binary) + --enable-hardcoded-tables use hardcoded tables instead of runtime generation Quick Source Analysis ffmpeg provides source for a utility called ffplay.  I know that ffplay uses SDL and also know that the WebOS PDK doesn’t support HWSURFACE.  I did a quick grep for SDL_HWSURFACE in ffplay.c and sure enough, there it was. static int video_open(VideoState *is){ int flags = SDL_HWSURFACE|SDL_ASYNCBLIT|SDL_HWACCEL; int w,h; ... case SDL_VIDEORESIZE: if (cur_stream) { screen = SDL_SetVideoMode(event.resize.w, event.resize.h, 0, SDL_HWSURFACE|SDL_RESIZABLE|SDL_ASYNCBLIT|SDL_HWACCEL); screen_width = cur_stream-width = event.resize.w; screen_height= cur_stream-height= event.resize.h; } ... After changing both instances of SDL_HWSURFACE to SDL_SWSURFACE, I decided I felt lucky enough to compile. Compiling From the scratchbox2 prompt: ./configure disable-doc enable-runtime-cpudetect enable-hardcoded-tables Testing I figured the easiest test would be to play the video that comes with the Pre.  I fired it up command-line style and SUCCESS!  But this video, an h264 at 320480 totally rocked the CPU on the Pre.  Top showed 99% CPU usage, I guess it’s time to start looking into DSP coding on the TI OMAP3430.  I tried some lighter-weight flv files grabbed with youtube-dl and it plays those no problem, averaging about 25% CPU for a 320200 file. Next Steps An app that leverages some simple web services along with ffserver/ffmpeg on the server side could be used to browse my audio/video library from the Pre.  I setup a quick ffserver/ffmpeg test where I re-encoded an avi using ffmpeg, pushed it out as an rtsp stream using ffserver and then slurped it down on the Pre with ffplay.  The test performed well, I pushed a 320240 video with a bitrate of 256 KB/s @20fps over Wifi.  Not surprisingly, my poor Atom processors were the bottleneck for HD video, only being able to re-encode at about 7fps.  Rtmpdump could be added to this mix for a HULU streaming solution (or any flash video for that matter). There are 2 challenges with these solutions. How well will they work when you don’t have Wifi?  The bottleneck when away from home is going to be the connection speed to your home server. These solutions are difficult to bring to the masses since a server is required.  Most folks with enough knowledge to do this stuff run Linux, so you’ll need a Linux server if you want to follow along. Stream Hulu to your WebOS device, all you need is this app and a Linux server in your home! That is a hard sell, but the world needs more Linux servers in the home "},{"title":"Using the Palm Pre as Xbox 360 storage","baseurl":"","url":"/2010/09/01/using-the-palm-pre-as-xbox-360-storage/","date":"2010-09-01 00:00:00 -0700","categories":["pdk","webos"],"body":"I have a dream where I plug my Palm Pre into my Xbox 360 and it is recognized as a USB Mass Storage device.  I then save all my games and downloaded content to it.  Later, I fire up a PDK app that loads x360 (A FUSE filesystem driver for the 360) and I edit my saved games, adding more health, more gold, etc. What would it take for a USB newb to turn this into a reality?  I’m not sure, but I am willing to try. Spoofing Vendor/Product ID My first thought was that the 360 was blocking Vendor/Product IDs, much like Apple’s iTunes does.  So my first step was to spoof the USB Vendor/Product ID. First, I found a USB stick that worked with the 360, I popped it into my linux box, ran lsusb -v and copied down the details. Bus 002 Device 002: ID 1307:0163 Transcend Information, Inc. 512MB/1GB Flash Drive Device Descriptor: bLength 18 bDescriptorType 1 bcdUSB 2.00 bDeviceClass 0 (Defined at Interface level) bDeviceSubClass 0 bDeviceProtocol 0 bMaxPacketSize0 64 idVendor 0x1307 Transcend Information, Inc. idProduct 0x0163 512MB/1GB Flash Drive bcdDevice 1.00 iManufacturer 1 iProduct 2 iSerial 3 bNumConfigurations 1 I then did the same for my Palm Pre Bus 002 Device 003: ID 0830:0101 Palm, Inc. Device Descriptor: bLength 18 bDescriptorType 1 bcdUSB 2.00 bDeviceClass 0 (Defined at Interface level) bDeviceSubClass 0 bDeviceProtocol 0 bMaxPacketSize0 64 idVendor 0x0830 Palm, Inc. idProduct 0x0101 bcdDevice 2.16 iManufacturer 1 Palm Inc. iProduct 2 Pre iSerial 3 a5470bf024bd51193d15a9614fe7d302960c955f bNumConfigurations 1 Unfortunately, the only way of changing the Vendor/Product ID is to recompile the kernel .  If we grep through a default config file for the Pre (look in /boot on the device), we’ll find: CONFIG_USB_ROCKHOPPER=y CONFIG_USB_ROCKHOPPER_MANUFACTURER_STRING=Palm Inc. CONFIG_USB_ROCKHOPPER_PRODUCT_STRING=Pre CONFIG_USB_ROCKHOPPER_VID=0x830 CONFIG_USB_ROCKHOPPER_PID_DEV_1=0x100 CONFIG_USB_ROCKHOPPER_PID_DEV_2=0x101 CONFIG_USB_ROCKHOPPER_PID_DEBUG=0x8002 CONFIG_USB_ROCKHOPPER_PID_PASSTHRU=0x8003 CONFIG_USB_ROCKHOPPER_PID_RETAIL=0x8004 For the curious, here is the secret decoder table: parameter function USB_ROCKHOPPER_VID The USB Vendor ID USB_ROCKHOPPER_PID_DEV_1 The USB Product ID for RNDIS Ethernet + Passthru USB_ROCKHOPPER_PID_DEV_2 The USB Product ID for RNDIS Ethernet + Mass-Storage + Novacom. USB_ROCKHOPPER_PID_DEBUG The USB Product ID for Mass-Storage + Novacom USB_ROCKHOPPER_PID_PASSTHRU The USB Product ID for Passthru USB_ROCKHOPPER_PID_RETAIL The USB Product ID for Mass-Storage only Grep through the source tree for these config variables and you’ll find all the goodies in drivers/usb/gadget/rockhopper.c.  We don’t actually have to modify this source file, but long term, the elegant solution would be compiling this as a module and adding some of these config strings as arguments. Results of Spoofing After compiling the kernel and loading it on the Pre, the device now looked like this: Bus 002 Device 004: ID 1307:0163 Transcend Information, Inc. 512MB/1GB Flash Drive Device Descriptor: bLength 18 bDescriptorType 1 bcdUSB 2.00 bDeviceClass 0 (Defined at Interface level) bDeviceSubClass 0 bDeviceProtocol 0 bMaxPacketSize0 64 idVendor 0x1307 Transcend Information, Inc. idProduct 0x0163 512MB/1GB Flash Drive bcdDevice 3.16 iManufacturer 1 iProduct 2 iSerial 3 But, the 360 didn’t seem to care and still couldn’t see the device :( Ok, time to regroup and educate.   USB in a Nutshell became my new best friend. USB Configuration and Interface Descriptors As I read about the USB spec, a couple things started nagging at me, particularly  bNumInterfaces and Endpoint Descriptors. USB devices can have multiple Interfaces defined.  On the Palm Pre, you can use it in Mass Storage mode, usb networking mode or Novacom mode (for debugging).  Each of these modes has an Interface Descriptor where bNumInterfaces is the total number of these Interface Descriptors. Each Interface Descriptor contains 1 or more Endpoint Descriptors.  From an embedded background, I’m imagining these to be data pins, where each pin is configured either for input or output (source/sink). I started taking a survey of USB devices that worked with my 360 and those that didn’t work and found the following. Device bNumInterfaces # of Endpoint Descriptors for Mass Storage Works with Xbox 360 Transcend flash drive 1 3: Bulk In, Bulk Out, Interrupt In yes Kingston flash drive 1 2: Bulk In, Bulk Out no Generic flash drive 1 3: Bulk In, Bulk Out, Interrupt In yes Palm Pre 2 (In mass storage mode, 4 otherwise) 2: Bulk In, Bulk Out no I’m not surprised that all of my flash drives have only a single interface descriptor.  The two questions I need to answer are: Does the Xbox 360 care if multiple interface descriptors exist? Does the Xbox 360 require an Interrupt Endpoint to work properly? Well, question 1 can easily be avoided.  The reason I’m seeing 2 interfaces is because I am in developer mode (USB_ROCKHOPPER_PID_DEBUG).  A quick konami code later and I’m out of developer mode and down to a single interface descriptor in mass storage mode. Question 2 will require a hack of the Pre mass storage driver. Hacking the Palm Pre Mass Storage Driver Conveniently, Palm provides all of their kernel modifications as a patch.  Grep’ing through this master patch, it was quite easy to spot the interface descriptors in f_mass_storage.c. When I started this little project, I knew nothing about USB other than how to plug it in.  I’m a bit further now, but to be clear, the intention of this hack is purely cosmetic. The simple goal is: Add an interrupt Endpoint for USB_ROCKHOPPER_PID_RETAIL After a bit of hacking, I came up with the following patch. --- f_mass_storage.c.orig 2010-09-01 08:27:17.223869104 -0700 +++ f_mass_storage.c 2010-09-01 20:22:28.093870603 -0700 @@ -339,6 +339,7 @@ struct usb_ep *bulk_in; struct usb_ep *bulk_out; + struct usb_ep *intr_in; struct fsg_buffhd *next_buffhd_to_fill; struct fsg_buffhd *next_buffhd_to_drain; @@ -450,6 +451,8 @@ name = bulk-in; else if (ep == fsg-bulk_out) name = bulk-out; + else if (ep == fsg-intr_in) + name = intr-in; else name = ep-name; FSG_DBG(fsg, %s set haltn, name); @@ -501,7 +504,7 @@ .bLength = sizeof intf_desc, .bDescriptorType = USB_DT_INTERFACE, - .bNumEndpoints = 2, /* Adjusted during fsg_bind() */ + .bNumEndpoints = 3, /* Adjusted during fsg_bind() */ .bInterfaceClass = USB_CLASS_MASS_STORAGE, .bInterfaceSubClass = US_SC_SCSI, .bInterfaceProtocol = US_PR_BULK, @@ -530,13 +533,23 @@ /* wMaxPacketSize set by autoconfiguration */ }; +static struct usb_endpoint_descriptor +fs_intr_in_desc = { + .bLength = USB_DT_ENDPOINT_SIZE, + .bDescriptorType = USB_DT_ENDPOINT, + + .bEndpointAddress = USB_DIR_IN, + .bmAttributes = USB_ENDPOINT_XFER_INT, +}; + static struct usb_descriptor_header *fs_function[] = { (struct usb_descriptor_header *) intf_desc, + (struct usb_descriptor_header *) fs_intr_in_desc, (struct usb_descriptor_header *) fs_bulk_in_desc, (struct usb_descriptor_header *) fs_bulk_out_desc, NULL, }; static struct usb_endpoint_descriptor @@ -560,9 +573,20 @@ .bInterval = 1, /* NAK every 1 uframe */ }; +static struct usb_endpoint_descriptor +hs_intr_in_desc = { + .bLength = USB_DT_ENDPOINT_SIZE, + .bDescriptorType = USB_DT_ENDPOINT, + + /* bEndpointAddress copied from fs_bulk_out_desc during fsg_bind() */ + .bmAttributes = USB_ENDPOINT_XFER_INT, + .wMaxPacketSize = __constant_cpu_to_le16(64), + .bInterval = 8, +}; static struct usb_descriptor_header *hs_function[] = { (struct usb_descriptor_header *) intf_desc, + (struct usb_descriptor_header *) hs_intr_in_desc, (struct usb_descriptor_header *) hs_bulk_in_desc, (struct usb_descriptor_header *) hs_bulk_out_desc, NULL, Every time you hack a USB endpoint, a USB programmer dies :/  Sorry! Mass Storage Driver Hack Results After recompiling and loading the mass storage driver, the endpoints look like this: Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 0 bAlternateSetting 0 bNumEndpoints 3 bInterfaceClass 8 Mass Storage bInterfaceSubClass 6 SCSI bInterfaceProtocol 80 Bulk (Zip) iInterface 0 Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x00 EP 0 OUT bmAttributes 3 Transfer Type Interrupt Synch Type None Usage Type Data wMaxPacketSize 0x0040 1x 64 bytes bInterval 8 Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x81 EP 1 IN bmAttributes 2 Transfer Type Bulk Synch Type None Usage Type Data wMaxPacketSize 0x0200 1x 512 bytes bInterval 0 Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x01 EP 1 OUT bmAttributes 2 Transfer Type Bulk Synch Type None Usage Type Data wMaxPacketSize 0x0200 1x 512 bytes bInterval 1 The interrupt endpoint is there but the fact that it is enumerated as EP0 is a problem, EP0 is reserved and not supposed to have a descriptor!   Clearly I am missing something here!  I took several passes at this, trying to get the interrupt endpoint recognized as 082 / EP2, which is used in other device configurations as an interrupt endpoint.  So far none have sticked. As I build up my understanding of the composite and gadget usb frameworks, I found a good article on the composite usb framework which unfortunately shows that most folks writing USB mass storage drivers in linux leave the endpoint descriptor definitions alone.  The endpoint details happen behind the scenes and get implemented as part of the framework rather than being defined by your driver.  This project is getting put on the back burner for now until I have some quality time to devote to understanding the frameworks involved. Continued in  Part 2. "},{"title":"Using the Palm Pre as Xbox 360 Storage (part 2)","baseurl":"","url":"/2010/09/12/using-the-palm-pre-as-xbox-360-storage-part-2/","date":"2010-09-12 00:00:00 -0700","categories":["pdk","webos"],"body":" Configuration Descriptor: bLength 9 bDescriptorType 2 wTotalLength 39 bNumInterfaces 1 bConfigurationValue 1 iConfiguration 4 bmAttributes 0xc0 Self Powered MaxPower 500mA Interface Descriptor: bLength 9 bDescriptorType 4 bInterfaceNumber 0 bAlternateSetting 0 bNumEndpoints 3 bInterfaceClass 8 Mass Storage bInterfaceSubClass 6 SCSI bInterfaceProtocol 0 Control/Bulk/Interrupt iInterface 0 Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x81 EP 1 IN bmAttributes 2 Transfer Type Bulk Synch Type None Usage Type Data wMaxPacketSize 0x0200 1x 512 bytes bInterval 0 Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x01 EP 1 OUT bmAttributes 2 Transfer Type Bulk Synch Type None Usage Type Data wMaxPacketSize 0x0200 1x 512 bytes bInterval 1 Endpoint Descriptor: bLength 7 bDescriptorType 5 bEndpointAddress 0x82 EP 2 IN bmAttributes 3 Transfer Type Interrupt Synch Type None Usage Type Data wMaxPacketSize 0x0040 1x 64 bytes bInterval 8 Digging Deeper I figured that snooping USB traffic might lend an insight into what’s happening.  To this end I used usbmon, luckily the required module was already built-in to the stock Pre kernel.  Taking traces of both the stock driver and the modified working driver, I was very disappointed to find that the traffic for the USB detection phase was identical for both drivers?!?  How could this be?  Just a note, if you are ever looking to understand the raw text output that usbmon produces, the  official readme and the  cheat sheet are essential. For reference, this is the repeating pattern I see in all of the usbmon traces, my decoding notes included. //Not sure what this is but guessing that it is normal for BOT aebf8a80 14.472869 S Ii:1:001:1 -:-606348325 2 //Get Port 1 Status ad8f4580 14.472930 S Ci:1:001:0 s a3 00 0000 0001 0004 4 ad8f4580 14.472930 C Ci:1:001:0 0 4 = 07010000 //Clear Port 1 Feature 2 (over current) ad8f4580 14.472961 S Co:1:001:0 s 23 01 0002 0001 0000 0 ad8f4580 14.472961 C Co:1:001:0 0 0 //Get Port 2 Status ad8f4580 14.473419 S Ci:1:001:0 s a3 00 0000 0002 0004 4 ad8f4580 14.473449 C Ci:1:001:0 0 4 = 00010000 //Get Port 3 Status ad8f4580 14.473480 S Ci:1:001:0 s a3 00 0000 0003 0004 4 ad8f4580 14.473480 C Ci:1:001:0 0 4 = 00010000 //Get Port 1 Status ad8f4580 14.512908 S Ci:1:001:0 s a3 00 0000 0001 0004 4 ad8f4580 14.512939 C Ci:1:001:0 0 4 = 03010400 //Clear Port 1 Feature 12 (ch suspend) ad8f4580 14.512939 S Co:1:001:0 s 23 01 0012 0001 0000 0 ad8f4580 14.512939 C Co:1:001:0 0 0 //Get Device Status ad8f4580 14.533111 S Ci:1:002:0 s 80 00 0000 0000 0002 2 ad8f4580 14.534149 C Ci:1:002:0 0 2 = 0000 //Set Port 1 Feature 2 (over current) ad8f4580 14.833068 S Co:1:001:0 s 23 03 0002 0001 0000 0 ad8f4580 14.833099 C Co:1:001:0 0 0 aebf8a80 14.853027 C Ii:1:001:1 -2:-606348325 0 I feel like I’m on the wrong side of the usbmon trace!  I was hoping to find that one of the Ci requests resulted in an error code, no such luck.  Apart from errors produced from the requests bound for the Interrupt EP (Ii), things look clean.  I should note that not only is the request structure identical between the 2 drivers, but the return codes for device and port status as well. A Simple Working Solution From pouring over the source for f_mass_storage.c, I knew that this work was based on file_storage.c by Alan Stern.  This well-documented piece of code is essentially a file-backed USB Mass Storage Device (MSD).  Meaning, when you load the module, you specify a file or device which is then presented as if it were a MSD. I figured I may as well compile this simple module (file_storage.c) and give it a try.  Due to the way the Palm USB solution is setup, I had to do just a bit more.  It helps to understand the Palm USB ecosystem which I have painted below. The rockhopper module wraps around 5 separate USB modules, handling the loading of the desired module.  Because of this, rockhopper is always loaded in the background which means the USB ports will always be in-use.  Because rockhopper is built into the kernel by default, a recompile was necessary to compile rockhopper as a loadable module.  This allows loading my own file_storage module in place of it. After loading the file_storage module and specifying my spare ext3fs partition (thanks to Meta-Doctor), I plugged into my Xbox 360 and was pleasantly surprised to be able to see and use my Pre as mass storage! file_storage.c vs f_mass_storage.c Why does one module work where the other does not?  I began to look for some new tools to help me find out.  The first tool I used was Intel’s USB Command Verifier, this utility lets you run a gamut of tests against your USB device, from the ones listed in Chapter 9 of the USB Spec to ones specific for MSDs.  USB CV was incredibly frustrating to get going on my 64-bit system, the process looked like this: Install USB CV Reboot to disable driver-signing Manually install Intel EHCI debug driver due to bug in 64-bit version of USB CV Go find an AT keyboard/mouse because USB device signals are being hijacked 4 reboots later and I was in business.  I ran the chapter 9 tests and the mass storage tests against both drivers.  The chapter 9 test results were identical, the mass storage results were close to identical with the following discrepencies: f_mass_storage Serial Number Test Invalid characters in Serial Number Test Case 4,8 No stall after zero-length data Command Set Test No stall after zero-length data Power-Up Test Could not find device after enumeration I should note that I also ran the Chapter 9 and mass storage tests against every USB mass storage device I own.  2 of the devices that work with my Xbox 360 failed the Serial Number Test.  Also, I believe the Power-Up Test warning was triggered due to the amount of time it takes to plug-in the Pre and initiate mass storage mode.  All-in-all, I didn’t find the smoking gun I was hoping for in these tests. A New Goal I have a working solution, but I don’t know why it works.  My new goal is to provide a patch for f_mass_storage.c that will allow it to work with the Xbox 360.  This means analyzing and understanding the changes between file_stoage.c and f_mass_storage.c. Read all the answers in the finale . "},{"title":"Using the Palm Pre as Xbox 360 Storage (the finale)","baseurl":"","url":"/2010/09/16/using-the-palm-pre-as-xbox-360-storage-the-finale/","date":"2010-09-16 00:00:00 -0700","categories":["android","webos"],"body":" 15 days ago, I set out to determine why I couldn’t use my Palm Pre on my Xbox 360.  We all start somewhere, but I started with absolutely no knowledge of the composite gadget framework or Palm’s rockhopper composite driver.  In retrospect, I spent a lot of time asking silly questions and researching irrelevant things.  But once again I have been awarded for persistence, while picking up some great knowledge and tools for the future. For the last 2 days, I have relished the joy of discovery.  Truth be told, this problem was solved 2 years ago by one Felipe Balbi on the linux-usb mailing list.  The next logical question is why the problem exists on so many devices if it was fixed 2 years ago?  Am I really the first person to discover this problem and solution and actually care enough to try and get it fixed on Palm and Android devices? Read on as I reveal all that I know. (Palm users can skip to the good stuff by grabbing the latest UberKernel release and plugging in to their Xbox 360) Setting the Stage I am going to describe the process by which data is transfered from a host to a device using the BOT protocol.  Before doing so, I need to present two of the data structures that are used to send data and reply with status.  These data structures are defined in great detail in the  spec for the USB Mass Storage Class .  The definitions here are pulled from Alan Stern’s monolithic file storage gadget. CBW /* Command Block Wrapper */ struct bulk_cb_wrap { __le32 Signature; /* Contains 'USBC' */ u32 Tag; /* Unique per command id */ __le32 DataTransferLength; /* Size of the data */ u8 Flags; /* Direction in bit 7 */ u8 Lun; /* LUN (normally 0) */ u8 Length; /* Of the CDB, = MAX_COMMAND_SIZE */ u8 CDB[16]; /* Command Data Block */ }; CSW /* Command Status Wrapper */ struct bulk_cs_wrap { __le32 Signature; /* Should = 'USBS' */ u32 Tag; /* Same as original command */ __le32 Residue; /* Amount not transferred */ u8 Status; /* See below */ }; I have to point out something incredibly useful that is defined in the spec. For CBW - For CSW - dCSWSignature: Signature that helps identify this data packet as a CSW. The signature field shall contain the value 53425355h (little endian), indicating CSW. How does this help?  For those of us running usbmon, you can easily identify CBW and CSW which will allow you to reverse engineer the data contained in the structures.  For instance, this string from usbmon: ffff88008d4e0480 975285546 C Bi:2:040:1 0 13 = 55534253 01000000 00000000 00 ffff88008d4e0480 975286954 S Bo:2:040:1 -115 31 = 55534243 02000000 00000000 00000600 00000000 00000000 00000000 000000 In the first line, 55534253 identifies a CSW (convert to little endian), whereas the second line 55534243 is a CBW!  When transferring data using the BOT protocol, the basics work like so, to keep things simple I’ll refer to the host as the Xbox and the device as the Pre. The Xbox sends a CBW to the Pre (this arrives on the Bulk-Out EP) The Pre analyzes the CBW and attempts to satisfy the host request The Pre returns status to the Xbox regarding the results of the CBW using a CSW All SCSI commands are wrapped by the CBW struct.  Regardless if we are reading/writing data or requesting device information, all of these commands arrive in a CBW payload. The Problem The problem,  originally patched by Felipe Balbi back in September of 2008 is best described by the author. /* Special case workaround: There are plenty of buggy SCSI * implementations. Many have issues with cbw-Length * field passing a wrong command size. For those cases we * always try to work around the problem by using the length * sent by the host side provided it is at least as large * as the correct command length. * Examples of such cases would be MS-Windows, which issues * REQUEST SENSE with cbw-Length == 12 where it should * be 6, and xbox360 issuing INQUIRY, TEST UNIT READY and * REQUEST SENSE with cbw-Length == 10 where it should * be 6 as well. */ So going back to steps 1-3 above, here is what happens: The Pre receives the CBW, it looks at the length of the CDB and notes that it is wrong.  Because the correct logic doesn’t exist to handle this particular size (10), the request is dropped on the floor. The Solution The solution is quite simple, check the length of the command and fix it if need be.  A more generic fix that can handle varying faulty request lengths is what Felipe submitted. Why the Problem Still Exists In 2008, Mike Lockwood from the Android team created f_mass_storage.c which was based on file_storage.c by Alan Stern. In Sept 2008, a patch for file_storage.c was submitted that fixes functionality with buggy SCSI implementations.  This patch was never carried over to the Android f_mass_storage.c. In 2009, Palm released the Palm Pre using the Android version of f_mass_storage.c as-is. The Final Solution In the mainline kernel there is a new f_mass_storage.c written by Michal Nazarewicz, also based on file_storage.c.  This new driver includes the fix for the Xbox 360.  Android has abandoned their copy of f_mass_storage.c in favor of the mainline version.  Unfortunately, since they just did this recently it means that Droid/DroidX/Droid2/EVO users will not be able to use their device with the Xbox 360 until Android 2.3 is released. My recommendation to Palm is to adopt the mainline f_mass_storage.c.  Until then, I am using the following patch to allow my Palm Pre to act as mass storage for the Xbox 360. This patch has been submitted to the WebOS Internals team and will hopefully show up in a future UberKernel release  is in the latest UberKernel. diff -BurN linux-2.6.24/drivers/usb/gadget/f_mass_storage.c linux-2.6.24/drivers/usb/gadget/f_mass_storage.c.fix --- linux-2.6.24/drivers/usb/gadget/f_mass_storage.c 2010-09-16 16:37:58.651375877 -0700 +++ linux-2.6.24/drivers/usb/gadget/f_mass_storage.c.fix 2010-09-16 16:35:10.783868776 -0700 @@ -1980,11 +1980,24 @@ /* Verify the length of the command itself */ if (cmnd_size != fsg-cmnd_size) { - /* Special case workaround: MS-Windows issues REQUEST SENSE - * with cbw-Length == 12 (it should be 6). */ - if (fsg-cmnd[0] == SC_REQUEST_SENSE fsg-cmnd_size == 12) - cmnd_size = fsg-cmnd_size; - else { + /* Special case workaround: There are plenty of buggy SCSI + * implementations. Many have issues with cbw-Length + * field passing a wrong command size. For those cases we + * always try to work around the problem by using the length + * sent by the host side provided it is at least as large + * as the correct command length. + * Examples of such cases would be MS-Windows, which issues + * REQUEST SENSE with cbw-Length == 12 where it should + * be 6, and xbox360 issuing INQUIRY, TEST UNIT READY and + * REQUEST SENSE with cbw-Length == 10 where it should + * be 6 as well. + */ + if (cmnd_size = fsg-cmnd_size) { + DBG(fsg, %s is buggy! Expected length %d + but we got %dn, name, + cmnd_size, fsg-cmnd_size); + cmnd_size = fsg-cmnd_size; + } else { fsg-phase_error = 1; return -EINVAL; } "},{"title":"An Ares View Menu Example for WebOS","baseurl":"","url":"/2010/09/24/an-ares-view-menu-example-for-webos/","date":"2010-09-24 00:00:00 -0700","categories":["ares","webos"],"body":" Want a menu that looks like this? Are you using Ares as your development environment? Are you frustrated because this menu doesn’t exist in the list of Widgets? Read on for a full example of how to easily implement this widget along with how to tie in the functionality for the buttons. ## The viewMenu Widget This is a special widget that is not created in the same way that other widgets are.  There isn’t a viewMenu UI Widget in the Palette to drag onto your scene.  The link above explains more details about this, what you need to know is that this menu is declared programatically. We’ll dive right in by creating a new Ares project and modifying main-assistant.js by adding the createMenu function: function MainAssistant(argFromPusher) { } MainAssistant.prototype = { setup: function() { Ares.setupSceneAssistant(this); this.createMenu(); }, cleanup: function() { Ares.cleanupSceneAssistant(this); }, createMenu: function() { this.controller.setupWidget(Mojo.Menu.viewMenu, this.attributes = { spacerHeight: 50, }, this.model = { visible: true, items: [ { items: [ { label: Mount Image, command: mount, width: 160 }, { label: Create Image, command: create, width: 160 } ], toggleCmd: mount } ] }); } }; The createMenu function The createMenu function is quite simple, all it does is call Mojo.Controller.SceneController. setupWidget (name, attributes, model) .  Let’s walk through each parameter. name The name of the widget we are configuring, in this case it is Mojo.Menu.viewMenu attributes A list of the attributes for this widget.  There are only 2 choices for the viewMenu widget.  As you can see on line 15, I have specified spacerHeight to be 50.  What this does is push down the contents of your scene by 50 pixels.  The viewMenu widget is overlayed on the scene, so with the default spacerHeight of 0, the content of the scene will be obscured by the viewMenu. model The model defines the items in the menu.  You'll notice that above, I am creating an array within another array.  The reason for this is to create what is called a toggle group .  If I had put the menu items in only 1 array, the menu would like like this. The toggle group makes the menu behave like the radio button widget.  Notice that for each item, we have defined: label What you see. command A reference for when we make the button come to life. width Calculated so that the sum of all buttons is the width of the screen (320). Finally, line 25 sets the default state of the toggle group so that mount is selected. Making the Buttons Work In this example, I am going to make the buttons display/hide a panel on the screen.  Based on my scene layout, you should be able to see quite easily what I am doing. When the mount button is pressed, I will hide panelCreate and show panelMount and vice versa.  The code to do so uses the command references from our items and looks like this. handleCommand: function(event) { if (event.type == Mojo.Event.command) { switch (event.command) { case mount: this.$.panelCreate.setShowing(false); this.$.panelMount.setShowing(true); break; case create: this.$.panelMount.setShowing(false); this.$.panelCreate.setShowing(true); break; default: break; } } } Full Source The full source is below. function MainAssistant(argFromPusher) {} MainAssistant.prototype = { setup: function() { Ares.setupSceneAssistant(this); this.createMenu(); }, cleanup: function() { Ares.cleanupSceneAssistant(this); }, createMenu: function() { this.controller.setupWidget(Mojo.Menu.viewMenu, this.attributes = { spacerHeight: 50, }, this.model = { visible: true, items: [ { items: [ { label: Mount Image, command: mount, width: 160 }, { label: Create Image, command: create, width: 160 } ], toggleCmd: mount } ] }); }, handleCommand: function(event) { if (event.type == Mojo.Event.command) { switch (event.command) { case mount: this.$.panelCreate.setShowing(false); this.$.panelMount.setShowing(true); break; case create: this.$.panelMount.setShowing(false); this.$.panelCreate.setShowing(true); break; default: break; } } } }; "},{"title":"The Path of Most Resistance","baseurl":"","url":"/2010/10/03/the-path-of-most-resistance/","date":"2010-10-03 00:00:00 -0700","categories":["ares","pdk","webos"],"body":" I am building a track record of choosing projects that somehow flex the limits of what is easily doable on the Palm Pre. First I wanted to use the Palm Pre as a mass storage device on my Xbox 360.  The unraveling of that rabbit-hole took a full month.  Now I want to be able to create and mount regular files as USB devices on the Palm Pre (an app I'm calling USB Mass Storage Tool).  This time I am hitting a technical limitation imposed by Palm. The current limitation is that sysfs is not available for applications to read/write from within the jailed environment in which apps run. The Need Q. Why would anyone need USB Mass Storage Tool?  I mean, the Pre now works with the Xbox 360, isn't that enough? A. It is not enough.  When you plug the Pre (or any storage device) into the Xbox 360 and attempt to use the device for data storage (storing saved games/profile information), the Xbox 360 will format the entire device.  For the Palm Pre owner this means you lose /media/internal. If you want to cry, try this.  Hook your Pre into the Xbox, format it to be used as Xbox storage and then unplug your device.  You will notice that all of your apps, photos, videos, etc are forever gone. The Dream My dream is to be able to store a USB image for my Wii, Xbox 360 and PS3 as an actual file on the Pre (I don't own a PS3, but PS3 owners should be interested in this).  So when you browse the files on my device you will see: xbox.img wii.img ps3.img When I want to save games on my Xbox, I choose xbox.img and present this file as my USB storage device.  The Xbox formats the entire device (which is just a file sitting safely on /media/internal) and everyone walks away with no data loss. Realizing the Dream Not only is this possible, it is incredibly easy to do.  You can mount any file or device by echoing said file/device into an entry in the sysfs filesystem: echo /media/internal/xbox.img /sys/devices/platform/musb_hdrc.0/gadget/gadget-lun0/file This sysfs entry was created by f_mass_storage.c for this very purpose.  However, since Palm does not allow apps to access sysfs, what would have been a simple app has once again turned into usb module hacking.  The sysfs filesystem contains non-process related information about drivers and hardware.  Preventing user-level access to sysfs while allowing access to procfs doesn't make a lot of sense from a security perspective.  I have asked Palm for details on whether they plan on changing this policy. In the meantime, because Palm allows access to /proc, I am going to modify the USB Mass storage driver so that it creates a procfs entry on module startup.  This procfs entry will mimic the behavior of the sysfs entry. "},{"title":"Android Torch LED","baseurl":"","url":"/2010/12/06/android-torch-led/","date":"2010-12-06 00:00:00 -0800","categories":["android"],"body":" Many Android devices have an LED next to the camera lens that can be used as a flash.  I’ve been doing some research on the so-called Torch mode of this LED.  Torch mode simply turns the LED on and leaves it on as opposed to temporarily flashing the LED during a photo.I’m sure everyone has seen the flashlight apps that do just this. I am looking at possible scenarios where the torch could help with image analysis.  A secondary goal is to have the LED light enabled automatically based on measurable conditions. The Torch API The API calls for using the torch are incredibly simple.  The Android Camera API contains a Camera.Parameters class that is used to change the parameters for the camera.   A parameter called FLASH_MODE_TORCH was implemented in API level 5 (Android 2.0). To turn the torch on, you simply set the camera parameter Camera.Parameters.FLASH_MODE_TORCH: (side-note, these code snippets are not production ready by a long shot) Camera mCamera; Camera.Parameters mParameters; //Get a reference to the camera/parameters mCamera = Camera.open(); mParameters = mCamera.getParameters(); //Set the torch parameter mParameters.setFlashMode(Camera.Parameters.FLASH_MODE_TORCH); //Comit camera parameters mCamera.setParameters(mParameters); To turn the torch off, set Camera.Parameters.FLASH_MODE_OFF Device Support Before using the torch, be sure that the phone supports it.   There are many reports of devices whose camera driver does not return FLASH_MODE_TORCH as a supported option.   Many of these devices can still turn the torch on but cannot turn it off without releasing the camera object. *EDIT* In my recent testing, I have come across at least one phone (Droid X) that has very poor torch implementation.  This is no doubt a bug somewhere in the camera driver which makes the flash cycle on/off when autofocus is engaged.  I have reported this to Motorola and hope to see it fixed in the future. Another caveat is that you cannot change camera parameters while autofocus is running.  In my mind, this exposes a huge deficiency in the Android API.  You see, there is no mechanism that tracks autofocus state in the API.  You have to set a flag when you call autofocus and clear it in the autofocus callback.  Oh, and don’t try canceling autofocus as a shortcut the autofocus callback will never get called and you will have an unknown delay before the asynchronous cancelAutofocus actually finishes (naturally there isn’t a callback for this either). A sample that checks for torch support might look something like this: ...inside some class //Create camera and parameter objects private Camera mCamera; private Camera.Parameters mParameters; private boolean mbTorchEnabled = false; //... later in a click handler or other location, assuming that the mCamera object has already been instantiated with Camera.open() mParameters = mCamera.getParameters(); //Get supported flash modes List flashModes = mParameters.getSupportedFlashModes (); //Make sure that torch mode is supported //EDIT - wrong and dangerous to check for torch support this way //if(flashModes != null flashModes.contains(torch)){ if(flashModes != null flashModes.contains(Camera.Parameters.FLASH_MODE_TORCH)){ if(mbTorchEnabled){ //Set the flash parameter to off mParameters.setFlashMode(Camera.Parameters.FLASH_MODE_OFF); } else{ //Set the flash parameter to use the torch mParameters.setFlashMode(Camera.Parameters.FLASH_MODE_TORCH); } //Commit the camera parameters mCamera.setParameters(mParameters); mbTorchEnabled = !mbTorchEnabled; } Controlling Torch Brightness The Camera API does not allow you to control the brightness of the torch mode.  I did some prototyping to test the possibility of controlling torch brightness by using pulse width modulation (PWM). Unfortunately, I was unable to toggle the duty cycle fast enough to maintain persistance of vision.  Even at 120 Hz and full duty cycle, the flickering was noticeable to the naked eye which means it was likely clocking in at under 60 Hz. I have to believe that this delay is either due to the overhead of the function calls or a hardware induced delay.  Needless to say, unless you have direct control to drive the LED, PWM is not a viable option for controlling the brightness level of the Torch. Torch Activation Criteria A secondary goal was the possibility of automatically enabling the torch based on certain criteria. Ambient Light It made sense that the light-sensor on the device would be a good fit to accomplish automated torch activation. Android devices have a built-in ambient light sensor, the current generation of devices have front-facing light sensors only.  An application can register to be notified when there is a change in light sensor data.  I wrote a prototype to display the current ambient light-level and then tested the lux accuracy in known lighting conditions. The light sensors in the devices I tested (Nexus One & a few Samsung devices) were not very accurate, the biggest issue is the level of quantization from the sensor.  The lowest discrete level below 160 lux is 10 lux. Because the API only notifies you when a change in light-level has occurred based on discrete intervals, this means that a reading of 10 lux followed by a reading of 159 lux wouldn’t produce a notification.  The indoor lighting conditions I am targeting are around 10-100 lux, clearly the sensor data is not sufficient in this scenario. Oh well, the sensor was on the wrong side of the device to begin with. Image Analysis I briefly considered using image data to control torch activation.   This would be prone to the same pitfalls that the camera world has been solving for the last 25+ years handling complicated scenes with odd mixes of dark or white colored objects (snow and dark-skinned people to name a few). User Activation Definitely the easiest option, just give the user the option to toggle the torch.   The concern here is whether there are a set of scenes where the torch would degrade the  reading experience. "},{"title":"How To Make Graphics Without Being An Artist","baseurl":"","url":"/2011/01/20/how-to-make-graphics-without-being-an-artist/","date":"2011-01-20 00:00:00 -0800","categories":["android","graphics","iphone","webos","wm7"],"body":"If you were to look at the following drawing, you might think it to be the doodles of a 4 or 5-year-old child.  I mean, what the heck is it?  Is it a crazy alligator crawling on a globe?  Is it a bat carrying a piece of fruit?  No and no, it’s clearly a dragon perched atop a magical ball and quite clearly the sad artistic maximum of this 31-year-old man. As an artist, I have clearly failed.  But where artistry has failed, geekery has prevailed.  Even with my lack of artistic ability, I am able to take the previous sketch and turn it into the below image: Why Make Your Own Art? If you’re a homebrew app developer or game maker then I am sure you have many projects that could use graphical assets.  I used to think I was incapable of making graphics, that was before I caught up with the latest wave of **FREE **rendering software.  If you aren’t convinced yet, let me make a small case for at least trying to create your own art. Autonomy You need artwork anyway, why not make it yourself?  Imagine if you knew you could whip out an awesome icon in an hour, could this take your homebrew to the next level? Balance The left side of your brain needs a break from all that coding.  Why not engage your creative side?  Even if you don’t use what you produce, you might find a fun hobby in the process. Geek++ As a counter to #2, rendering is incredibly geeky and technical.  In many ways it is like photography in that there is a mix of technical and creative. Free The software is free. Not convinced?  Ok, go back to watching American Idol. How To Get Started Use Free Software To make the final image above, I used two pieces of software.  These two products are amazingly easy to learn, have thriving communities behind them and are just plain cool! Daz Studio 3 You might call Daz Studio a figure design tool.  You load a figure, pose it, add lights and render.  For my image, I used a figure called Millenium Dragon.  Daz releases a free model every week and I was lucky to grab my dragon for free about a year ago. They have all sorts of pre-made assets you can purchase for reasonable prices as well.  From swords and shields to spaceships and laser guns. Daz also allows you to create primitives (Sphere, cube, cylinder, etc.).  I used a simple sphere with a combination of a subsurface stone and glass material to create the orb above.  You'd be amazed at how convincing a couple primitives can be with a texture slapped on top city scapes, spaceships, etc are quite easy to fake. Vue 9 Vue can be summed up by saying Digital Nature.  Many big name movies use Vue to generate landscapes and environments.  The free PLE edition (Personal Learning Edition) does put a visible watermark on your images. Vue also has a primitives system that is slightly more advanced than Daz Studio and  I've seen some amazing modeling done with these.  But most of the time, I could care less about primitives since I mostly use Vue to make backdrops. I used Vue to make the background for the image above.  I dragged 3 landscapes out, added some clouds, changed the color of the ambient light and moved the sun where I wanted it.  In all, the background took about 15 minutes to create.  Here it is by itself. Use Pre-made Assets I think there is a strong case for using pre-made assets.  If I had tried to model a dragon, honestly, I probably would have never finished.  If I had, it wouldn’t be anywhere close to the quality of the pre-made one that I got for free.  But that’s not to say that everything has to be free. There are a slew of pre-made assets that can be had for a few dollars.  In my mind, a few bucks was a small price to pay for these cool weapons that I used in one of my Android apps. One thing to keep in mind is that if your target image size is an icon, say 60 x 60.  You aren’t going to have much visible detail and can get away with using the cheaper pre-made assets. Use Science If you’re not an artist and don’t trust your artistic eye, use science to aid you.  Science has done a good job of breaking down art and creating formulas to help understand why some compositions and color combinations are pleasing to the eye.  Use this to your advantage! The Color Wheel Don’t do what I used to do cluelessly stare at a color palette wondering what the right color was.  There are tools to help, like Palette Builder which will build a scientifically proven palette of colors based on 1 color of your choice.  Also you might want to learn the basics about the 6 primary color schemes and what situations each one works best in. Basic Composition If you only learn 1 rule, learn the  rule of thirds .  If you have time and motivation learn the rest .  If you are having issues coming up with creative compositions on your own, head down and support your local comic book store comic books are full of interesting compositions. Have Fun From the guy that sketches worse than a 5-year-old, I hope that this gives hope to some of the artistically challenged coders out there.   Above all, I hope you enjoy yourself and maybe even find yourself a new right-brained hobby.  If you make any cool renders, send them my way! "},{"title":"Shiva 3D for Mobile Games","baseurl":"","url":"/2011/02/16/intro-to-shiva-3d-for-mobile-games/","date":"2011-02-16 00:00:00 -0800","categories":["shiva 3d","webos"],"body":"Why Use a Game Engine? If you are an aspiring game developer, you might have considered whether a game engine is right for you.  There are a lot of game engines out there and I’m not going to try and compare the qualities of Unreal Engine vs Unity vs Shiva. Maybe you’re like me and have rolled some subset of your own 2D or 3D engine in the past using SDL, openGL or XNA.  If you’re also like me, you probably have about a half-dozen or so projects that have been shelved because you either stopped having fun, got frustrated at one of the minutia of tiny details or realized that you did things horribly wrong.  Sounding familiar? When I have a game idea that I’d like to prototype, what I find most deflating is spending all of my time remembering how to initialize the screen, import textures, draw textured quads, etc.  I want to work on MY idea instead of recreating science. Why Shiva ? My reasons for choosing Shiva as a game engine may be different than yours.  Here they are: I want the simple things to be easy. I want to publish to as many mobile platforms as possible. I own a Palm Pre and want to prototype on it. Shiva is the only engine that meets these 3 requirements as of the time of this writing.  Also, there is a Personal Learning Edition that is free. Who Uses Shiva? The immediate person that comes to mind is PDK Hot Apps Winner Jopacus Parrott.  More recently the folks at Modern Alchemists used it for their game Aeon Racer.  Both of these are great examples of modern mobile games with unique styles made in the Shiva engine. What about 2D? Shiva is a 3D game engine but as you can see in Aeon Racer, it can also do 2D.  There are a couple different options .  I am planning a future set of tutorials on making 2D games in Shiva using a method similar to Aeon Racer.  Goodbye SDL and HTML5 Canvas. "},{"title":"Shiva 3D  Benchmarking for Mobile Performance","baseurl":"","url":"/2011/02/18/shiva-3d-benchmarking-for-mobile-performance/","date":"2011-02-18 00:00:00 -0800","categories":["shiva 3d","webos"],"body":"As I was meeting with my indie game team to discuss progress on a project, the topic of 3D models and performance came up.  How many vertices should our models have?  This seemed to be an incredibly valid question to which I had no answer. The following is how we determined our vertex budget. Starting a Vertex Budget Step 1. Assign ratios to your categories. We knew that our scenes would consist of Models, Scenery and Particles.  We assigned our budget as follows: 30% Models 65% Scenery 5% SFX ### Step 2. Benchmark the low-end target The Palm Pre was the low-end device that we wanted to target.  If we could find the number of vertices that this device could handle while maintaining 30 fps, we’d have all we need.  I wrote a simple benchmark in Shiva to do this, see below if you’d like to download the package. I primarily wanted to see the effects that texture size and realtime lighting had on our performance.  I’ll possibly add shadows in the future. Step 3. Interpret Results I ran my benchmark on an EVO 4G along with my overclocked Palm Pre Plus.  With no textures or lighting, the EVO 4G was pulling around 6000 vertices where the Pre was clocking in at 9000?!?  I scratched my head for a second before realizing that the EVO was running at a much higher screen resolution. Here are the graphs from my Palm Pre runs. Armed with these numbers, we calculated our final values using 6000 vertices as a target to give ourselves a little headroom: 30% Models 6000 vertices * 30% / 6 models on screen = 300 vertices per model 65% Scenery 6000 * 0.65 = 3900 5% SFX 6000 * 0.05 = 300 Get The Benchmark UPDATE: There has been some discussion about this benchmark over on the Shiva forums.  Some points mentioned were: The log..*() function kills performance and needs to be removed. Multiple textures would be more realistic Multiple lights desired Ability to choose light mode To disclaim some of my poor variable names, I have to say this benchmark embraces the essence of late-night feverish coding.  Also, as of now, the Shiva UI isn’t very forgiving about renaming functions and handlers so forgive me for not planning more. I welcome suggestions and considerations for this project, please let me know if you see something I could do better. Hopefully this will give you a starting point for your own vertex budget. Shiva Benchmark "},{"title":"Stories of Warning for HP (Palm)","baseurl":"","url":"/2011/03/16/stories-of-warning-for-hp-palm/","date":"2011-03-16 00:00:00 -0700","categories":["webos"],"body":" What are the consequences of a device whose mass storage driver is down-revved?  What dangers loom on the horizon for those that ignore these dangers?  It is these questions that I seek to answer through the use of two extremely realistic and likely scenarios. Update! I am incredibly happy to say that HP has saved the elderly as well as countless weddings.  The mass storage driver in the Touchpad kernel (2.6.35) works well with everything, including the previously problematic Xbox 360. o/ Story 1 Elderly Mass Storage Fail A 91-year old grandmother wakes up one morning in the room of her assisted living home.  She reaches for her glasses and finds a gift-wrapped box on her nightstand.  Like a giddy schoolgirl, she tears the wrapping off to find her first computer, it’s a Touchpad! The grandmother’s mysterious gifters have put an adorable video on the device that she wants to share with her elderly friends.  Because of the complexities involved with maneuvering multiple wheelchairs, it is decided to put the video on the big screen.  Luckily, the nursing home is equipped with a large flat-panel TV with a USB cable dangling off of it that says Plug cameras and USB drives in here. The grandmother plugs her Touchpad into the Mitsubishi TV via USB and waits for the TV to show her the list of videos that are on the device.  This always works when her friends plug in their $85 digital cameras, $10 usb thumb drives and portal hard drives.  The list of videos is never displayed because the mass storage driver has been neglected.  The grandmother and all of her friends waste a precious afternoon (possibly their last) cursing the complexities of technology in general and HP specifically. Later that evening, as the bewildered and stressed grandmother drifts off to her final rest, she sees her Touchpad sitting on her nightstand.  As she fades, her last thoughts are of her first computer and the gifters that would be so cruel as to give an old lady technology that doesn’t work. Story 2 Wedding Slideshow Fail The big day has finally arrived!  After surviving the chaotic pre-wedding photo session, the groom realizes that he has left his laptop at the hotel.  At hearing this news, the bride-to-be bursts into tears, What about the slideshow?!? The ceremony is ruined!. The fun uncle pipes up, explaining how he put a copy of the slideshow onto his Touchpad. What good will that do? blubbers the sulking bride as she circles the precipice of insanity. Trust me says the uncle as he gives her hand a reassuring squeeze. Once the bridal party arrives at the church, the uncle heads upstairs to the PA room.  The only accessible jacks to get to the projector and house speakers are Component video and optical audio.  The uncle swears under his breath, but in the corner of the room he spies a derelict Xbox 360, could it work? He hooks up the 360, fires up the projector and house speakers and is greeted with the Xbox logo.  In premature excitement, he plunges his USB cable into the device and enters mass storage mode.  Nothing happens.  He unplugs the cable and plugs it back in, silence. 20 minutes later, the bride has lost her grip on reality, the groomsmen are drunk, the cake has melted, the hand-whittled gazebo has been burnt to the ground and the groom and uncle are upstairs playing Black-Ops.  All of this because of a simple mass storage driver. Conclusion HP, after reading these 2 stories you have to ask yourself, how can we let this happen?  How can we punish the elderly and ruin weddings?  Well, one sure-fire way to do so is by not fixing the mass storage driver.  When the Touchpad is released and stories like this start pouring in, don’t pretend like you didn’t hear it from me first. "},{"title":"Using the Homebrew Javascript Service Framework","baseurl":"","url":"/2011/03/18/using-the-homebrew-javascript-service-framework/","date":"2011-03-18 00:00:00 -0700","categories":["homebrew","webos"],"body":" WebOS 2.x allows us to create services using node.js.  Services are great, but by default they run in a jail that prevents them from accessing the entire device.  For many services this is fine because there is no need to do anything outside of this jail.  On the other hand, there are some services that are only useful if they have root access.  To obtain root access, these services can use Jason Robitaille’s  Homebrew Javascript Service Framework for webOS , hereto referred to as HJSF. This article is going to explore the essential configuration requirements and validation steps for using HJSF in a node.js service. Requirements To correctly utilize HJSF, 3 requirements must be met. Requirement 1 HJSF Must Be Installed Get it from Preware or download the ipk from the google code project page. Requirement 2 Custom dbus File A file named **dbus **should exist in the root of your services source tree, at the same level as your service assistant.  The contents of the dbus file are as follows: [D-BUS Service] Name=APPID Exec=/var/usr/bin/run-homebrew-js-service /media/cryptofs/apps/usr/palm/services/APPID Where APPID is the id of your application, as an example, the dbus entry for my mass storage service (com.wordpress.mobilecoder.umst.service) would be: [D-BUS Service] Name=com.wordpress.mobilecoder.umst.service Exec=/var/usr/bin/run-homebrew-js-service /media/cryptofs/apps/usr/palm/services/com.wordpress.mobilecoder.umst.service Requirement 3 Install Scripts The primary purpose of the install scripts is to copy the dbus file to the proper location with the proper name.  The scripts handle copying the dbus file to /var/palm/ls2/services/pub and /var/palm/ls2/services/prv.  Be careful, your dbus file must be named APPID.service for this to work.  This means that if your APPID is com.joeblow.service the resulting dbus file must be named com.joeblow.service.service .  You can see how this might be confusing if your ending prefix is .service. APPID dbus filename Correct? com.joeblow.service com.joeblow.service WRONG com.joeblow.service com.joeblow.service.service CORRECT! Below, I have posted the 4 install scripts for my Mass Storage Tools Service .  You can use these as examples, I believe you’d be hard pressed to find a more barebones service to copy from.  I used Jason’s  SysToolsMgr Service as a reference to base my scripts on.  To be clear, if you replace the ID below with your app ID, the scripts will handle naming your dbus file correctly. prerm / pmPreRemove #!/bin/sh ID=com.wordpress.mobilecoder.umst.service #remount root using technique that won't cause the random remounting error if [ -z $IPKG_OFFLINE_ROOT ]; then /usr/sbin/rootfs_open -w fi #remove dbus service file /bin/rm -f /var/palm/ls2/services/prv/$ID.service /bin/rm -f /var/palm/ls2/services/pub/$ID.service exit 0 postinst / pmPostInstall #!/bin/sh ID=com.wordpress.mobilecoder.umst.service SERVICES_PATH=/media/cryptofs/apps/usr/palm/services/$ID if [ -z $IPKG_OFFLINE_ROOT ]; then /usr/sbin/rootfs_open -w fi #make directories in the rare event they don't exist /bin/mkdir -p /var/palm/ls2/services/prv /bin/mkdir -p /var/palm/ls2/services/pub #copy dbus service file /bin/cp -f $SERVICES_PATH/dbus /var/palm/ls2/services/prv/$ID.service /bin/cp -f $SERVICES_PATH/dbus /var/palm/ls2/services/pub/$ID.service exit 0 Validation Once you pull a basic service together, you will want to validate that you have root access.  One simple way to do so, recommended by Jason is to run the id system command.  To run this command, you will need to grab Jason’s CommandLine.js file from the SystoolsMgrService and add it to your sources.json. A fictitious service assistant that runs the id command might look like this: var MyAssistant = function(){ } MyAssistant.prototype.run = function(future) { //Using the commandline we can determine what ID we are running as. //You will need to print this value from your companion application this.cmd = new CommandLine(id, this.future); this.cmd.run(); } The desired output would show that you are running with a UID and GID of root. Stuck? There are plenty of places along the way to get stuck, if you find yourself stuck, just leave a comment with your issue and we can enrich the community with an answer. "},{"title":"Lab Notes  Hacking file_storage.c","baseurl":"","url":"/2011/03/25/lab-notes-hacking-file_storage-c/","date":"2011-03-25 00:00:00 -0700","categories":["homebrew","webos"],"body":"Progress Update I made attempt 3 to use file_storage.c as a means of presenting an ISO image on the Pre as a CD-ROM drive over USB.  I tried grabbing several of the more recent versions of file_storage.c from git.kernel.org in an attempt to back-port them to the 2.6.24 kernel. I found the following: * file_storage.c got CD-rom support in January of 2009: * <pre>vermagic: 2.6.32-28-server SMP mod_unload modversions parm: file:names of backing files or devices (array of charp) parm: ro:true to force read-only (array of bool) parm: luns:number of LUNs (uint) parm: removable:true to simulate removable media (bool) parm: stall:false to prevent bulk stalls (bool) parm: cdrom:true to emulate cdrom instead of disk (bool) </pre> At some point, part of the contents of file_storage.c was moved to storage_common.c.  From this point on, the backport seems more difficult. Compile Notes I proceeded by using the January 2009 version, there was really no work needed to compile this in place of the old version.  I did need to make one change.  By default, if you load the resulting module (g_file_storage), it will attempt to use the same sysfs entries as f_mass_storage.c - */sys/devices/platform/musb_hdrc.0/gadget/gadget-lunX. * Attempting to insert the module is very disappointing when this occurs.  To work around this, I changed the sysfs entry name by modifying the following line  in fsg_bind() from snprintf(curlun-dev.bus_id, BUS_ID_SIZE, %s-lun%d, gadget-dev.bus_id, i); to snprintf(curlun-dev.bus_id, BUS_ID_SIZE, %s-cdlun%d, gadget-dev.bus_id, i); This enumerates the lun entries starting with gadget-cdlun0 . Results The results are not pleasing.  I tried 3 tests: 1. Presenting an Ubuntu ISO from the Pre to a Windows 7 box. The OS detected the device as File-CD based storage gadget, took about 5 minutes to install the driver and then didn’t see anything presented.  I noted that if I rmmod’d the module, I would see a CD-ROM drive pop-up in windows explorer temporarily ultimately not helpful. 2. Presenting an Ubuntu ISO from the Pre to a computer on boot. The BIOS post confirmed that there was a File-CD based storage gadget plugged in.  I attempted to boot off the device and got the familiar message that my boot media was invalid. 3. Presenting an Ubuntu ISO from the Pre to a linux box. After plugging this in and watching /var/log/messages fly by, I saw that the system discovered the gadget.  It did take an unusual amount of time for the system to mount the device after discovering it (~20 second).  After the device was mounted, I had no problems accessing the contents of the ISO. Unknowns Is the Rockhopper driver muddying the waters?  It has been loaded for all of these tests just because I didn’t want to recompile my kernel to remove it. Should I just wait this out until 3.0 and a newer kernel? "},{"title":"ShiVa 3D  Making a 2D Laser","baseurl":"","url":"/2011/04/23/shiva-3d-making-a-2d-laser-attempt-1/","date":"2011-04-23 00:00:00 -0700","categories":["shiva 3d"],"body":" We needed a laser for our WIP space schmup.  This solution seemed to be the most obvious way to create a variable length laser.  I am hoping someone will post a comment telling me how un-optimized it is so that I can improve it How to Make a Laser Step 1 . Model and texture a cube whose origin is the center of an edge face. Above is the reference design I gave to my artist for this simple design.  It is important that the origin be on the edge to simplify the math involved.  It is unfortunate that ShiVa does not allow you to change the origin of shapes in the model view as it would have been trivial to make a cube and slap a texture on it. Step 2 . Scale the cube so that the length of the laser has a scale of 1. After applying a scale of (0.3, 0.001, 1), you can probably figure out how this is going to work. Step 3 . Program the laser. The basic structure looks like the below photo.  The main userAI passes mouse coordinates (nPointX, nPointY) to the laserAI when the user clicks or moves the mouse/their finger. We will explore the laserAI implementation below, I have crossed out some items that are specific to my implementation so that things don’t get overly complicated. The Variables hSat An object that will be firing the laser.  In my case it is a satellite, hence the name hSat.  The reason we need this object reference is so that we can position the laser in the scene.  This could just as easily be some fixed point in your scene if your camera never moves. mbFiring  A boolean that tracks whether the laser is firing. The Handlers onFireLaser(x, y) This is so simple, I don’t think I need to explain it.  Set our boolean to true and call the fireLaser function passing the x and y coordinates in. -------------------------------------------------------------------------------- function laserAI.onFireLaser ( x, y ) -------------------------------------------------------------------------------- this.mbFiring ( true ) this.fireLaser ( x, y ) -------------------------------------------------------------------------------- end -------------------------------------------------------------------------------- onStopFiring -------------------------------------------------------------------------------- function laserAI.onStopFiring ( ) -------------------------------------------------------------------------------- this.mbFiring ( false ) -------------------------------------------------------------------------------- end -------------------------------------------------------------------------------- onEnterFrame This is where it gets fun. Every frame we determine whether the laser should be drawn based on the boolean that is set.  Lasers shouldn’t just sit there and look pretty, they should pulse with power!  We make this happen by calculating the amplitude of the wave based on the elapsed time of the game. -------------------------------------------------------------------------------- function laserAI.onEnterFrame ( ) -------------------------------------------------------------------------------- if(this.mbFiring ( )) then --Cache the object handle local hObject = this.getObject ( ) --Set the laser to be visible object.setVisible ( hObject, true ) --Calculate the width of the laser based on a sine wave local amp = math.sin ( application.getTotalFrameTime ( ) * 2000 ) * 0.9 + 2.5 --Get the current scale of the laser so we can change the width only local sx, sy, sz = object.getScale ( hObject ) object.setScale ( hObject, amp, sy, sz ) else --Set the laser to be invisible since we aren't firing local hObject = this.getObject ( ) object.setVisible ( hObject, false ) end -------------------------------------------------------------------------------- end -------------------------------------------------------------------------------- A simple breakdown of the sine wave is: math.sin ( application.getTotalFrameTime ( ) * 2000 ) * 0.9 + 2.5 math.sin I’m not stupid enough to think I’m going to sum up sine waves in one bullet point.  For the purpose of games, it is helpful to think of the standard arguments of the sine wave using the definition sine ( w * t) where: t is time w (omega) is 2 * PI * f f is the frequency that the wave is oscillating at application.getTotalFrameTime ( ) This provides the total elapsed frame time in seconds since the start of the game.  It is used for time in the equation. 2000  is the w or the 2 * PI * f component of the wave.  Because I don’t want to waste CPU time calculating 2 * PI * f every single frame, I just mooshed these together into a single number.  If you do the math you’ll find this frequency is about 318 Hz, not magic by any means, just what I thought looked good in the ShiVa editor. 0.9 This scales the amplitude of the wave.  Essentially it determines how much to exaggerate the pulsation of the laser.  Higher numbers result in more pulsating. 2.5 This sets the fattiness factor of the wave.  Try a value of 10 and you will quickly see! fireLaser This is where we actually calculate where the laser gets drawn. -------------------------------------------------------------------------------- function laserAI.fireLaser ( nPointX, nPointY ) -------------------------------------------------------------------------------- --See Note 1 --Find the target z value for the unproject function local rx, ry, rz = camera.projectPoint ( application.getCurrentUserActiveCamera ( ), 0, 0, 0 ) --convert the tapped screen coords to global 3d space local x, y, z = camera.unprojectPoint ( application.getCurrentUserActiveCamera ( ), nPointX, nPointY, rz ) --Get the location of the satellite local satx, saty, satz = object.getTranslation ( this.hSat ( ), object.kGlobalSpace ) --See Note 2 --Find the distance from the satellite to the point clicked, this is the length of the laser * 2 --Note my wanky coordinate system, all objects are constrained to Y=0 local distance = math.vectorLength ( math.vectorSubtract ( satx, 0, satz, x, 0, z ) ) --Set the translation of the laser to the position of the satellite. This is easy thanks to the origin --point of the laser! object.setTranslation ( this.getObject ( ), satx, 0, satz, object.kGlobalSpace ) --set the scale of the laser to the distance calculated above, being sure to preserve the other scale values --that are being modulated in onEnterFrame local sx, sy, sz = object.getScale ( this.getObject ( ) ) object.setScale ( this.getObject ( ), sx, sy, distance/2 ) --face the beam towards the tap point object.lookAt ( this.getObject ( ), x, 0, z, object.kGlobalSpace, 1 ) -------------------------------------------------------------------------------- end -------------------------------------------------------------------------------- Note 1 : We received x and y coordinates for where the user clicked. These coordinates are in Screen Space which range from -1 to 1. We need to convert these coordinates to global 3d space.  This is called projection , specifically we want to project the X, Y mouse coordinates into the 3D space of the current camera.  The one gotcha when doing this is that you have to specify how deep within the scene you want to project these points. The easy way to determine this depth is by picking a known location in 3D space and projecting a point to it.  This will give you the depth of that location in space.  I chose (0,0,0) because depth in my scene is determined only by the Y-axis my camera is constrained to look directly down on the Y-Axis.  Once   rz is obtained, we can turn around and unproject a point using the mouse coordinates + our depth to get the global 3D space coordinates of the tap. Note 2:  The length of the beam is simply the distance between the thing shooting the beam and the point where the user tapped.  We can easily find this distance by finding the vector between these 2 points and then getting the length of that vector.  Note that when we set the scale of the laser we need to divide this distance by 2. That’s It! If you have done a similar effect, please show me!  If you know of a less-involved way of drawing a laser, please share! "},{"title":"Diary of an Indie Game Developer","baseurl":"","url":"/2011/07/03/diary-of-an-indie-game-developer/","date":"2011-07-03 00:00:00 -0700","categories":["shiva 3d"],"body":"Dear Diary, A little over 3 months ago, my brother and I were sitting around the man-cave throwing ideas around.  We were strongly impressed by the concept of a game like Angry Birds only in outer space where orbital physics would be used to slingshot projectiles around planets.  After much sleep deprivation, school-girl giddiness and white-board scribblings, we embarked on the journey of indie game development. I’ve always wanted to make video games, my code-closet can attest that I’m great at starting them and chronically horrible at finding the motivation to finish them.  For the first time though, I am not alone in this effort.  So far this has been one of the most challenging and enjoyable projects of my life. At times it feels like the project will never end, sometimes it even feels like we make negative progress, but bit-by-byte it is starting to look like an actual game.  I only hope that when finished that it’s half-as-fun as our original vision. Scheduling Life & Indie Games There are 3 contributors to this game, which I shall refer to using the shop title Angry Satellites (AS for short).   My brother is a full-time student at Portland Art Institute studying game graphic design, he also has a 2-year old; he does all the modeling, graphics and figures out cool particle effects.  My college friend Matt works full-time as an electrical engineer, he solves our more difficult math problems and in-general helps out with code. We’ve all got pretty full days with work/school/family during the week, so this has been a project that we typically work on once per week together.  Typically Sunday is our big day where we all huddle together in my office which reaches sweat-shop temperatures with 3 computers/warm bodies.  I take public transit to work on most days which gives me 2 hours of code-time, but after 8 hours of coding at work, you can only take so much. Project Planning Like most projects, when you start out with the initial concept, you think gee, this will be easy.  We figured it would take about a month to finish the game.  Lol!  In retrospect, I believe the biggest challenges and setbacks were due to poor planning.  Let’s just say that our game design strongly adheres to Agile software development practices no Big Design Up Front.  This, I believe was a folly. When you don’t know what exactly you’re working on, you get easily side-tracked into making things that might not have any place in your game.  Often we would get to a point where someone would say, Well, I finished implementing XYZ? and someone else would say I thought we were doing UVW instead and then we’d have to sit down and figure out what in the world kind of game we were making. When you do the above a half-dozen or so times, well, it can get old.  Now I understand why every game design book I have read begins by explaining how to write a game design document. Technical Decisions It was decided early on that we would use Shiva as the game engine and that we would target mobile devices.  The decision to use Shiva was mandated by myself because from my research, Shiva had the widest range of platform support, the best pricing model and a completely free editor to test until we were ready to publish.  I had been learning Shiva for a couple months prior and felt ready to take on a full project. On the graphics front, Blender and Gimp, that’s it.  I *think* most of the texturing has been offloaded to the Shiva side since we are doing a lot of UV animation.  You can see a few production tutorials on a low vert spaceship along with some mothership models . Game Details What is unique about our game is that using your phone, you are actually in direct control of a real satellite.  When you tap the screen to fire your Gatling gun, you are firing a real Gatling gun in outer space at real aliens.  Oddly enough, spaceships and projectiles in outer space all tend to align themselves on the XZ plane, this lends nicely to displaying the battlefield from a top-down perspective where movement is constrained on the Y-Axis. There’s really almost too much to talk about here.  In my next diary entry I will include videos, screen-shots and a deep-dive on some of the more technical aspects of the game.  Including: Drone AI navigation implementation Dynamics vs Translation for movement Camera AI system Projectiles vs Raycasting for projectiles Rocket orbital physics Explosions & Particle systems "},{"title":"Designer Diary: Shiva AI Navigation  Dynamics vs Translation","baseurl":"","url":"/2011/07/04/designer-diary-shiva-ai-navigation-dynamics-vs-translation/","date":"2011-07-04 00:00:00 -0700","categories":["shiva 3d"],"body":" AI Navigation Dynamics vs Translation We got to the point where we wanted to start moving the mothership around, again using a setTranslation model meant that we had to figure out equations to get it to a specific location on screen.  We did learn a lot about splines and I was happy that Shiva accomodated us on our fools errand by providing functions to evaluate BSpline, Bezier and CatmullRom. I call this translation model the Infinite Momentum Model because we were trying to use the dynamics system for collisions, but as you can guess, setting a translation every frame completely breaks the collision system.  For collisions to work, the dynamics system needs the opportunity to allow the results of the collision to take place. By setting translation every frame, you are overriding the result of the collision as if your object had infinite momentum. In game, this manifested itself by drone ships colliding with and pushing planets off the screen. . . Our model was clearly broken for a few reasons: We spent more time arguing over the correct pronunciation of Bezier than we spent implementing it We were unable to use the collisions system Each navigation destination required hand-calculated math and difficult spline creation for arcs Overall the system was inflexible New AI Implementation The new implementation was discovered by a question on the game development stackexchange site .  One of the answers linked to a paper by Craig W. Reynolds titled Steering Behaviors For Autonomous Characters .  For anyone looking how to implement AI steering, this is the gold mine.  The AI behaviors implemented from this paper were: Arrive Avoid Seek Flee Evade Offset Pursuit Pursuit We also implemented some higher-level behaviors that simply mix the lower-level behaviors together, like OffsetPursueFireAvoid to do a shooting flyby.  In this video you can see an example of the Arrive and Pursuit behaviors.  The Red ship is being controller by a mouse click while the green ship is pursuing. This video is an example of OffsetPursueFireAvoid.  Also at the end, you can see an example of OffsetPursuit being used to create a formation of drones. Shiva Specific Implementation of AI Steering I wanted all AI behaviors to have a self-preservation priority, meaning they don’t just crash directly into the planet if it lies in their path.  To accomplish this we used the following basic flowchart. One disadvantage to this behavioral flow is that you get bouncies.  Sometimes when close to an obstacle, the drone will bounce between Avoid and another AI behavior quite rapidly.  I thought there might be an advantage to building some hysteresis into this flowchart but that is on the back-burner. You’ll notice in the first video above that I begin in debug mode and that you can see the sensors attached to the ship.  Our drone ship has 2 sensors with unique sensor IDs A body sensor A long navigation sensor The navigation sensor is what protrudes from the front of the ship and alerts the ship of an impending collision.  Right now, we have the drones avoid only the planet, but we could easily add other things for them to avoid (like each other).  The length of the navigation sensor is supposed to be a function of the max velocity of the object.  We didn’t implement this dynamic sizing because modifying the sensor size and offset programatically sounded problematic.  Instead, to determine the required length of the sensor we followed a 3-step process. Before looking at the process, it is important that your object has mass set correctly in the dynamics controller. The default mass is something like 75.  Because we are dealing with planets and motherships, we setup our mass scale like this: drone laser 1 drone 5 rocket 10 sat 50 mothership 1000 planet 9999 The process to determine the required sensor length is then: * Choose the maximum velocity of the drone.  This is based on what feels right, too fast, too slow, just right. * Choose the maximum turning force of the drone.  Again based on how good it feels, the drone was either turning in too large of arcs or was turning far too quickly. * Set the sensor length so that the drone has time to avoid collision when approaching a planet from dead center at maximum velocity. If I ever have to go back and tweak this, I might just take a try at the dynamic sizing model. I am incredibly pleased with how simple this AI model is to use.  I have overloaded handlers in the model so that I can simply say onSeek(this.someTarget) or onSeek(x, z). "},{"title":"Design Diary: Shiva 3D  Camera Implementation for 2D in 3D","baseurl":"","url":"/2011/07/07/design-diary-shiva-3d-camera-implementation-for-2d-in-3d/","date":"2011-07-07 00:00:00 -0700","categories":["shiva 3d"],"body":"This post is part diary and part tutorial. I will cover: 2D in 3D camera basics Camera view extent detection Fitting the camera view to contain certain objects Special FX 2D in 3D Camera Basics ### Field of View One of the first things you should consider for your camera that is often overlooked is your field of view (fov). If you take the defaults, you will be set to 35mm. Here’s why this is important, the higher your FOV is, the more distorted objects will be that are on the edge of the screen. See these screenshots for an illustration. FOV=40 The objects at the edge of the screen are stretched. FOV=40 Objects in the center of the screen appear normal. FOV=22 Objects on the edge of the screen are not stretched as much at lower FOV. One reason why you may want to lower your FOV is that it gives an improper sense of perspective in a 2D game. A high FOV may make it appear like an object is underneath another object when it is not. It may also give the user a false sense of distance on the edge of the screen, this is particularly relevant if your playfield wraps (an object goes off the screen to the right and appears on the left side of the screen. We chose a fov of 22, it had the right amount of depth without making planets look on the periphery look like ovals. Up Axis You need to decide on the orientation of your world. This is personal preference and getting slightly into holy war territory. The common options used are: No movement on the Y axis: The entire playfield is on the X-Z plane. No movement on the Z axis: The entire playfield is on the X-Y plane. We chose the first option really for just 1 reason. In Shiva, there is a function** object.lookAt()** which allows you to have an object look at a position in space. For instance, you might have a target defined and you want your drone ship to look at the target, you might do something like this in the drone ship AI: --Get the position of the target local tx, ty, tz = object.getTranslation(this.hTarget(), object.kGlobalSpace) --Make ourself look at the target object.lookAt ( this.getObject ( ), tx, ty, tz, object.kGlobalSpace, 1 ) The lookAt function orients an object so that the objects -Z axis points at the given coordinate with the +Y axis being up. Take a look at our drone ship model and imagine that the target is in the direction of the squiggly red arrow. Shiva also provides a lookAtWithUp method where you can define what the Up vector is. I say minimize confusion after writing a couple thousand lines of code you might rest easier knowing you didn’t screw up an Up vector somewhere. Camera View Extent Detection For many 2D games, it is vital to know when an object hits the edge of the screen. For instance, in our game, you can only fire a single rocket at a time. If the player misses a target, the missile flies off into space. We detect when the missile hits the edge of the screen and destroy it so that the player can fire another. There are many ways to do this. Let me briefly describe the common solutions and then explain which one we chose. Render Extents This solution is a simple one that is low precision but may fill certain needs. The idea is to get the bounding sphere representation of each object you are interested in checking and seeing whether that bounding sphere is inside the camera’s frustum. object.getBoundingSphereRadius(..) and object.getBoundingSphereCenter(..) would be used to get the bounding sphere of the object and the check would be done with camera.isSphereInFrustum ( hObject, x, y, z, nRadius ) . The reason I say this is a low precision method is that the check only tells you whether or not the camera can see the object. It doesn’t tell you whether the object collided with the right/left-hand side of the screen. Depending on your scene, you could probably calculate this without much trouble. One issue with this solution is not being able to easily spawn object soutside of the viewable scene. For instance if you wanted to have enemies flying into the scene from outside the viewable area. Absolute Position Checking A brute force method is to calculate the translation of the scene extents based on the current position of the camera. Once you know the global space coordinates of the extents, you can loop through objects of interest and see whether they exceed these limits. This is computationally expensive because you are checking every object every single frame. Also, if your camera is moving, you have to recalculate the global space coordinates every frame as well. Extent Sensors We created a custom solution that was more Shiva-centric, in retrospect the solution seems so obvious that I’m sure others have done the same. The basic idea can be summed up by the following photo: As you can see, we have created a boundary around the edges of the scene. The model that we use for the boundary is nothing more than a 1x1x1 cube with a collision sensor on it, note the origin of the object. Keep this in mind for the following code, we used this origin so that when we scale the object, it grows in one direction instead of growing in two directions (like it would if centered at (0, 0, 0). When the camera is created, we create 4 of these boundaries and position them with the following code: --Get a reference point at y = 0, this is where game objects will be --colliding with the edge of the screen local rx, ry, rz = camera.projectPoint ( this.getObject ( ), 0, 0, 0 ) --Find the coordinates for the edges of the screen. This is made easy by --projecting points from screen space to global space. Screen space ranges --from -1 to 1 i.e. the top-right of the screen is (1, 1), bottom-left is (-1,-1) --top right local trx, dc, trz = camera.unprojectPoint ( this.getObject ( ), 1, 1, rz ) --top left local tlx, dc, tlz = camera.unprojectPoint ( this.getObject ( ), -1, 1, rz ) --bottom right local brx, dc, brz = camera.unprojectPoint ( this.getObject ( ), 1, -1, rz ) --bottom left local blx, dc, blz = camera.unprojectPoint ( this.getObject ( ), -1, -1, rz ) --Set the translation of our 4 boundary objects so that we can scale them to cover their --respective edges. object.setTranslation ( this.mhBoundaryTop ( ), tlx, 0, tlz, object.kGlobalSpace ) object.setTranslation ( this.mhBoundaryBottom ( ), blx, 0, blz, object.kGlobalSpace ) object.setTranslation ( this.mhBoundaryLeft ( ), tlx, 0, tlz + ((blz - tlz)), object.kGlobalSpace ) object.setTranslation ( this.mhBoundaryRight ( ), trx, 0, trz + ((brz - trz)), object.kGlobalSpace ) --Set the scale of the objects so that they cover the width or height of their screen edge object.setScale ( this.mhBoundaryTop ( ), trx-tlx, 1, 1 ) object.setScale ( this.mhBoundaryBottom ( ), brx-blx, 1, 1 ) object.setScale ( this.mhBoundaryLeft ( ), 1, 1, tlz-blz ) object.setScale ( this.mhBoundaryRight ( ), 1, 1, trz-brz ) In the past, I used some pretty ugly trig to calculate the screen edges, using the coordinate transformation is far cleaner. It is important to note that any FOV changes or Y-axis changes will require this to be recalculated. At this point, if you are never going to change the Y position or FOV of your camera, you could parent these objects to the camera and you’d never have to calculate their positions again. To extend this even further, you could set each boundary to have a unique sensor ID and then you’d know exactly which screen edge is being hit. Finally, be sure to set the visibility of the edge objects to false. Fitting The Camera View to Contain Certain Objects How do you zoom the camera so that it fits a set of objects? It turns out that my solution took me quite a a while to get working due to some of the trig. I am very eager to hear of a simpler model to do this. Here is what I do: Create an empty table in the camera AI Send the cameraAI a list of objects to zoom to (they are saved in the table) Loop over the table, get the bounding box for each object and calculate the minimum and maximum point for all of the objects. Based on the min and max point, create a helper object that is in the very center of the points. Calculate the Y value of the helper based on width/height of the screen and the camera FOV. Move the camera to the helper position Here is the relevant code: local hScene = application.getCurrentUserScene ( ) local nCount = 0 local hCamera = application.getCurrentUserActiveCamera ( ) local fov = camera.getFieldOfView ( hCamera ) local hFov = (2 * math.atan(math.tan(fov) * this.mnScreenWidth ( ) / this.mnScreenHeight ( ))) local width, height = 0, 0 local xMinFinal = 0 local xMaxFinal = 0 local zMaxFinal = 0 local zMinFinal = 0 local yMaxFinal = 0 --Loop over objects for i = 0, table.getSize ( this.objects ( ) ) - 1 do local xmin, ymin, zmin = object.getBoundingBoxMin ( scene.getTaggedObject ( hScene, table.getAt ( this.objects ( ), i )) ) local xmax, ymax, zmax = object.getBoundingBoxMax ( scene.getTaggedObject ( hScene, table.getAt ( this.objects ( ), i )) ) width = xmax - xmin height = zmax - zmin --Prime values if this is the first object if( nCount == 0 ) then xMinFinal = xmin xMaxFinal = xmax zMinFinal = zmin zMaxFinal = zmax yMaxFinal = ymax end --Find minimums and maximums xMinFinal = math.min ( xMinFinal, xmin ) zMinFinal = math.min ( zMinFinal, zmin ) xMaxFinal = math.max ( xMaxFinal, xmax ) yMaxFinal = math.max ( yMaxFinal, ymax ) zMaxFinal = math.max ( zMaxFinal, zmax ) nCount = nCount + 1 end --Add some padding for the scene extents, simply personal preference xMinFinal = xMinFinal - 10 xMaxFinal = xMaxFinal + 10 zMinFinal = zMinFinal - 5 zMaxFinal = zMaxFinal + 5 --Set the final distance of the camera yMaxFinal = yMaxFinal / 2 --Create the helper object if it doesn't exist if(this.mhZoomHelper ( ) == nil) then this.mhZoomHelper ( scene.createRuntimeObject ( application.getCurrentUserScene ( ), cameraHelper ) ) end --Calculate camera x, y, z local x = xMinFinal + (xMaxFinal - xMinFinal) / 2 local z = zMinFinal + (zMaxFinal - zMinFinal) / 2 --Calculate y values for fitting the camera height or width to the scene local yVert = ((zMaxFinal - zMinFinal) * 0.5) / math.tan ( fov ) + yMaxFinal local yHoriz = ((xMaxFinal - xMinFinal) * 0.5) / math.tan ( hFov / 2) --Fit whichever y value is greater - may not be the best choice for all situations if(yVert yHoriz) then object.setTranslation ( this.mhZoomHelper ( ), x, yVert, z, object.kGlobalSpace ) else object.setTranslation ( this.mhZoomHelper ( ), x, yHoriz, z, object.kGlobalSpace ) end Now you just have to move the camera to the location of this.mhZoomHelper(). Special FX Finally, one of the last components of the initial camera implementation was a simple screen shake. For this we simply set a stop time in onEnterFrame and then if the effect is running do something like this: local time = application.getTotalFrameTime ( ) local dx = math.sin ( time * 1000 ) * 0.5 local dy = math.sin ( time * 2000 ) object.translate ( this.getObject ( ), dx, dy, 0, object.kLocalSpace ) I believe I may have got this basic idea from one of the Shiva included tutorials. Our basic camera implementation has a few more frills that are specific to our game, but I’ve covered the fundamentals that I would carry over to all future 2D games. You can see our camera shake when destroying drones in this video. "},{"title":"WebOS Camera Patch: Advanced Camera Configuration","baseurl":"","url":"/2011/07/10/webos-camera-patch-self-timer-and-burst-frames/","date":"2011-07-10 00:00:00 -0700","categories":["homebrew","webos"],"body":"**UPDATE 2: **What was formerly the Self-Timer and Burst-Frame patch has been replaced.  To maintain compatibility with older camera patches, the new patch combines the following patches into a single Tweaks-supported patch: Video camera flashlight Capture with volume-key Shutter sound Self-Timer and Burst-Frames UPDATE: Did you come here wondering why the camera patch will not install?  Unfortunately, due to code ninjas, there are a few patches that this patch does not play well with the video camera flashlight patch and a few naming patches.  I am looking into things and am beginning to form a solution that I hope will clean up the camera patch ecosystem.  Stay tuned. Please leave comments below or twitter me if you have issues or a feature request for this patch. Usage Note: When the camera app first starts, a service call is made to query all of the Tweaks settings.  This service call can take a second or two to return at worst.  As a result, you may notice that your configured settings do not take affect until 1-2 seconds after the camera app has initialized. Changelist: Version 1.0.0 Self-timer enable/disable Self-timer custom time setting Burst frames enable/disable Burst frames custom frame count Burst frame shutter release time.  Note that this value may not work well below 2 seconds if you are using flash.  Even if you aren’t, I’ve had hiccups where the device sometimes misses a frame. Shutter-sound enable/disable Capture with volume keys enable/disable Video camera flashlight enable/disable "},{"title":"Design Diary  Shiva 3D Sensor Performance","baseurl":"","url":"/2011/08/11/design-diary-shiva-3d-sensor-performance/","date":"2011-08-11 00:00:00 -0700","categories":["shiva 3d"],"body":"The use of the proper sensor in Shiva is critical to obtaining optimal frame-rate. In the past, you may have hap-hazardly populated onSensorCollision without a second thought. This could cause serious performance issues down the road. The largest difference in the 3 sensor handlers are the circumstances under which they are fired. I’m sure you are familiar with these differences already, but if you aren’t here is a quick recap. onSensorCollision Called on every frame as long as sensors are colliding. onSensorCollisionBegin Called every frame that sensors are colliding as long as they weren’t colliding the previous frame. onSensorCollisionEnd Called every frame that sensors are not colliding as long as they were colliding the previous frame. You may be thinking, yeah dude I already know all of this but what you might not know is the minimum performance impact that these sensor handlers impose. The Benchmark I devised the following benchmark to measure this minimum performance impact. This benchmark can easily be reproduced by anyone, the steps to do so are as easy as 1-2-9: Create a blank project Create an empty scene with a camera Create a new shape (box) Create a new empty object AI Create a new empty user AI Create a HUD containing a label to hold the FPS In the user AI onEnterFrame, print the FPS In the user AI onMouseButtonUp handler, create a runtime object and print the current count of objects Count the number of objects that can exist while maintaining 30 FPS Because objects spawn in the center of the scene, all sensors will overlap. This results in the sensor handlers being called (if applicable). Below are the resulting data sets from these tests. The Tests I compared box vs spherical sensors, sensor quantity and onSensorCollision vs onSensorCollisionBegin/End. I ran these tests inside the editor with the preview mode set to Runtime (no debug objects being shown). This was ran on my 1.2 GHz netbook. The Baseline First I wanted to establish a baseline for sensor performance. This test was ran without any sensor handlers attached to the object. The performance can’t get any better than this. The baseline indicated that box sensors were less performant than spherical sensors (head scratch). onSensorCollision I then added an onSensorCollision handler and repeated the tests while varying the sensor count. Keep in mind that my onSensorCollision is doing *absolutely nothing*, it literally doesn’t have any code in it. The extreme performance difference here is due to the overhead involved in the sensors. onSensorCollisionBegin I then switched collision handlers to onSensorCollisionBegin and ran the same tests as above. Whoah! Huge difference! Clearly the reason why is that the sensor collision only hits the CPU a single time (when the sensors first collide) vs every single frame. These numbers are very close to the baseline, on average being ~15 objects off. There is still a bit of overhead, but clearly far better performance than the frame-by-frame situation. Takeaway The main items to take away from this data are: Spherical sensors seem to be more performant than box sensors Don’t blindly use onSensorCollision unless you actually need collision logic throughout the entirety of a collision. Using onSensorCollisionBegin/End will minimize the CPU impact and allow for a higher object count in the scene. "},{"title":"Design Diary  Shiva 3D Mobile Optimization","baseurl":"","url":"/2011/08/14/shiva-3d-design-diary-mobile-optimization/","date":"2011-08-14 00:00:00 -0700","categories":["android","shiva 3d","webos"],"body":"As we are narrowing in on the release of our game, we turned our attention on the game performance.  Our low end target is an Android HTC Inspire, mid-range is an HP Pre Plus and super high-range is an HP Touchpad. We were having an issue with 1 scene in-particular where on the Touchpad, we were consistently dropping to 15 fps. For a dual core beast, this was bad. This article is partly about the debug process and highlights 3 areas we found where we gained performance. <h1 class=\"more\"> Finding the problem area </h1> It was difficult to pinpoint the problem area.  The question I was trying to answer was whether we were cpu or memory starved.  I began my investigation by opening 2 consoles on the Touchpad, running top in one and executing the game in the other.  I discovered that we were pegging the CPU.  So I began to wonder: Do I have a runaway AI that is churning cpu?  Perhaps I’m forgetting to set something into the idle state. Do I have a log statement in one of my sensor collision handlers that is causing an I/O bottleneck? Do one of my hidden objects still have their dynamics engine enabled? Is my AI navigation algorithm too costly? Are the quantity of objects using dynamics simply too much to handle? Is my dynamics iteration count too large? So I began tearing things apart 1 at a time: Disable all AI in the scene Disable all dynamics in the scene Disable all sensors in the scene At this point I had a static scene with 3 objects in it and the performance problem persisted?! Then I started setting objects to be invisible in the scene, 1 at a time.  Here I discovered that a single 300 vertice object was the source of this scene’s problem. Textures The problem object had 3 textures assigned to it.  Each texture was 10241024. To make matters worse, these textures were completely flat colors that could be replaced by setting the diffuse color in the material lighting. By not using ridiculous texture sizes for things that werent even textures, we solved a major performance problem.  After seeing this, I browsed through our entire texture library and deleted any textures that were just a flat solid color.  I knew this would cause shiva to crash when a material using the deleted texture was used.  Anticipating this, I grep’d through the materials folder, found all materials using these textures and fixed them (removing the texture and changing diffuse lighting). Audio Performance We noticed that on our low end target, every time we had music playing or fired the gun, the framerate would drop by 10 fps.  Not only that but the max fps topped out around 18 horrible!  What we discovered is that audio can create a huge performance bottleneck on android.  There are 2 aspects to audio on android, the audio subsystem and the audio you are puting through it. First, we were using 44 kHz stereo mp3s for music and 44 kHz mono mp3s for sound effects.  In shiva, when you do sound.play , the whole audio file is thrown into memory.  You can imagine that if the file is big and memory is low, this could be a problem.  Alternatively, using music.play will stream the music rather than load the entire file into memory. Our first problem was the music, given the sample rate, this was simply pushing more bits into memory than the phone could handle.  We resampled the music down to mono 22 kHz and overall performance in the scene improved significantly!  However, this did not solve the slowdown we experienced when firing the gun (sound.play). After much debugging, we discovered that when you export a game in Shiva, you are given the choice of which audio sub-system to use, OpenAL or Android.  At this point, all I know is that OpenAL produces HORRIBLE performance on the HTC Inspire (Android 2.2) Sensor Performance After more trial and error testing, I discovered that having approximately 15 overlapping sensors firing was pegging the CPU.  This surprised me because for testing, I had short circuited the sensor handlers so that they returned immediately.  So just the overhead of the sensors being called was destroying performance.  This was due to using onSensorCollision where we really didn’t need to be, see my sensor post for more details. Object creation and deletion The final area that we found issues was dynamic runtime object creation and deletion.  We would notice a slight pause every time and enemy was destroyed because for every enemy death, we need to create an explosion object.  On the low end target , we also noticed that spawning a group of X drones would cause the same blips after around the 5th drone. I thought that just by forcing models to stay loaded that I would work around these issues, but this didn’t seem to be the case.  I solved this problem by creating pools of objects and some simple object allocation/deallocation handlers that initialize and return objects to the pool.  I like this architecture because now I can set the limit to the number of decals or ships in a scene.  With object pools, you take the hit at the beginning of the scene instead of at the time of creation. "},{"title":"Design Diary  Shiva3D Screen Boundary Detection in 2 Steps","baseurl":"","url":"/2011/08/24/design-diary-shiva3d-screen-boundary-detection-in-2-steps/","date":"2011-08-24 00:00:00 -0700","categories":["shiva 3d"],"body":"In a previous design diary , I talked about screen boundary detection.  What is an easy way to determine when an object has left the screen so that you can act on it? I discovered a new solution  that is incredibly cheap and easy with a few caveats: You will not know which edge of the screen the object has passed (so it won’t work for wrapping objects around on the screen) You only get one notification when the object moves beyond the camera (unless you setup a timer) How to Implement in 2 Steps 1 . For each model that you want to get boundary notifications for, open the model properties and under general, check Frustum Activation. 2 . In one of the AI models attached to your object, implement the onDeactivate handler. Now any time an object with this AI leaves the screen, onDeactivate will be called.  Similarly, onActivate is called when the object enters the screen.  For us, this made destroying rockets at the screen boundary as easy as: -------------------------------------------------------------------------------- function whistlePeteAI.onDeactivate ( ) -------------------------------------------------------------------------------- scene.destroyRuntimeObject ( application.getCurrentUserScene ( ), this.getObject ( ) ) -------------------------------------------------------------------------------- end -------------------------------------------------------------------------------- "},{"title":"Play mkv, avi, flv and wmv files on webOS Touchpad","baseurl":"","url":"/2011/08/26/play-mkv-avi-flv-and-wmv-files-on-webos-touchpad/","date":"2011-08-26 00:00:00 -0700","categories":["homebrew","webos"],"body":" This article is no longer maintained, please see TouchPlayer Documentation . One thing the touchpad clearly lacks is the number of supported video formats available. Out of the box, the mp4 container with h.264 codec is pretty much it.  If all of your videos are in this format already, you can benefit from the hardware acceleration that the stock video player provides for this codec. If you want to play any other format, you have a few options: Convert your existing media to mp4 h.264 with an app like Handbrake . Leave it running overnight to take advantage of spare CPU cycles. Run a media server like Playon that transcodes media on the fly.  You need a reasonably fast cpu to do this (atom powered servers do not apply) as you are converting and serving the video real-time. Get a new video player for your touchpad. This article is going to explore the 3rd option in more detail. Presenting TouchPlayer I have taken the FFmpeg project, thrown a crappy wrapper on it and compiled it for the Touchpad.  If you aren’t familiar with FFmpeg, the library that comes from it (libavcodec) powers mplayer and vlc.  You can find my app in Preware, it is called TouchPlayer. TouchPlayer Limitations Update 10/16/2011 1.0.4 is now available . Update 10/14/2011 First off, let me say that if you are looking to play MKV files, you should really check out KalemSoft Media Player.  I use it, it’s fast, it’s beautiful etc. There are limitations with KalemSoft though, such as reading files from an NFS/CIFS/SSHFS mounted filesystem.  Also, no subtitles. I have abandoned the ffplay version of this project in favor of mplayer.  Progress is steady, you can read progress updates if you’d like.  An update has not yet been pushed, the mplayer version is all or nothing, meaning if I released it as-is there would be no controls. Update 9/23/2011 It’s possible that I don’t know what I’m talking about below.  KalemSoft media player can play most of my HD MKV files quite well.  The big difference here is that they seem to draw to the screen using openGL rather than SDL.  This may not be the only factor but I’m sure it is a large factor. As ffplay is something of a dead-end, I am now working on replacing it with mplayer.  There are a couple reasons, first there is already an openGL ES patch for mplayer from either WOSI or Chomper (hard to track the attribution for the patch).  Also, mplayer has an OSD and a GUI with several skins available. The project is building but I haven’t had the time to test it on the Touchpad.  You can find the source on my github page if you are eager. Old Content TouchPlayer cannot play 720P mkv videos with AC3 audio at 30 fps.   It never will unless the Touchpad gets more hardware accelerated codecs.  Who knows, maybe it will someday. Keep in mind that every video played in TouchPlayer gets decoded by the CPU.   Decoding 720p with multi-channel audio on the CPU is too big of a task for any tablet.  If your primary goal is to play unmodified mkv files then you may want to consider new goals.  Using NEON, it is possible to do, all on the CPU. If you insist though, I have provided a switch to disable audio and increase thread count to take advantage of both CPU cores.  If you do have any level of success with playing hd videos, please let me know. Also if you are an expert in the FFmpeg ecosystem and know of some parameters that might give a boost, please pass them along. Source for the modified FFmpeg is here: https://github.com/error454/FFmpeg Source for TouchPlayer is here: https://github.com/error454/TouchPlayer Precentral Forum on TouchPlayer: http://forums.precentral.net/webos-homebrew-apps/293654-touchplayer.html Direct link: http://t.co/gnKEkkt "},{"title":"Note to self: Port mplayer to Touchpad","baseurl":"","url":"/2011/09/25/note-to-self-port-mplayer-to-touchpad/","date":"2011-09-25 00:00:00 -0700","categories":["homebrew","webos"],"body":"I have been working on porting mplayer over to the Touchpad.  When I work on projects, I write a note to myself at the end of the day to remember where I left off.  This is a copy of those notes. 1:15 AM 11/6/2011 I have just release version 1.0.6.  Talk about an incredibly painful release.  I got sidetracked with trying out mplayer2 vs mplayer and finally settled on sticking with mplayer.  It seems like the initial push for using mplayer2 was better matroska support, this was owed in large part to the ffmpeg-mt experimental branch.  At this point, the master FFmpeg branch has merged in the goodies from ffmpeg-mt so I didn’t see much advantage in using mplayer2.  Add to that the odd play/pause issues I was having and it was an easy choice to move back to mplayer. I got the freetype libraries in order which means no more bitmap fonts.  Now freetype fonts are available, so everything in /usr/share/fonts can be used. I was hoping to find some more enhancements for MKV playback.  I keep meaning to do a full gprof build of libavcodec to see where cycles are being spent.  From the little benchmarking I’ve done, I believe that most of the CPU cycles are being spent on  audio.  At this point, you can play an MKV with audio disabled and subtitles on and it will play at full frame rate.  This wasn’t possible before the NEON color space conversion. Sure there are lots of UI enhancements that could be done, but I’m actually kind of tired of working on this project.  I’ve fully opened up the API and everything is open source to begin with, so if folks want more flash and features they can easily add them.  Until I encounter serious issues watching movies on my Touchpad, this project is hitting the back burner to simmer for a bit. 6:00 AM 10/16/2011 I have released version 1.0.4.  This actually came together quite quickly towards the end.  I was digging around the xbmc code and found a NEON accelerated YUV to RGB conversion function (I think this was pulled out of the chrome source), it makes MKV files play a lot smoother.  Still, not all of mine play perfectly. I got the basic swiping/tapping controls in, still need to play with the sensitivity. Now the big problem is the 1000s of command line variations.  How do I choose what to put into the GUI?  It seems like a task bound for defeat.  I’ve had the flu the last few days and in my feverish state I considered writing a web-based mplayer configuration file generator.  Must be the sickness talking. Having issues with the commandline launcher, sometimes I can’t launch, sometimes threads don’t get cleaned up.  Pinged @jaycanuck to see if he has done any work on this, may need to look at how WOSI launches Xecutah. 10:10 PM 10/14/2011 I now have subtitles and OSD working! The only oddity with the built-in matroska subs is that I have to hit j’ to select the first subtitle index. This is annoying since there is only 1 available subtitle and none of the command line options seem to force this. I spent a few minutes digging into the command and input stuff to determine how to call this manually. I didn’t reach the end. Another thing I need to determine is how to package up the fonts, not just technically but also legally. I downloaded a packaged font pack for mplayer and extracted it to /media/internal and simply point mplayer to the font.desc (-font font-arial-cp1250/font-arial-18-cp1250/font.desc). I’d like to implement behavior where a single tap pauses/resumes, swiping right/left scrubs, perhaps also a 2 finger swipe? Perhaps swiping up/down can enable/disable subs. 9:01 PM 10/7/2011 Alone in a dark room, my keyboard illuminated only by the green characters of my VI session, I code deep into the night.  Tappity tap tap, click. . . I lean back and let out a sigh as I watch the compiler fill my screen with garbage.  Up enter, up up enter, tab up enter, I’ve repeated the process so many times now I almost do it without thinking.  The screen goes blank on my Touchpad and I prepare for another round of dissapointment as mplayer launches. I AM SPARTACUS!!! the cry explodes from my Touchpad as bright vivid 720p erupts from my screen.   I try to shield my eyes from the sharp crisp image, it’s so beautiful, so beautiful that I can’t look away!  Hey wait, why the hell is it all choppy? Theatrics aside, I’m not in a dark room coding late into the night.   I’m sitting at my evenly lit kitchen table laughing at my 2 year old niece, munching on leftovers and coding during the lulls in activity.   I’ve just gotten my YUV shader to work properly but it’s no faster than doing the conversion in software, why? After thinking and reading, one theory is that the whole pipeline is just too slow when using GLES2.   The problem is that you have to send 3 textures to the graphics card before conversion can be done.   So you get these 3 texture arrays (Y + U + V) which the CPU has to load into memory, then to use the shader for color conversion you have to copy these textures (again using CPU) from system memory to the GPU. It seems that whatever time was saved by doing the color conversion in the GPU is more than made up for by the time it takes to load the textures into memory and copy them to the GPU.   Ideally we could short circuit this process by loading the textures into a Pixel Buffer Object, this is memory that is allocated directly on the GPU, then the copy would essentially be free, I mean the data is already there right? Unfortunately GLES2 does not support Pixel Buffer Objects :’( How much data are we talking about here?  Let’s see: Y: 1280 x 720 bytes = 900 kB U: 640 x 360 bytes = 225 kB V: 640 x 360 bytes = 225 kB So every frame is 900 + 225 + 225 =  1.3 MB At 30 frames per second this is 1.3 * 30 =  39 MB / second But hold on, is there really that big of a difference between copying 900 kB per frame vs 1.3 MB?  Really?  Is it really that much more work for the CPU?  It seems hard to believe.  I mean, any drawing method requires copying at minimum 900 kB to draw to the screen.  I really don’t know, I suppose I would need to capture some performance metrics. On the bright side, I now completely understand how different video formats are stored and how to write a fragment shader for color conversion. The question is what is next? How can I improve video performance knowing what I do? Well first of all, I am going to stick with GLES2, but I’m going to try and avoid doing the color conversion with it.  The only real options for drawing to the screen are SDL or openGL, both drawing operations require that you copy what you want to draw from 1 place to another. So long as I’m only copying 1 RGB texture, whether I copy it to a shader or blit it to a surface doesn’t matter. GLES2 still gives more flexability for doing post-processing, so that’s what I’m going with.  I keep circling back on not being able to believe that things are as slow as they are, I really need to capture metrics. What if I use NEON extensions to perform the color conversion, this would let me exploit SIMD without the overhead of the copy.  Assuming that the theory on the bottleneck is true. 8:46 PM 10/6/2011 I posted a question on dsp.stackexchange.com for help with my YUV conversion. It turns out that I was not taking the image stride into account. Microsoft has a great article on what image stride is (http://msdn.microsoft.com/en-us/library/windows/desktop/aa473780(v=vs.85).aspx) basically it is extra padding stored after the pixels. My guess is that it makes storing the frames in memory more efficient. Inserting a print debug into the draw_slice function for a YV12 video, it printed the following continuously: draw_slice: strideY: 1312 strideU: 656 strideV: 656 x: 0 y: 0 w: 1280 h: 720 So the textures I’ve been mapping to the fragment shader have been the wrong size all along.   Also, I was under the impression that draw_slice() requested drawing of video frames in slices, like a grid a tiles that would be updated as needed.  Here I can clearly see that every draw call is drawing the entire frame.  Perhaps this is codec specific but the only difference between draw_slice() and draw_frame() from this perspective is that you get the stride in draw_slice(). The current solution that I need to achieve is the cropping of the extra stride pixels in the video frame given to draw_slice(). More research, I found a GL parameter GL_UNPACK_ROW_LENGTH to specify the pixel format (stride/pack) of image data. Unfortunately this parameter is not available in GLES2 More research, I see several folks setting an option in glTexParameter* called GL_TEXTURE_CROP_RECT_OES, unfortunately this seems to be some kind of extension that is not generally available in GLES2 The two solutions that are common are not available, what now? It seems there are 2 paths: Copy the frame data to a memory location before uploading the textures. This lets me crop the image, getting rid of the extra stride pixels. Huge downside that this requires 3 memory copies (Y+U+V). This would be incredibly inefficient. Bind the stride data in the shader and perform the image crop in the shader. The downside here is that I have no clue how to do this, but as it seems it is the better solution, I will focus research effort down this path. The fragment shader provides a built-in variable gl_FragCoord that is supposed to give you the window coordinates of the current fragment. I’m hoping I can use this along with the discard command to skip the color calculation for fragments that exist in the stride.   What I’m not sure of is whether the shader factors texture clamping into the calculation . . . i.e. when using edge clamping for my texture, will the fragment coordinate of an area in the stride be > 1.0 or == 1.0?  Wow, no clue!   Looks like I need to figure out how to print debug data from the fragment shader. 8:52 PM 10/4/2011 More work on a shader for YUV420p, something is clearly wrong here.  I think it must have to do with the content coming in through draw_slice.  I’ve tried every YUV fragment shader I could find and finally settled on a modified version that follows some of the equations and discussion points on the fourcc website.  All of them essentially look like this, I need to post this on a stackexchange site and see if someone can identify what the decoding issue might be. . . Other than that, the framerate seems ok.  I’ve been hung up on this for quite some time, I may have to back-burner the YUV shader and get the other low-hanging fruit done like the OSD.  At least we’d have a fully functioning player with subtitles, it just wouldn’t have accelerated colorspace conversion for h.264. 3:46 PM 10/2/2011 I made an attempt to get the YUV shader going. At first nothing was being drawn to the screen with even a simple shader, I made some changes to the program loader so that it does a hard fail if the vertex or fragment shader fails to compile. Now that I know the shader is at least good in theory, I tried several shaders that I found out on the interwebs for YUV conversion. I can’t get an intelligable picture using any of the implementations, I’m guess it is due to these conversion formulas expecting non-planar data. 12:43 AM 9/29/2011 I finally had some time to put towards this. I was able to get in correct aspect ratio scaling for the texture coordinates so squares and circles now look like squares and circles. It could be worth revisiting once the OSD is enabled to allow for different scaling methods, right now the height is modified to achieve the correct aspect ratio (this works since it’s a 4:3 screen). MKV playback is a little better but it still doesn’t run at full framerate.  I’ve seen some discussion about doing the YUV conversion in a fragment shader, one side claims it’s faster because the calculations are being done in the GPU while the other side claims it’s slower because you have to copy 3 textures from memory.  If anyone knows a definitive answer please say so.  Right now the YUV conversion is not being done in a fragment shader although I had hoped to move that direction. I’m getting a segfault when exiting mplayer and this oddly makes mplayer stick around in the process list.  Also I need to add in the event loop check to pause video on card minimize but I might just wait for the OSD since there’s no point in pausing if you can’t hit play again. 9:03 AM 9/25/2011 I contacted Chomper who got back to me a few hours later and sent an updated version of vo_sdl.c.  This version uses libswscale to do the yuv conversion and also has some other optimizations.  He also said there were some other bugfixes to be done. I compiled the new stuff from Chomper and MKV playback made a huge leap in performance.   It went from unplayable to near 24 fps with audio enabled. At this point I am trying to get subtitles and OSD enabled but am having issues getting the configuration step to recognize libfreetype. Not sure if I should compile this myself or try to point to the libs/headers from the preware cross compile.   I don’t want any other libs in there to be accidentally pulled in over my optimized compiled versions. For future note, I have been using the following configure settings: extra-libs=-lGLESv2 enable-neon enable-gl disable-dvdnav disable-dvdread disable-dvdread-internal disable-libdvdcss-internal disable-alsa enable-menu extra-cflags=’-O3 -funroll-loops -marm -march=armv7-a -ftree-vectorize -mfpu=neon -mfloat-abi=softfp -mtune=cortex-a8 -I/opt/PalmPDK/include/ -I/opt/PalmPDK/include/GLES2 -L/opt/PalmPDK/device/lib/’ Also, I was doing some research on the best build options for performance on armv7 and stumbled on this page.  I’ve been building ffmpeg and mplayer with the options that they say get the best coremark scores: -O3 -funroll-loops -marm -march=armv7-a -mtune=cortex-a8 10:56 AM 9/23/2011 I love pushing code via freetether while riding public transit!  I have now integrated the WOSI/chomper openGL ES code into my mplayer fork. The code compiles but there are a couple things I need to revisit: The code currently hijacks the vo_sdl.c output module. It would make more sense if we kept vo_sdl.c the same and made a new vo_gles.c module. This will require a few additions to video_out.c (correct name?). The screen initialization code in the above code specifies a width, height, bit depth. I hard-coded the bit depth to 0 and think I should do the same for width/height (as per Palm recommendation, pretty sure this just autodetects dimensions of screen). The patch I hand applied turned all of the OSD stuff into no-ops. Once I have a chance to test the actual video performance, I need to revisit this and get the OSD stuff back in. Possibly I may need to look at the existing openGL module and convert the OSD contents to GL ES. 1:03 AM 9/22/2011 I now have mplayer compiled for ARM. I am unable to get SDL video output working, I double-checked that SWSURFACE is being used. I can use fbdev output but this doesn’t pull up in an app window and doesn’t fill the whole screen. I would really like to be able to use openGL as the output but mplayer does not support openGL ES. I would basically have to copy vo_gl.c and make an ES version. The advantage of doing this in mplayer rather than ffplay is primarily being able to use the OSD and the numerous skins available for it (I think). I’m actually unsure if I can even use the OSD with all the GTK stuff. I should take a look at the chomper patch for mplayer, it is old but might be enough to help me figure out how to get SDL working. Once this works, I can compile with the OSD stuff enabled and see whether I can at least get the OSD going. "},{"title":"TouchPlayer Documentation","baseurl":"","url":"/2011/10/16/touchplayer-documentation/","date":"2011-10-16 00:00:00 -0700","categories":["homebrew","webos"],"body":" How to Get TouchPlayer TouchPlayer is distributed through a private feed, you need to add it (mobilecoder.touchpadhp.info/mcfeed) so that TouchPlayer will show up in Preware.  Alternatively, if you know what you are doing, you can download the ipk directly (http://mobilecoder.touchpadhp.info/touchplayer/) Directions for Preware Open Preware Tap the top left of the Preware screen and select Manage Feeds Scroll to the bottom and under New Feed, give the feed a name (any name) and enter/select the following corresponding details URL : mobilecoder.touchpadhp.info/mcfeed IS Compressed : NO Select Add Feed Again in Preware, select Update feeds Install (Search for TouchPlayer in Preware, select it and hit install Luna Restart (Preware menu, Luna Manager, Luna Restart) Features/What’s New 1.0.7 Added UI field to paste rtsp streams Added scale parameter to the play service call which basically does: args += -vf scale=: + inArgs.scale + ;) Playing a file now automatically kills existing mplayer instances 1.0.6a Modified post-install script to attempt to fix install issues.  I was able to reproduce the issue on my device.  I verified that after fixing the files, a simple Luna restart did not resolve the issue but a reboot did.  Therefore, a reboot is now required after install 1.0.6 Added install check Modified back-end launching, hopefully making emergency kill unecessary Now uses TTF fonts, user can choose from fonts installed in /usr/share/fonts/. This makes old font.desc junk obsolete and should also immediately support all locals Font scaling now works (sizes in the GUI are multipliers not points) New API call for getting list of fonts New API call for validating installation New API parameters for playing videos Updated to latest FFmpeg and mplayer as of 11/5/2011 1.0.5 (Experimental) Improvements to back-end mplayer launching Preliminary Cyrillic character support Move subtitles into empty black area if possible 1.0.4 Runs mplayer underneath the hood meaning that most video formats are supported. NEON accelerated YUV to RGB conversion Pause/Fast forward/Rewind Subtitle support Controls Pause To pause the video, single tap on the screen.  You can also minimize the video window into card view.  To unpause, single tap again. Fast Forward/Rewind To fast forward or rewind, swipe horizontally right or left.  1 finger = small increments, 2 fingers = large increments. Subtitles On/Off To toggle subtitles on/off, swipe down with 3 fingers.  You will see a confirmation in the top/left of the video indicating the status of the subtitles. Subtitle Index Toggle To cycle between the available subtitles, swipe up with 3 fingers.  You will see a confirmation in the top/left of the video indicating the name of the current set of subtitles. UI Font Size The font size determines the size of the subtitles. Emergency Kill As of 1.0.6, this should no longer be needed but I've left it in just in case.  Please report if you have threads that aren't being cleaned up. I apologize in advance that this button is even necessary.  I have noticed that sometimes mplayer does not clean up all of the threads that it spins up, occasionally this can cause issues launching a new instance of mplayer.  This button is here to resolve these issues.  Pressing this button will kill all running instances of mplayer. Q. When should I press this button? A. If suddenly you find that videos are not launching, press this button and then try launching your video again. Q. Can anything bad happen if I hit this button when I don't need to? A. Nothing bad will happen, feel free to press it without fear! Service Calls For folks that would like to leverage TouchPlayer there are 3 service calls available: play Plays a given file or stream. Sample service definition. { name: playFile, kind: PalmService, service: palm://com.wordpress.mobilecoder.touchplayer.service, method: play, onSuccess: fileStarted } Arguments source: string The full path to file audio: boolean Whether to enable audio or not font: string The name of the font (for a list of names see below) fontsize: integer fontscale: integer How large to scale the font charset: en movesubs: boolean Whether to move subs into the black area onsuccess: function A callback to be called once the play command completes scale: int Adds the command  -vf scale=: scale Sample call this.$.playFile.call( { source: /media/internal/movie.mp4, audio: true, fontscale: 2, font: times, movesubs: false }); killall Kills all mplayer processes Sample service definition { name: emergencyKill, kind: PalmService, service: palm://com.wordpress.mobilecoder.touchplayer.service, method: killall } Sample call //Kill all instances of mplayer this.$.emergencyKill.call(); getfonts Returns a list of fonts stored in /usr/share/fonts Sample service definition { name: getFontList, kind: enyo.PalmService, service: palm://com.wordpress.mobilecoder.touchplayer.service, method: getfonts, onSuccess: gotFonts } Sample success handler: gotFonts: function(inSender, inResponse, inRequest){ //Populate a font dialog box this.$.fontName.setItems(inResponse.reply); } Help! For help, please post a comment below.  Note that comments are moderated and may not appear once you submit them.  I will receive an email when you leave a comment. The mplayer faq may also be of interest. The following is the output of the FFmpeg configuration step, this shows you which codecs are compiled in. source path . C compiler gcc ARCH arm (generic) big-endian no runtime cpu detection no ARMv5TE enabled yes ARMv6 enabled yes ARMv6T2 enabled yes ARM VFP enabled yes IWMMXT enabled no NEON enabled yes debug symbols no strip symbols yes optimize for size no optimizations yes static yes shared no postprocessing support yes new filter support yes network support yes threading support pthreads SDL support yes Sun medialib support no libdxva2 enabled no libva enabled no libvdpau enabled no AVISynth enabled no libcelt enabled no frei0r enabled no libcdio support no libdc1394 support no libdirac enabled no libfaac enabled no libaacplus enabled no libgsm enabled no libmodplug enabled no libmp3lame enabled no libnut enabled no libopencore-amrnb support no libopencore-amrwb support no libopencv support no libopenjpeg enabled no libpulse enabled no librtmp enabled no libschroedinger enabled no libspeex enabled no libstagefright-h264 enabled no libtheora enabled no libutvideo enabled no libvo-aacenc support no libvo-amrwbenc support no libvorbis enabled no libvpx enabled no libx264 enabled no libxavs enabled no libxvid enabled no openal enabled no zlib enabled yes bzlib enabled no Enabled decoders: aac dirac mp1 aac_latm dnxhd mp1float aasc dpx mp2 ac3 dsicinaudio mp2float adpcm_4xm dsicinvideo mp3 adpcm_adx dvbsub mp3adu adpcm_ct dvdsub mp3adufloat adpcm_ea dvvideo mp3float adpcm_ea_maxis_xa dxa mp3on4 adpcm_ea_r1 eac3 mp3on4float adpcm_ea_r2 eacmv mpc7 adpcm_ea_r3 eamad mpc8 adpcm_ea_xas eatgq mpeg1video adpcm_g722 eatgv mpeg2video adpcm_g726 eatqi mpeg4 adpcm_ima_amv eightbps mpegvideo adpcm_ima_dk3 eightsvx_exp msmpeg4v1 adpcm_ima_dk4 eightsvx_fib msmpeg4v2 adpcm_ima_ea_eacs eightsvx_raw msmpeg4v3 adpcm_ima_ea_sead escape124 msrle adpcm_ima_iss ffv1 msvideo1 adpcm_ima_qt ffvhuff mszh adpcm_ima_smjpeg flac mxpeg adpcm_ima_wav flashsv nellymoser adpcm_ima_ws flashsv2 nuv adpcm_ms flic pam adpcm_sbpro_2 flv pbm adpcm_sbpro_3 fourxm pcm_alaw adpcm_sbpro_4 fraps pcm_bluray adpcm_swf frwu pcm_dvd adpcm_thp g723_1 pcm_f32be adpcm_xa g729 pcm_f32le adpcm_yamaha gif pcm_f64be alac gsm pcm_f64le als gsm_ms pcm_lxf amrnb h261 pcm_mulaw amrwb h263 pcm_s16be amv h263i pcm_s16le anm h264 pcm_s16le_planar ansi huffyuv pcm_s24be ape idcin pcm_s24daud ass idf pcm_s24le asv1 iff_byterun1 pcm_s32be asv2 iff_ilbm pcm_s32le atrac1 imc pcm_s8 atrac3 indeo2 pcm_u16be aura indeo3 pcm_u16le aura2 indeo5 pcm_u24be avs interplay_dpcm pcm_u24le bethsoftvid interplay_video pcm_u32be bfi jpeg2000 pcm_u32le bink jpegls pcm_u8 binkaudio_dct jv pcm_zork binkaudio_rdft kgv1 pcx bintext kmvc pgm bmp lagarith pgmyuv c93 loco pgssub cavs mace3 pictor cdgraphics mace6 png cinepak mdec ppm cljr mimic prores cook mjpeg prores_lgpl cscd mjpegb ptx cyuv mlp qcelp dca mmvideo qdm2 dfa motionpixels qdraw qpeg svq1 vp3 qtrle svq3 vp5 r10k targa vp6 r210 theora vp6a ra_144 thp vp6f ra_288 tiertexseqvideo vp8 rawvideo tiff vqa rl2 tmv wavpack roq truehd wmalossless roq_dpcm truemotion1 wmapro rpza truemotion2 wmav1 rv10 truespeech wmav2 rv20 tscc wmavoice rv30 tta wmv1 rv40 twinvq wmv2 s302m txd wmv3 sgi ulti wmv3image shorten utvideo wnv1 sipr v210 ws_snd1 smackaud v210x xan_dpcm smacker vb xan_wc3 smc vc1 xan_wc4 snow vc1image xbin sol_dpcm vcr1 xl sonic vmdaudio xsub sp5x vmdvideo yop srt vmnc zlib sunrast vorbis zmbv Enabled encoders: a64multi h263 pcm_u24le a64multi5 h263p pcm_u32be aac huffyuv pcm_u32le ac3 jpeg2000 pcm_u8 ac3_fixed jpegls pcx adpcm_adx ljpeg pgm adpcm_g722 mjpeg pgmyuv adpcm_g726 mp2 png adpcm_ima_qt mpeg1video ppm adpcm_ima_wav mpeg2video prores adpcm_ms mpeg4 qtrle adpcm_swf msmpeg4v2 ra_144 adpcm_yamaha msmpeg4v3 rawvideo alac msvideo1 roq amv nellymoser roq_dpcm ass pam rv10 asv1 pbm rv20 asv2 pcm_alaw sgi bmp pcm_f32be snow dca pcm_f32le sonic dnxhd pcm_f64be sonic_ls dpx pcm_f64le srt dvbsub pcm_mulaw svq1 dvdsub pcm_s16be targa dvvideo pcm_s16le tiff eac3 pcm_s24be v210 ffv1 pcm_s24daud vorbis ffvhuff pcm_s24le wmav1 flac pcm_s32be wmav2 flashsv pcm_s32le wmv1 flashsv2 pcm_s8 wmv2 flv pcm_u16be xsub g723_1 pcm_u16le zlib gif pcm_u24be zmbv h261 Enabled hwaccels: Enabled parsers: aac dvdsub mpegaudio aac_latm flac mpegvideo ac3 h261 pnm cavsvideo h263 rv30 dca h264 rv40 dirac mjpeg vc1 dnxhd mlp vp3 dvbsub mpeg4video vp8 Enabled demuxers: aac image2 pcm_u24le ac3 image2pipe pcm_u32be act ingenient pcm_u32le adf ipmovie pcm_u8 aea iss pmp aiff iv8 pva amr ivf qcp anm jv r3d apc latm rawvideo ape lmlm4 rl2 applehttp loas rm asf lxf roq ass m4v rpl au matroska rso avi microdvd rtp avs mjpeg rtsp bethsoftvid mlp sap bfi mm sdp bink mmf segafilm bintext mov shorten bit mp3 siff c93 mpc smacker caf mpc8 sol cavsvideo mpegps sox cdg mpegts spdif daud mpegtsraw srt dfa mpegvideo str dirac msnwc_tcp swf dnxhd mtv thp dsicin mvi tiertexseq dts mxf tmv dv mxg truehd dxa nc tta ea nsv tty ea_cdata nut txd eac3 nuv vc1 ffm ogg vc1t ffmetadata oma vmd filmstrip pcm_alaw voc flac pcm_f32be vqf flic pcm_f32le w64 flv pcm_f64be wav fourxm pcm_f64le wc3 g722 pcm_mulaw wsaud g723_1 pcm_s16be wsvqa g729 pcm_s16le wtv gsm pcm_s24be wv gxf pcm_s24le xa h261 pcm_s32be xbin h263 pcm_s32le xmv h264 pcm_s8 xwma idcin pcm_u16be yop idf pcm_u16le yuv4mpegpipe iff pcm_u24be Enabled muxers: a64 ipod pcm_s16le ac3 ivf pcm_s24be adts latm pcm_s24le aiff m4v pcm_s32be amr matroska pcm_s32le asf matroska_audio pcm_s8 asf_stream md5 pcm_u16be ass microdvd pcm_u16le au mjpeg pcm_u24be avi mlp pcm_u24le avm2 mmf pcm_u32be bit mov pcm_u32le caf mp2 pcm_u8 cavsvideo mp3 psp crc mp4 rawvideo daud mpeg1system rm dirac mpeg1vcd roq dnxhd mpeg1video rso dts mpeg2dvd rtp dv mpeg2svcd rtsp eac3 mpeg2video sap ffm mpeg2vob segment ffmetadata mpegts sox filmstrip mpjpeg spdif flac mxf srt flv mxf_d10 swf framecrc null tg2 framemd5 nut tgp g722 ogg timecode_v2 g723_1 pcm_alaw truehd gif pcm_f32be vc1t gxf pcm_f32le voc h261 pcm_f64be wav h263 pcm_f64le webm h264 pcm_mulaw wtv image2 pcm_s16be yuv4mpegpipe image2pipe Enabled protocols: applehttp md5 rtmps cache mmsh rtmpt concat mmst rtmpte crypto pipe rtp file rtmp tcp gopher rtmpe udp http Enabled filters: abuffer deshake nullsrc abuffersink drawbox overlay aconvert fade pad aevalsrc fieldorder pixdesctest aformat fifo rgbtestsrc amovie format scale anull gradfun select anullsink hflip setdar anullsrc hqdn3d setpts aresample lut setsar ashowinfo lutrgb settb blackframe lutyuv showinfo boxblur movie slicify buffer mp split buffersink mptestsrc testsrc color negate transpose copy noformat unsharp crop null vflip cropdetect nullsink yadif delogo Enabled bsfs: aac_adtstoasc mjpeg2jpeg mp3_header_decompress chomp mjpega_dump_header noise dump_extradata mov2textsub remove_extradata h264_mp4toannexb mp3_header_compress text2movsub imx_dump_header Enabled indevs: Enabled outdevs: License: GPL version 2 or later Creating config.mak and config.h... config.h is unchanged libavutil/avconfig.h is unchanged "},{"title":"Promote Cheating for a Better Future (a retrospect of a geeky childhood)","baseurl":"","url":"/2011/11/17/promote-cheating-for-a-better-future-a-retrospect-of-a-geeky-childhood/","date":"2011-11-17 00:00:00 -0800","categories":["personal"],"body":" I recently turned 0b100000, on this momentous transition from my 5th to 6th bit, I took some time to reflect back on how I got where I am today.  Of all the events in my life that affected the direction I went, the desire to cheat stands alone as having an overwhelmingly positive impact. The Year 1991 In 1991 at the age of 12, I was like any other kid. I spent half my time playing my new Super Nintendo (SNES) and the other half playing with my 486 DX2 running MS DOS 6.X.  Wow, don’t even get me started on what a kid gamer had to learn about conventional memory management in the 90s just to run the latest Wing Commander! On the SNES, one of my friends had a game genie. It was basically this cartridge that you plugged your SNES cartridge into and then plugged the whole thing into the SNES.  The Game Genie came with a booklet that had a list of cheats for all the popular games, stuff like: Super Mario: Higher Jump -  D4C7-3FA7 So you’d plug this monster into the SNES, power on and were greeted with a Game Genie interface where you could enter these codes for your game.  Once you entered all the codes you wanted, you started the game with your cheats enabled. The cheating was cool and all, but at the time I was more fascinated with how and why this all worked. I mean, think about this a second, you put a number into this black box and on the other end it makes mario jump higher?!?  Whaaaaat?  This was literally a magic genie in a cartridge. The Magic of Cheating I became so fixated on how the game genie worked and the possibility of creating my own codes that I started searching for answers.  The first place I turned was the manual.  I’ve always had a thing for reading manuals in their entirety and this was no exception.  I found it incredibly dissapointing that the manual didn’t tell me how to make my own Game Genie codes.  I even called the Nintendo Hotline, thinking that they would surely know how.  I didn’t know the Game Genie wasn’t made by Nintendo at the time but I certainly did after the hotline attendant made it sound like the Game Genie was going to cause my SNES to spontaneously combust. Where now?  Back in 1991, you couldn’t hop onto google and start searching for these types of things.  No, back then I had a handful of BBS numbers and AOL.  Not many people may remember, but AOL had access to Newsgroups.  I didn’t know what a newsgroup was at the time, I just thought it was like a Prodigy message board but bigger.  I don’t recall what search terms I used but I do remember making an incredible discovery that day that changed my life forever. Hexediting I found a tutorial on Hexediting Save Game Files.  The author explained things in a way that a kid my age understood, he didn’t know all the terminology or the reasons why things worked the way they did, but he knew enough to get the job done.  At the age of 12, that’s really all I cared about. He started simple, explaining how when you save a file on the computer, it stores things in hexadecimal.  He pointed to some tools to convert from normal numbers to hex (base10 to base16) and mentioned that he wasn’t sure why, but sometimes instead of searching for FF AB, you had to search for AB FF.  I had just learned big-endian vs little-endian although I wouldn’t know the proper name until years later.  I already had the software I needed (pctools) and with this new knowledge I began an amazing journey. Cheating was intoxicating, I hex edited everything.  This really wasn’t about cheating at all, this was about solving a complex and satisfying puzzle, it was about feeling like I had control of a vastly mysterious universe that was once outside of my understanding.  It gave me the confidence that I could learn anything and it inspired me to dig deeper into how things work. I was Legend- There wasn’t any good note taking software back then, and since things were single-tasking in DOS it didn’t matter anyway.  I had a notebook full of addresses for all my games.  In Eye of the Beholder 2, I had mapped every character stat, every inventory slot and every item.  Kids at school would bring their saved games on a 3.5 disk and I would call them once I got home and say ok, what sword do you want in inventory slot1?  Hmm, you have a bunch of magic missile scrolls taking up space, is it ok if I delete them?, I was a legend. I remember the first encrypted file I came across, the game was Shadow of Yserbius on The Sierra Network (TSN).  I used the usual method of saving my game, dropping an item and saving again so that I could compare the files.  I remember that night vividly because the base16 covered over 3 notebook pages, I couldn’t make any sense of it, why had so much changed for just 1 item? Through some social networking, I discovered a BBS on the other side of the US that supposedly contained a document on how to hack this game.  I got permission from my parents, dialed in, chatted with the sysop (system operator) and got the file.  This document was a little above my head at the time but I remember learning that trying to hack an encrypted file was fruitless unless you knew the encryption being used.  Instead this article taught about editing live memory while the program was running by loading a TSR (terminate stay resident aka a background process).  This literally blew my mind and world, I was truly unstoppable at this point. Once you start editing live memory, you learn what not to do pretty quickly things like trying to promote a single byte value to two bytes (buffer overflow).  A year later with several more tutorials under my belt, I asked for the Turbo Pascal compiler for my 13th birthday (my parents must have thought I was nuts), I actually wanted to program the Z80 (since I knew it was the primary processor in the NES)  but was told I shouldn’t jump straight to assembly language. Today I often wonder who the author of that hexediting tutorial was and what he is doing today.  I’ve done a few searches on textfiles.com  but haven’t been able to find anything close.  Today, in single player games, I occasionally cheat, it’s almost like my own personal crossword puzzle.  I love peering inside save game files to see whether the designers made an attempt at obfuscating the data or whether they just threw their hands up and called it a day.  One of the best tools around is by far Cheat Engine , and not just for cheating in games, I get an amazing amount of utility in my day job (which has nothing to do with video games) with this program. Cheating was the gateway to a satisfying future and tutorials written by inspired tinkeres were the catalyst.  For anyone who has ever written a tutorial, thank you!  It doesn’t take big words or proper names to teach someone a process.  You never know how you might change someone’s world by simply explaining how to do something, so hit up stackexchange.com , tumblr , blogspot or wordpress  and make the world a better place. "},{"title":"Why App Stores SUCK  Segregation","baseurl":"","url":"/2012/02/17/why-app-stores-suck-segregation/","date":"2012-02-17 00:00:00 -0800","categories":["android","iphone","marketplace  publishing","webos"],"body":"I was recently going through the Blackberry developer forums and stumbled across a thread that is all too common. A developer had an app on the market and was doing well until a negative comment was posted and sales stopped. It’s possible that the app in question wasn’t any good and the loss of sales had nothing to do with the comment. . . but, for the sake of this article, I am siding with the developer because when it comes right down to it, app stores kind of suck. Get the Tide out because it’s time to air some dirty laundry. The scenario I described above was on the Blackberry App World, but it highlights something that is wrong with ALL app markets in existence.  This includes Apple, Google, HP and Amazon. Developers are segregated from the customer and then punished Imagine that you’ve spent the last 6 months writing software in the copious amounts of free time between your real job and family life.  You wrap it up, put it in a flashy box listing all the features you worked so hard on and then you throw it into a room full of sharpies and lunatics.  Wait, what? Yes, welcome to the app store. Now you get to stare through the 1-way sound-proof mirror as you watch customers write a bunch of untrue crap on the front of your software for all the other customers in the world to see.  If you’re lucky, you’ll get away with just a couple 1-star uninstall! comments but often-times things can get far worse. Customers can post things that have no basis in reality with deliberate malicious intent.  For instance, you may be surprised to one day wake up and discover that you have written the world’s first iPhone virus.  Often times this is an app competitor using subterfuge. This type of negative PR is hard to control because the typical customer will gladly replace their own personal experience with the experience of the app reviewer.  Side-note, this scenario actually happened to me. The developer is ironically punished for having no control over comments because the app market creates this facade where customers believe that comments put them in touch with the developer.  Customers ask for help and post their problems in app store comments and the developers are standing on the other side of the sound-proof window screaming in agony because they: Know what the likely problem is Can’t get the customer’s contact info Can’t respond to the comment Couldn’t warn the customer if an axe murderer was standing behind them I think there are viable solutions if any of the app stores were actually interested in attracting developers. Option 1 Allow the developer to respond to comments but don’t allow threads.  A customer can post and modify only 1 review, the developer can post and modify only 1 response to that review. It’s important that the developer is delineated as being part of the company instead of just another user claiming to be part of the company (the problem with Amazon).  This would at least allow the developer to offer support, clear the air for downright malicious posts or at the bare minimum provide some backing support for comments like. Option 2 Let the community sort things out.  Turn the app store comments section into a mini Stackexchange, just be sure to show the score on each comment.  Google has attempted something like this but the implementation is poor.  For upvotes/downvotes to mean anything you need to show the meta-score for each comment.  Without meta-score being visible, moving comments up/down only affects the temporal perspective, i.e. it makes comments look like they were posted later/earlier than other comments. This is how things are now The Stackexchange model Developers should have more say At the end of the day, developers pay the app store 30% of their profits to throw their product into an alleyway full of graffiti.  It’s sad that for this price, it takes a week to paint over a vulgar comment and is impossible to address the untrue and misleading ones.  It’s infuriating that none of the app stores seem to care about this!?!  The first app store that truly gets this is going to have happy developers.  Happy developers typically morph into platform evangelists. "},{"title":"ShiVa Native Projects for Android  A Guide for the Forgetful","baseurl":"","url":"/2012/04/14/shiva-native-projects-for-android-a-guide-for-the-forgetful/","date":"2012-04-14 00:00:00 -0700","categories":["android","java","shiva 3d"],"body":"This article covers the basics of exported Android ShiVa projects.  If you are trying to integrate Java or C libraries, for instance the ScoreLoop API, the following information could come in handy.  I had to stumble through this process with the scattered bits of documentation and I get tired of re-learning it every time I start a new project.  The article assumes basic familiarity with Eclipse, Java, C, Android and JNI. Most of this content is based on the file that Stonetrip provides, on windows it is: C:\\Program Files (x86)\\Stonetrip\\ShiVa Authoring Tool\\Data\\Windows\\Windows\\Build\\S3D SDK - Readme.txt STKs & Project Authoring It’s important to understand how things fit together in ShiVa so that you don’t waste your efforts.  The ShiVa project eco-system works like this.  From the ShiVa Editor, you export an .stk file, this is your actual game. Once your .stk is exported, you open it up in the Authoring Tool and you export as a Project instead of an APK package. Before you export as a project, you need to choose your Build settings correctly.  For instance if you’re going to use Network then be sure to check that box, if you’re using openGL 1.1 be sure to check that etc.  It’s possible to go back and change these later but it can be a bit tedious if you’re not familiar with where the parameters materialize in the exported project. When you export as a project, you’re given a zip file.  The zip file contains an eclipse project, once you import this project into eclipse you are done exporting your project from the authoring tool for good. Eclipse Project Details Take a look in the assets folder once you’ve imported the project.  Take special note that the S3DMain.smf file is actually just an .stk file that is renamed. Every time you update your game, you are going to export the .stk, rename it to S3DMain.smf and overwrite this file in the assets folder. To build, you can’t just click the play button, you have to build with Ant. Window->Show View->Ant Drag the build.xml into the Ant window Double click on Build Debug APK Interfacing ShiVa with Other Code There are 2 basic scenarios that I’ll cover here. Scenario 1: Call C/C++/Java Code from ShiVa The way you accomplish this is by setting a listener for a ShiVa user event.  In this example, I have the following userAI: What we’re going to do is make it so anytime these handlers are called in your game, some Java code will run as a result.  As far as the order of operations, my logging has told me that once the handler is called, the native code is ran and then the remainder of the LUA handler runs.  Here is how to setup a listener for a handler. Step 1.  Open up S3DClient.cpp and search for BEGIN_JNI_INSTALL_EVENT_HOOKS,  you seriously can’t miss it. Step 2. Add listeners for each handler.  Here’s what mine looks like: //---------------------------------------------------------------------- // @@BEGIN_JNI_INSTALL_EVENT_HOOKS@@ //---------------------------------------------------------------------- S3DClient_InstallCurrentUserEventHook( \"achievementAI\", \"onIncrementAchievement\", incrementAchievement, NULL ); S3DClient_InstallCurrentUserEventHook( \"achievementAI\", \"onAwardAchievement\", awardAchievement, NULL ); //---------------------------------------------------------------------- // @@END_JNI_INSTALL_EVENT_HOOKS@@ //---------------------------------------------------------------------- Note the parameters (userAI, handler, function, NULL).  The functions named in the above will be called whenever the handlers are called. Step 3. Write the JNI function (also in S3DClient.cpp just above the definition for engineInitialize).  If you’ve never done this before then buckle up and read some sun articles .  Here is the JNI for incrementAchievement, the goal is to call a Java function in my main class called incrementAchievement() . void incrementAchievement(unsigned char _iArgumentCount, const void *_pArguments, void *_pUserData){ JNIEnv *pJNIEnv = GetJNIEnv(); if (pJNIEnv){ if ( _pArguments && ( _iArgumentCount 0 ) ){ const S3DX::AIVariable *pVariables = (const S3DX::AIVariable *)_pArguments ; for ( uint8_t i = 0 ; i _iArgumentCount ; i++ ){ //We only want strings if(pVariables[i].GetType() == S3DX::AIVariable::eTypeString){ LOGI( \"incrementAchievement returned string: %s\", pVariables[i].GetStringValue() ); //Find our main class so we can call the incrementAchievement function. For me, my package name is: //com.hypercanestudios.acceleroketer //within that package I have a class called accelerocketer //so package name/class name and replace \".\" with \"/\" jclass pJNIActivityClass = pJNIEnv-FindClass ( \"com/hypercanestudios/acceleroketer/acceleroketer\" ); if(pJNIActivityClass == NULL) LOGI(\"jclass was null!?!\"); else{ //Now we have to find the function we're trying to call. We use the class defined above since the function //is a member of that class. (Ljava/lang/String;) is the set of arguments that the function takes, a single String //V means that the function returns void //void incrementAchievement(String blah) //See table 3-2 http://docs.oracle.com/javase/1.3/docs/guide/jni/spec/types.doc.html#597 jmethodID pJNIMethodID = pJNIEnv-GetStaticMethodID(pJNIActivityClass, \"incrementAchievement\", \"(Ljava/lang/String;)V\" ); if(pJNIMethodID == NULL) LOGI(\"jmethodID was null!?!?\"); else{ //Create a new string jstring arg; arg = pJNIEnv-NewStringUTF(pVariables[i].GetStringValue()); //Call the method and pass the string parameter along pJNIEnv-CallStaticVoidMethod(pJNIActivityClass, pJNIMethodID, arg); //Free the string pJNIEnv-DeleteLocalRef(arg); } } } } } } } You could eliminate some of the checks against NULL in your final version but these are pretty helpful during the initial creation phase. When you crash due to a JNI error, you don’t get a nice clean stacktrace so any debug you can print goes a long ways. Search stackoverflow.com for ways of debugging JNI stacks, you can use addr2line and get the function and line number. Step 4. Write the Java function public static void incrementAchievement(String id){ Log.i(\"acceleroketer\", \"Hello World!!1 \" + id); } Scenario 2: Call ShiVa code from Java The other scenario is when you want to call down to your ShiVa code from a Java function.  Let’s call achievementAI’s onShowAchievement handler. Step 1. Copy S3DX header files into your JNI folder, i.e. drag them into the JNI folder in eclipse and select copy.  On windows the headers are located in C:\\Program Files (x86)\\Stonetrip\\ShiVa Authoring Tool\\Data\\Windows\\Android\\Build\\S3DX Step 2. Add the S3DXAIVariable header to your S3DClient.cpp: //---------------------------------------------------------------------- // @@BEGIN_JNI_INCLUDES@@ //---------------------------------------------------------------------- #include #include android/log.h //---------------------------------------------------------------------- #include #include #include GLES/gl.h //---------------------------------------------------------------------- #include \"S3DXAIVariable.h\" #include \"S3DClient_Wrapper.h\" Step 3. In your Java code, define a native function, this will be used to call ShiVa. public native static void showAchievementInShiva(String id); Step 4. In S3DClient.cpp implement the native method being sure to handle any parameters. JNIEXPORT void JNICALL Java_com_hypercanestudios_acceleroketer_acceleroketer_showAchievementInShiva ( JNIEnv *_pEnv, jobject obj, jstring id ){ //Convert java string 'id' to a const char * so we can pass it to shiva const char *nativeString = _pEnv-GetStringUTFChars(id, NULL); S3DX::AIVariable args[1]; args[0].SetStringValue( nativeString ); S3DClient_SendEventToCurrentUser( \"achievementAI\", \"onShowAchievement\", 1, (const void*)args); //Release string _pEnv-ReleaseStringUTFChars(id, nativeString); } Step 5.  Create the handler in your ShiVa userAI and print something useful to make sure it’s working. -------------------------------------------------------------------------------- function achievementAI.onShowAchievement ( id ) -------------------------------------------------------------------------------- log.message ( \"Got id: \" .. id ) -------------------------------------------------------------------------------- end -------------------------------------------------------------------------------- "},{"title":"ShiVa PoC: Box Particle Lighting","baseurl":"","url":"/2012/06/02/shiva-poc-box-particle-lighting/","date":"2012-06-02 00:00:00 -0700","categories":["shiva 3d"],"body":"I have started a new code repository on github where I will be placing various self-contained Proof of Concept projects for the ShiVa game engine.  Each project will be contained in a separate directory for easy dissemination.  I will also include live web-based samples of all projects here on my blog as I create them.  This is the first of those projects, click the image to load the live web version. This demo is meant to test particle interaction with colliders along with a moving dynamic light source. Scene Setup The project scene is a simple set of planes, set as colliders.  These planes form a fully enclosed cube so that the object inside cannot escape.  The helper object has a point-light source, a particle emitter and a dynamics controller.  Every frame, a random impulse is added to the dynamics controller of the helper to give it motion.  The dynamics controller has gravity disabled but dynamics and collisions enabled. What to Tweak Particles   This is a great project to get a feel for particle system parameters, edit them while you run the demo.  One issue with particle systems is that we usually tweak the parameters while the particle emitter is static.  The particle system often looks completely different once the emitter is in motion, so tweaking while the emitter is static can be quite deceiving! Light   The radius of a point light source may be difficult to get right, this is a good environment to get a feel for the effect of point light source scaling as there aren’t any other light sources. Dynamics  Try changing the mass or maximum velocity of the emitter objects dynamics controller to get a feel for the wonderful dynamics that ShiVa provides.  Also try changing the random values generated in onEnterFrame that determine how large of an impulse to add to the object. Applications You might use this to simulate a firefly stuck in a jar, or an object with a chaotic trajectory.  The code is less than a dozen lines, the project is small and to the point, feel free to check it out and use it in your projects. Grab the source "},{"title":"ShiVa PoC: Newtons Cradle","baseurl":"","url":"/2012/06/03/shiva-poc-newtons-cradle-2/","date":"2012-06-03 00:00:00 -0700","categories":["shiva 3d"],"body":"This is an attempt to replicate a Newton’s Cradle device. This is very similar to the rope sample provided by Stonetrip.  The ball bearings have spherical dynamics bodies and are attached to an anchor point on the ceiling.  The string is cosmetic only, it has no dynamics or anchor points. The collision doesn’t happen like the real world counterpart despite my fiddling with dynamics parameters.  I believe a programmatic solution would not be difficult by simply detecting the last collision point and setting things to be stationary.  I didn’t go through the trouble with the programmatic solution since the goal was to see whether the dynamics system would handle the system automatically. Get the source. "},{"title":"Sony Smartwatch  SDK Impressions","baseurl":"","url":"/2012/06/15/sony-smartwatch-sdk-impressions/","date":"2012-06-15 00:00:00 -0700","categories":["android"],"body":"I recently received a Sony Smartwatch as part of Sony’s promotion .  I haven’t been this excited about a watch since I was rocking my Casio Calculator Watch back in 2nd grade. Unlike many SDKs, my first impression when peaking inside the Smart Extensions SDK was that Sony actually has android developers employed!  This excited me since many SDKs feel like Android is a 2nd thought entirely.  Overall the SDK is fairly straight-forward in that reading accelerometer data, getting screen tap coordinates and drawing to the screen are all simple tasks with well commented sample projects. There were a couple features that I thought were missing and that felt foreign: Having to write button collision detection from scratch rather than using button click handlers Being unable to use XML selectors Being unable to use other Android UI widgets like ProgressDialog Support This may seem a small thing, but I like how Sony is approaching developer support for the watch.  I feel that their choice to leverage Stack Overflow was a great one.  I got a response quickly and am also able to do simple searches to see other smartwatch related questions. Growing Pains When I started writing layouts I was momentarily baffled, leading to this SO question .  This turned out to be because my drawables weren’t in the drawable-nodpi folder, oops! New Implementation for onClickHandler I was immediately surprised that there wasn’t a simple way to load a view on the watch and then have it respond to onClickHandler events like any other android app.  I know that the watch is simply a screen hooked up via bluetooth but I expected to be met with Android-like wrappers for some of the common tasks. My initial scan through the methods provided by the SDK gave me the impression that everyone was supposed to write their own collision handlers for a simple screen tap.  I was curious how the 8 Game sample did this and no big surprise, that’s exactly what they did.  So for instance they have rectangles defined: private static final Rect sActionButton1Rect = new Rect(0, 88, 40, 128); private static final Rect sActionButton2Rect = new Rect(44, 88, 84, 128); Then for each tap event on the watch, they do collision detection: if (sActionButton1Rect.contains(event.getX(), event.getY())) { ... } else if (sActionButton2Rect.contains(event.getX(), event.getY())) { ... } To me this felt like a huge step backwards in terms of the level of effort required to make even a simple smartwatch app.  Collision handlers aren’t difficult to write, but writing one that doesn’t break every time you get a bigger screen takes more effort than a simple onClickHandler.  I enjoy the luxury of leveraging higher-level constructs whenever possible, especially if they make my code more resilient to the array of hardware out there.  Sony may announce a 256*256 version tomorrow and now everyone has to rewrite their collision handlers! With this in mind, I set out to write a layout implementation that would provide the following features: Be able to trigger onClickHandlers attached to a view Be able to use XML selectors to change a drawable when pressed/not pressed Draw logic that could be gated at a specific frame rate that re-used bitmaps to avoid GC After an afternoon of thought, I decided to implement the following infrastructure: The difference between my approach and the approach of the sample source is this: The sample source creates an XML layout, renders that layout to a bitmap and then throws that layout away (along with the bitmap) for every screen draw.  This implementation holds onto the layout and bitmap for re-use. The sample source provides tap events as x, y coordinates.  This implementation forwards tap events to the layout where they are interpreted as android Tap/Hold/Release events that click UI elements as expected. Initialization for this implementation looks something like this (inside SampleSensorControl): mSmartView = new SmartWatchLinearLayout(mContext, WIDTH, HEIGHT); mLayout = (LinearLayout)LinearLayout.inflate(context, R.layout.pauseplay, mSmartView); mSmartView.setParameters(mThis, mLayout); //You can then hookup buttons like usual, the collision detection is handled by the view mPausePlay = (Button)mSmartView.findViewById(R.id.imageViewPausePlay); mPausePlay.setOnClickListener(new OnClickListener() { @Override public void onClick(View v) { mXbmc.playPause(); } }); //Tell the smartview to monitor this item for touch events mSmartView.addViewToWatch(mPausePlay); //You can also request the screen to redraw, the smartview only redraws in response to a touch event mSmartView.requestDraw(); Finally, plumb-in the onTouch events and you’re ready to rock: @Override public void onTouch(ControlTouchEvent event) { super.onTouch(event); //Forward this touch event on to the smartview mSmartView.dispatchControlTouchEvent(event); } Dispatching tap events to the layout allows Android to trigger selectors and onClickHandlers.  Essentially you get to circumvent all of the collision detection!  This isn’t perfect, but it is a pattern I’ll be using on my next smartwatch project, if nothing else, to use click handlers. I’m still not fully satisfied with the draw loop of the implementation.  I’d like to completely eliminate the need to manually request drawing but so far haven’t found a solution that does so reliably without being a resource hog. If you’d like to see the full implementation, you can grab the source for my XBMC Smart Extension . Final Thoughts I like the app possibilities that the Smartwatch brings to the table.  The SDK might feel a little bare-metal to casual Android developers but there are enough samples to get by on.  I would most like to see some kind of wrappers for the most basic Android UI Widgets like ProgressDialog. "},{"title":"ShiVa PoC: 2D Platformer","baseurl":"","url":"/2012/06/22/shiva-poc-2d-platformer/","date":"2012-06-22 00:00:00 -0700","categories":["shiva 3d"],"body":"I finally got around to a little proof of concept for a 2D platform type game.  This PoC is really part tool and part test.  The idea was to test out a design for a 2D platformer that uses the dynamics engine and to make a tool to tweak  physics parameters to dial-in the correct feel.  In the end I don’t think I’ll go forward with this particular implementation.  This project has brought to mind several issues that one might encounter when using the dynamics system for platformer physics. I now think that using the dynamics system for a serious 2D platformer is probably the wrong approach.  The first problem is that you don’t have fine-grain control over jump-mechanics.   For instance it isn’t possible to easily define a way to make your hero  jump exactly 3 game units high. I still haven’t entirely solved the (working perfectly/not working at all)   moving platform problem either. The Problem With Jumping In the jump state, I wanted to allow the hero to jump higher by holding the jump button longer.  This works, but only at high frame-rates.  The problem is a combination of 2 things: The dynamics engine has its own timestep that is independent of framerate The jump state can only be controlled once every frame The jump AI works like this: Apply impulse -> Wait for X seconds and then set vertical impulse to zero -> Apply additional upforce for the next Y seconds if the user is holding the button ->Apply gravity So you can imagine that if framerate gets low, the logic that controls when to start/stop different jump stages is delayed.  At the same time, the dynamics engine is still moving things along separate from the frame calculation.  The end result is that at lower framerates, the character ends up jumping higher because the jump logic isn’t running as often. It’s not that the dynamics system is bad and it’s not that there’s a problem with the engine.  The problem is that the mechanism used to update the translation of the hero is decoupled from the mechanism used to control the hero state.  Therefore to really gain control of the situation, we’ll need to implement our own physics system that translates the character in a frame-independent way. The Ideal System The ideal  platformer (yet another PoC in the works here) would let you define: Hero jump height in units of Hero tallness.  i.e. 3 units means the hero jumps 3x his height Jump acceleration Fall acceleration Hero jump height is critical if you’re designing a solid paltformer world.  The level designer needs to know exactly how many units high and far the hero can jump.  You might still be thinking, hmm I could break out a couple physics story problems to solve this with dynamics: What velocity v should be applied to an object for it to reach height h ? What impulse  j is needed to achieve the velocity v in the time interval dt ? I think at this point we’re just fighting against the dynamics system, and also I think you’d go crazy trying to find the right time step and iteration count to make the dynamics engine accuracy meet your calculated values. Depending on the type of game you are making, the behavior I’ve created here may be good enough.  Please let me know if you’ve used other approaches! Get the source "},{"title":"Android ListView Contents Disappearing","baseurl":"","url":"/2012/08/03/android-listview-contents-disappearing/","date":"2012-08-03 00:00:00 -0700","categories":["android","interesting problems at work"],"body":" Sometimes you encounter issues on Android that seem so blatantly simple, you can’t imagine that you are the first person to have hit them.  This seemed like a simple case where my TextView items were not being rendered sometimes when I scrolled my ListView. Stackoverflow to the Rescue In typical programmer fashion, this is the first place I turned where I found that I was in good company.  Unfortunately all of the people encountering the issue on SO had been hiding elements in their List and were forgetting to unhide them.  I wasn’t doing any tricks in my CursorAdapter, just setting the raw data to the TextView.  There was something out of the ordinary for my case, in that some of my TextViews were using arabic characters, some were using western characters while others were mixing arabic and western. Hierarchy Viewer to the Rescue In typical Android fashion, I then turned to the tool we all use when something is amiss with our layouts.  Hierarchy Viewer!  One of many tucked away tools in the SDK.  Frustratingly, if an Arabic TextView was present in my layout, Hierarchy Viewer would not run!  This led me to file a bug against the hierarchy viewer . Ok, no problem, I’ll just inspect a blanked out view that doesn’t have any arabic in it.  Now when I run hierarchy viewer, the act of running hierarchy viewer fixes my view?!? Pop quiz, what are 2 things that hierarchy viewer does to every element in your layout? Calls invalidate Calls requestLayout A simple change to my CursorAdapter to requestLayout after setting the text solved the problem. Note to Arabic Developers From the arabic proverb A horse that will not carry a saddle must have no oats. Or perhaps the more familiar version ويجب على الحصان الذي لن يحمل سرج ليس لديهم الشوفان. What does this have to do with Android?  Absolutely nothing!  But if you plan on using both of these in a ListView, prepare to **requestLayout **or there will be much tearing of clothes and gnashing of teeth. "},{"title":"ShiVa3D Flexible Keyboard/Joystick Input Architecture","baseurl":"","url":"/2012/10/02/shiva3d-flexible-keyboardjoystick-input-architecture/","date":"2012-10-02 00:00:00 -0700","categories":["shiva 3d"],"body":"There are a couple different ways to do keyboard and joystick handling in ShiVa.  This article explores a unique method whose merits will be compared against the traditional method. Here is the traditional way of handling input in ShiVa: The pros of this implementation are: Easy to understand for even the most casual observer Easy to implement for simple cases No performance drawbacks The cons of this implementation are: State logic is broken up If your game supports keyboard and joystick, you have to duplicate the state detection code in each handler Managing multiple AIs that require input can be tiresome Let’s look at these cons more closely. Cons of the Traditional Method State Logic Broken Up I love States in ShiVa, they let us cleanly segregate the states of our game.  What I don’t like about the traditional implementation is that if you open up a State onLoop function, you don’t see the whole picture of what is happening in that state. Instead, you have to open up the keyboard/joystick handlers in addition to the State code to see the whole picture.  Because so much logic is contained in these input handlers, eventually they can become bloated and difficult to maintain. Duplicate State Detection Code Once you add joystick into the mix, you have a situation where duplicate state logic has to be created (am I in the menu state or game state?), one set of logic for keyboard keys and another duplicate set of logic for joystick. Multiple AIs Now imagine you have more than one AI that needs keyboard inputs.  Now you have to forward events from your master AI to the other AIs.  Moreover, you have to duplicate the key input detection code and the state detection in each keyboard handler. The Stateful Method of Input Handling I’m calling this method the Stateful Method, this is what it looks like. The primary feature of this method is the logic behind the Input AI.  This AI captures button and joystick events and throws them into a hashtable.  It then provides a GetKey method that allows external AIs to query a key state.  I wrote this implementation because I was working on a project that has several disconnected tools and I was tired of managing individual keyboard handlers for all of them. Advantages Some advantages of this implementation are: The InputAI is abstracted in a way that allows any AI to access it.  No forwarding of input handlers is required.  Every AI now has convenient access to keystates with the minor inconvenience of including a couple helper functions. Interesting opportunities are now available, like having an object AI read its own input directly instead of being forwarded commands/input. AI States can now be fully self-contained Here is what a simple example would look like using the traditional method: And here is what the new style would look like: Disadvantages There are 2 primary disadvantages to this method. This method requires that you implement 2 helper functions in each AI that will query input (3 if you want to query latched input). Input update logic runs every frame rather than being initiated by input.  This could potentially mean performance issues down the road, although I have not noticed any.  It could also mean re-thinking the way you handle input in some AIs. Source The full source to this is available on github .  InputAI is where the magic happens, getKey, getKeyLatched and getInputAIReturnValue are the 3 required helper functions in GameAI. "},{"title":"Introducing ShiVa Android All In One (AAIO)","baseurl":"","url":"/2012/10/22/introducing-shiva-android-all-in-one-aaio/","date":"2012-10-22 00:00:00 -0700","categories":["android","shiva 3d"],"body":"What is this? This is an open source project that I’ve started to help ShiVa developers get the latest/greatest Android SDK implementations.  The goal for this project is to be a single Android project that contains the implementations for any Android SDK that you could possibly care about. There are 2 pieces to this project.  The Android code is where the SDK implementations live as combinations of Java and JNI (for interacting with ShiVa).  The ShiVa project is where SDK specific AI implementations go.  So for every SDK implemented, there is an associated ShiVa AI model that exposes the functionality needed to use the SDK from inside ShiVa. Why did I make it? I often see folks posting large tutorials in the forums detailing how to implement various SDKs in Android.  These tutorials are awesome and the community members involved in writing them deserve tons of credit.  What happens when you start a new project though?  Do you comb back over the forums, find the 3 SDK tutorials of interest and start copying/pasting all over again?  What happens when the SDK is updated with breaking changes and the tutorial author is busy? Wouldn’t it be nice if you didn’t have to copy/paste any of this code?  Wouldn’t it be wonderful if all you had to do was select the SDKs that you wanted to use?  I answer yes to these questions and this is why I have started this project. Why should you use it? As a ShiVa user, what would compel you to use this? Easier integration Instead of copying/pasting code, you only have to configure the SDKs you want to use. Oversight More than a single person is looking at the implementation for any given SDK to spot potential issues, this leads to better code. Bug filing and enhancement requests Besides integrating SDKs, this is a project where we can patch up the UAT generated code if needed (it already has a couple fixes).  Anything from device specific workarounds to fixing outright bugs. Prerequisites Before getting started, I am assuming that you’ve done the following: Installed  JDK 6 Installed Eclipse 3.6.2 or greater Installed the  Android SDK Downloaded the latest Android 4.1 dev tools from Android SDK Manager Installed the  Android NDK Installed the  ADT plugin Installed  Apache Ant > 1.8 Installed Cygwin Configured your system paths to be able to build in Eclipse Unfortunately there’s not an easy way to improve on the Developer eXperience for these essential requirements.  If your existing exported ShiVa projects aren’t compiling in Eclipse, don’t expect AAIO to compile either. Please do not comment here asking for build help, post on the ShiVa Android forum instead, the build process has not changed in AAIO. How to Download, Configure & Build Here is a full run-thru of how to get started using AAIO. 1. Download the source The source is on the github page .  You can download it by clicking the zip button. Once you unzip the project, you will have 2 folders: android The eclipse project. shiva The ShiVa project that contains all of the necessary AI Models along with a simple UI that allows testing the SDK integration.  This is also compiled as S3DMain.smf in the eclipse/assets folder. 2. Rename the Source This step renames the namespace of the source files and updates the build scripts with the correct NDK location on your system.  Obviously you don’t want your project namespace to be com.wordpress.mobilecoder.aaio and Android All In One right?  To begin, open up the android/configure.sh file, I recommend using Notepad++  to edit this file, especially make sure that under Edit->EOL Conversion it is set to Unix .  If you try to edit this in windows notepad and save it out, cygwin is going to have issues when you try to execute the script. Here is the portion of the file we’re concerned with. #####REQUIRED VALUES##### #The name of your package. #Example: com.atari.frogger newPackage=\"default\" #The name of your project class, this cannot contain spaces. #Example: froggerClass newName=\"default\" #The title of your project, this can contain spaces and is what is displayed on the Android home screen. #Example: \"Frogger Extreme\" newFriendlyName=\"default\" #The absolute path that contains the ndk-build command from the Android NDK #Example: \"/cygdrive/C/sdks/android-ndk-r7/\" newNDKPath=\"default\" #####END REQUIRED VALUES##### We need to edit all of the entries that don’t start with a hash sign.  After finding the location of the NDK on my system, I change the values to look like this (don’t forget the trailing slash on the ndk path!) #####REQUIRED VALUES##### #The name of your package. #Example: com.atari.frogger newPackage=\"com.example.aaio\" #The name of your project class, this cannot contain spaces. #Example: froggerClass newName=\"example\" #The title of your project, this can contain spaces and is what is displayed on the Android home screen. #Example: Frogger Extreme\" newFriendlyName=\"An awesome AAIO Example\" #The absolute path that contains the ndk-build command from the Android NDK #Example: \"/cygdrive/C/sdks/android-ndk-r7/\" newNDKPath=\"/cygdrive/C/sdk/android-ndk-r8b/\" #####END REQUIRED VALUES##### Finally, open up cygwin and browse to the path where the file lives and run the script Note for the advanced: If you ever need to rename your project again, you can edit the lower portion of the script with the new values you just entered. Import into Eclipse Fire up eclipse and select File->Import Select Existing Android Code Into Workspace: Browse to the same folder where the configure.sh script was and hit ok, you should see this (although obviously with whatever values you chose) at which point you can click finish: Copy the ShiVa Libs The ShiVa libs are not included in this project, to get them you must: Export an android project from the Authoring Tool In the exported project, copy the obj  folder to your new project and overwrite the existing obj folder If Stonetrip permits it, I will include this in the future (hint hint) Building Building works like it always has. Drag build.xml into the ant view Double click a build Enabling SDKs etc Now that you can build, you are ready to start enabling SDKs etc.  Please see the following section of the README . Final Hint If you’ve updated your ShiVa project and need to bring it into eclipse, here is all you need to do: Export as an STK file from ShiVa editor Rename the stk file to S3DMain.smf Copy S3DMain.smf into your assets folder, overwriting the previous file Build! "},{"title":"Jelly Bean LWP Retrospective","baseurl":"","url":"/2012/11/07/jelly-bean-lwp-retrospective/","date":"2012-11-07 00:00:00 -0800","categories":["android","java","marketplace  publishing","shiva 3d"],"body":"My brother and I have been working on a game for the upcoming OUYA console.  We got a little burnt out on a particular design issue and needed a breather, a small fun project to lift our spirits.  So we decided to try our hand at a Live Wallpaper for Android.  1 week later and here we are. Coming into this project, I knew it would be challenging, there were many things I had never touched before.  As always with programming, much of your success comes from standing on the shoulders of others.  This project was an exemplary example of that. The actual 3D rendering, physics system and lighting are provided by the ShiVa 3D engine.  It took about 2 days to write the ShiVa game that handles switching camera views, adding forces to jelly beans and triggering animations.  My brother did all the modeling and animation. Live Wallpaper Integration To integrate ShiVa into an Android app, you typically just export the project which generates a bunch of Java/JNI that you don’t really need to care about.  I have an open source project that integrates various SDKs into this exported code so I was already familiar with the implementation details.  ShiVa basically provides a GLSurfaceView implementation, like any openGL engine would.  Unfortunately, Live Wallpapers don’t use GLSurfaceView, they instead integrate a WallpaperService Engine which does nothing more than hand you a surface to draw on.  I needed a way to convert the GLSurfaceView and Renderer over to the live wallpaper engine and I did not feel like writing a GL thread from scratch.  The main reason I use ShiVa is so that I can spend my time designing my game, not my engine. GLWallpaperService to the rescue!  This open source project helped immensely, it took around 3 days of work to perfect the whole ShiVa/Wallpaper integration.  The list of tasks were: Slightly modifying the ShiVa GLSurfaceView.Renderer to use GLWallpaperService Moving the ShiVa engine callbacks into a Wallpaper Engine JNI to handle screen changing events JNI to handle engine initialization and wallpaper configuration Solving concurrency issues with the ShiVa engine (Live Wallpapers can run two instances of your wallpaper simultaneously, something that the engine was having an issue with). On that last bullet point, sometimes limitations have to be worked around in a creative way.  The problem is that the ShiVa engine can’t run two instances at a time because of some file locking that occurs inside the engine (to the best of my debug ability).  But, a live wallpaper can be running in the background AND be displayed in a preview window. To overcome this, I made the decision of rendering a static image/text on the preview window.  It’s kind of cheap but it gets the job done. Looking back over my commits for the last 7 days, I can easily find the one that marks the completion of the ShiVa integration as a live wallpaper: Android Experience The next step was the Android experience.  As I looked around at live wallpapers, I noticed that very very few live wallpapers: Installed an app in the launcher Had a settings page Made an effort to have a nice android UX I wanted a classy ICS experience with a settings screen front and center, ability to hit my social network pages and some smooth sexy scrolling. Obviously the first thing I did was use ActionBarSherlock .  Really, this is a no-brainer and I’m surprised that Google hasn’t replaced the support package with ABS by now.  I made a couple visual layout widgets, scribbled out a few designs and went to work.  This was one of the most time consuming tasks just because I iterated on the design several times. The first issue was that I wanted the sexy ICS on/off switches.  You can see below that I got them, backwards compatible down to android 2.1 thanks to an awesome open source library called android switch widget backport . The next thing I wanted was a nice color picker.  Once again a few minutes on google uncovered a gem called devmil android color picker .  I made a few hacks and was quite pleased with the results. That’s a total of four amazing Apache license based projects that cut tons of time off of this 1 week project.  My design skills are not amazing but this is definitely the best looking live wallpaper settings app that I have ever seen AND it looks great on the handset as well as the tablet. Marketing After wrapping up the Android interface and wiring everything together, I had to do the stuff I hate, marketing.  One of my deficits that I’m continually trying to work on.  This is the step of creation that I always forget about and that I struggle to get through.  But yet it is kind of essential if you want to make any return on your work. So I sat down and did my best to write some good app store copy.  I struggled to get good screenshots to use in the feature graphic.  I sat in photoshop for a few hours, bothering my brother at one point to make some cool looking text.   I ended up with this. The goal was simple, make a graphic that children can’t NOT click on.  Wife test passed, 3-year old niece passed, I’m too exhausted to add shadows, let’s ship this stupid thing! Final Thoughts It always amazes me how complicated even simple projects can be.  This was a difficult project to get right that spanned Java, C++ and LUA.  I didn’t mention the 1.5 days spent writing a licensing and in-app purchase verification server prototype in Google App Engine (that I later abandoned) or the 0.5 days spent mastering my native build process and proguard some details are even too boring for me to talk about.  Now that I’ve taken care of the boring framework, I can spend time writing/updating live wallpapers! "},{"title":"ShiVa3D HUD Interpolators Test","baseurl":"","url":"/2013/10/19/shiva3d-hud-interpolators-test/","date":"2013-10-19 00:00:00 -0700","categories":["shiva 3d"],"body":" A simple app to view all of the HUD Interpolators for the ShiVa HUD system, i.e. the constants: hud.kInterpolatorTypeLinear stkobject( \"800\" , \"600\" , \"/assets/shiva/HudInterpolation.stk\" , null, null , null , null , null , null , 0 , 1 , \"<V t='2' n='S3DStartUpOptions.BackgroundColor'>034,034,034</V>\" , 0 , 0 , 0 , 0 , 1, null , null , \".png\", 0 , 222222,1); Get the source . "},{"title":"Platformer Physics 101 and The 3 Fundamental Equations of Platformers","baseurl":"","url":"/2013/10/23/platformer-physics-101-and-the-3-fundamental-equations-of-platformers/","date":"2013-10-23 00:00:00 -0700","categories":["game theory","shiva 3d"],"body":" There are tons of tutorials out there on doing platformer physics and implementing various types of platformers .  What there seems to be lacking is a tutorial on how to choose good values for your platformer physics. This article will present some core physics equations in a new light!  I will even be so bold as to christen these equations as The Fundamental Equations of Platformers.  The sample code here is presented in LUA and for my prototyping I am using the ShiVa 3D game engine. When designing physics for a platformer, the 2 fundamental values required are: The strength of gravity The initial velocity of a jump With these values, we’re able to use the the kinematic equations to make a character jump and eventually touch the ground again. Basic Physics 3/22/2014 Please see comment by Ricky below. These simple equations are the calculus versions broken down for simplicity, not the kinematic equations. As a refresher, the 2 basic equations we’ll use are:   This would be a typical implementation. -- Set Y velocity to the jump velocity this.nVelocityY( this.kVelocityJump ) -- Apply gravity every frame local dt = application.getLastFrameTime( ) local newVelocityY = this.nVelocityY( ) - this.kGravity( ) * dt local distanceToMoveY = newVelocityY * dt -- Assuming collision detection was ok, move the actor object.translate( this.getObject( ), 0, distanceToMoveY, 0, object.kGlobalSpace ) this.nVelocityY( newVelocityY ) This code is at the heart of platformer physics, but when it comes down to it, these equations alone kind of suck at being useful. Picking random values isn’t the best way to get a good feeling platformer. Let’s look at a method that will help get you started with initial physics values. The Fundamental Equations of Platformers What makes more sense is to calculate gravity and initial jump velocity by picking 2 simple properties of the universe: Max jump height Time to reach max height Which leads us to the first two fundamental equations:   So what we’ve defined above are: gravity as a function of the time to reach the top of the jump and maximum jump height. initial jump velocity as a function of gravity and maximum jump height The Derivations Equation 1 Assuming that we are standing on the ground preparing to jump, we start with the kinematic equation Setting initial velocity to zero Solving for a yields Equation 2 Assuming that we are standing on the ground preparing to jump, we start with the kinematic equation Setting initial velocity to zero Solving for v yields Early Jump Termination Edited 3/5/2014 As reader Chue points out, the equations below don’t work unless gravity is negative . This is true and was an oversight on my part. You could redefine the above equation for gravity and just tack a negative sign on it. This also means that the equation for jump velocity is going to yield an imaginary number (square root of a negative number), you can just throw that imaginary part away. I’ve updated the equations below to show that gravity is being plugged in as a negative number. The end numbers haven’t changed, I just failed to show my work. The spreadsheet at the end of the article was not affected. There are a few ways to do early jump termination but I am going to propose a method that is based on a single parameter, minimum desired jump height .  The idea is to calculate the downward velocity required to achieve this minimum height. I now present the 3rd fundamental platformer equation (which is a well known kinematic equation): To use this, we choose our minimum jump height of 1 unit and we arrive at: So then in code, when a player releases the jump key, you’d do the following: if( this.nVelocityY ( ) > 0 ) then -- Set velocity to whatever is smaller, termination velocity or current velocity this.nVelocityY ( math.min ( this.kJumpVelocityTermination ( ), this.nVelocityY ( ) ) ) end The one caveat with this method is that, depending on your world, you may set a minimum jump height that is impossible for a human to hit. Because obviously we have limitations, like not being able to press and release a key much faster than 200 milliseconds. So, as a friendly check, we can calculate the last possible second that a jump can be terminated using this equation: Demo To prove out the equations and method, I did my best to recreate the physics from Mario 1 inside the ShiVa 3D game engine.  Based on my reverse engineering efforts (see details below) I arrived at the following values: * Max Jump Height = 4 units * Time to reach max height = 0.44 seconds * Minimum jump height = 1 unit So plugging these into equations 1, 2 and 3 we get Now see it in action, spacebar jumps. stkobject( \"800\" , \"600\" , \"/assets/shiva/PlatformerPhysicsTuned.stk\" , null, null , null , null , null , null , 0 , 1 , \"<V t='2' n='S3DStartUpOptions.BackgroundColor'>034,034,034</V>\" , 0 , 0 , 0 , 0 , 1, null , null , \".png\", 0 , 222222,1); How I measured Mario Gravity Starting with Nestopia, I recorded a video of mario jumping.  I then saved that out as an AVI with uncompressed frames.  After pulling that into VLC Media Player, I used the scene filter to save out every frame of the video to a png. The video was recorded at 50 frames/second which was a bit overkill, but this gave me 0.02 sec/frame. Instead of counting everything in pixels or trying to convert to meters.  I decided to use units of game height.  In Mario, every object is based on a square of 16x16 pixels.  So I called this a game unit. Using photoshop, I overlaid a grid to display individual pixels. Notice that mario sits 1 pixel into the ground. I then proceeded to make some basic measurements.  The time to reach the apex of a jump occurred in 22 frames * (0.02 sec/frame) = 0.44 sec The total height traveled in a jump was 64 px / 16 = 4 units It was a head scratcher to first determine how to measure the jump distance of mario based on how his animation changes when he jumps.  I decided to measure using the bottom of mario’s feet when standing and the bottom of his lower foot when jumping.  This is because when standing, mario’s feet extend 1 pixel into the terrain and while jumping, his bottom foot has a 1 pixel toe that at the highest jump point will extend 1 pixel into any reachable obstacle. So then the gravity for mario is 2h / t^2 = 41.32 units/s^2 And then initial jump velocity is sqrt(2gh) == sqrt(2 41.32 4) == 18.182 I thought I’d look up the values for Mario’s gravity and see if perhaps I could compare my measurements.  I was excited when I found  Acceleration Due to Gravity: Super Mario Brothers  until I saw their measurement of the height of mario to be 39 pixels.  Perhaps this could have been due to over or underscan on their TV?  I mean, this value isn’t even a power of 2.  Mario is 1616 small and 1632 with a mushroom, everyone knows that! Conclusion This method may not match the exact jump model that mario uses, but as an approximation, I think it works very well. I’ve created a Google Docs template with the above equations built-in. Feel free to make a copy of it to calculate some starting values for your own platformer. Also if you’ve used the above method or see an issue with implications of the early jump termination method, please leave a comment! "},{"title":"Profiling OUYA (Tegra 3) Games using Nvidia PerfHUD ES","baseurl":"","url":"/2013/11/14/profiling-ouya-tegra-3-games-using-nvidia-perfhud-es/","date":"2013-11-14 00:00:00 -0800","categories":["android","graphics","ouya","shiva 3d","tegra 3"],"body":"PerfHUD ES has a good online manual  that I’m not trying to replicate.  The manual explains the UI, graphs and knobs. The purpose of this article is to introduce the tool to people that aren’t aware of it as well as to provide more details behind the directed tests section.  Specifically, how to make your game faster based on the results you get from the directed tests. To enable perfhud, in a shell run: adb shell setprop debug.perfhudes 1 Then fire up your game, launch perfhud and connect to your device. The Frame Debugger The frame debugger is so incredibly useful and awesome that it’s hard to describe it all.  Here are the top reasons. Seeing every draw call When you see every single draw call and exactly what it is drawing it is amazing.  You will discover things that you didn’t know were in your scene, things that shouldn’t be visible and other amazing stuff. Inspecting Geometry Yes, you can fully inspect geometry.  This can be incredibly useful for validating your mesh frustum culling.  You can even take a peak at games that you don’t have source to and reverse engineer special FX! Validating Mipmaps With the texture viewer, you can validate mipmaps and check your texture compression.  Seriously, amazing. Hot-Replacing Shaders You can view all the shaders used in your game in this tab.  You can see exactly which draw call(s) use them, how expensive the shaders are and can even replace the shaders on the fly. I found this incredibly useful when tracking down expensive shaders.  I would simply hot-replace a shader with a barebones shader to see if my performance increased. Directed Tests The directed tests area is where you’ll solve most of your performance problems.  The test methodology here is: Look at FPS (at the bottom of the screen) FPS is displayed as Current FPS/10 second Avg FPS Click a directed test Look at FPS If your FPS made a noticeable increase, then you now know what area your problem is in.  If not, continue to the next test.  We’re not going to cover all of these because frankly as someone who uses a game engine, I’ve never found the last 5 options helpful in solving performance issues. For each of these checkboxes, we are going to answer 2 fundamental questions: What does the checkbox do? If the checkbox increased framerate, what should we do? Note that performance is a very complicated subject.  Anytime you see the word is replace it with is most likely  and imagine me making little quotation signs in the air with my hands. Textures What it does Replaces all game textures with a texture measuring 2 pixels x 2 pixels. What to do If this increases your framerate then your bottleneck is most likely in the texture unit. Look at eliminating this bottleneck by the most common methods: Reduce texture count by combining multiple textures into a single texture atlas Reduce texture size e.g. from a max of 20482048 to 10241024 Use mip-maps, otherwise your 11 pixel object still uses a 10241024 texture! Use a faster texture filtering method, e.g. bilinear instead of anisotropic filtering Increase texture compression Ignore Draw Calls What it does Stops drawing to the screen by ignoring all DrawArrays and DrawElements calls.  Essentially simulates having the fastest possible GPU that could ever exist.  This is a good test to determine whether you might be CPU bottlenecked. What to do There are certainly 2 sides to the draw call coin.  If FPS does not improve when checking this, you have a CPU bottleneck.  If you are CPU bound then you need to take a good look at your code.  Basically stop here, you need a different set of profiling tools to help you find the code at fault.  Look to your debugger or do a simple pause test to track down the offending code and then apply optimization recommendations for your specific language. If FPS improves then you have a GPU limitation.  Some of the other tests will hopefully narrow down which specific GPU bottleneck you have.  If you’ve exhausted the other GPU tests without any improvement, then you may fall into 2 additional cases. Poly Count You are simply drawing too much geometry.  You should try to: Reduce your mesh and/or UV complexity Use frustum culling for large meshes Use Level Of Detail to reduce mesh complexity at a distance Draw Calls You are trying to draw too much in a single frame.  You should: Combine multiple meshes into a single mesh (possibly at runtime) Use batching (use the batching histogram to help determine if you aren’t batching efficiently) Remove frivolous objects for lower-end devices (ocean simulations and animated foliage come to mind) Disable Vsync What it does Prevents calls to eglSwapInterval from locking the framerate.  This allows seeing the true framerate of your game rather than, for instance, a max of 60 fps.  On mobile, this option has not been very helpful to me. What to do If vsync is making a significant difference, then you can look to your engine docs to determine whether it can be disabled.  There’s always the visual tradeoff here (screen tearing vs slight fps increase) on the PC.  I know that on iOS, you cannot disable vsync.  I am unsure if you can disable vsync on Android, it may be device or vendor specific. Null Fragment Shader What it does Replaces all fragment shaders with a simple and fast fragment shader. What to do If your fps improves then your fragment shaders are too complex.  If you are working in a game engine like ShiVa 3D then this means that you are doing a little too much in terms of materials and effects: Reduce or eliminate Post Processing (SSAO, Bloom, Gamma, Color filters, etc).  Or possibly, use offscreen rendering to render post effects at a smaller size to boost post processing speed.  Here’s a hint, if enabling null fragment shaders makes your entire screen disappear, it’s probably because you are using post-effects you can actually scrub through the frame scrubber and find a draw call that is blitting this frame to the screen to validate Reduce number of dynamic lights per object.  If you are using per-pixel lighting, you may need to change to per-vertex. Reduce shadow complexity Reduce material complexity for instance if you have a material animation that uses opaque textures, the cost to calculate each pixel of the material may be high. Null Viewport What it does Where Ignore Draw Calls just waits for draw calls and throws them on the floor, Null Viewport still renders everything but at an infinitesimally small size. What to do Drill down into specific tests Blending, Fragment Shader, 22 Textures.  If none of these are increasing your framerate, look at draw calls, polycount or disabling post-processing effects. Disable Blending What it does Prevents all alpha-blended drawing.  Arguably one of the most handy checkboxes.  Not only does it let you see if you are getting hit by overdraw, but it also visually shows you the areas where overdraw is happening in your game. What to do Uhh, get rid of alpha blending!  This could mean: Reducing the emitter count of a heavy particle system Swapping out opaque textures for solid textures Reducing the number of opaque textures that you layer on HUD items Reducing the number of lights hitting the same object (each light must be blended) "},{"title":"Make Your Ouya Games Run at 60 FPS with This Sneaky Trick!","baseurl":"","url":"/2013/12/14/make-your-ouya-games-run-at-60-fps-with-this-sneaky-trick/","date":"2013-12-14 00:00:00 -0800","categories":["android","game theory","ouya","shiva 3d","tegra 3"],"body":" I’ve been playing around in perfHUD ES a lot lately profiling OUYA performance.  One of the cool features it provides is the ability to look at and replace shaders on the fly.  I thought this was a great opportunity to investigate how costly various shaders are. If you’re only here for the graphs, scroll to the end.  If you’d like a little GPU 101 to help interpret the graphs, then read on. WARNING! This topic is a total Pandora’s box of awesomeness and potential confusion!  I’m going to do my best to help you assimilate some of the industry knowledge behind GPU performance in this article in a way that’s hopefully useful to you as a game developer. If you see any incorrect statements, please leave a comment or hit me on twitter so I can correct it/them! Everything in this article was measured on the Tegra 3 (OUYA), which means Android 4.1.2.  My 3D game engine of choice is Shiva Engine , but the knowledge I’m presenting applies to any game engine. GPU 101 GPUs are. . . complicated!  Let’s look at them from the perspective of a game developer working in a 3D game engine. Forget everything you know about GPU Architecture and let’s dive in! Here’s what my game looks like, it’s a riveting game consisting of 3 primitives. GPU Architecture Let’s use the above scene to break down the architecture of the GPU. So, every model in your scene lines up on the conveyor belt, waiting for his very own Draw Call.  When his turn (draw call) comes, he’s passed through the vertex shader(s), then the fragment shader(s) and is finally thrown onto the screen (technically not really, but from a high-level view just go with it).  When all the draw calls are finished, your game has officially drawn 1 frame.  Hopefully it does all this 60 times a second, because we all love 60 FPS right?! A Bit About Hardware There are 2 types of GPU Architectures out there.  Fixed function and Unified.  The super important difference between these comes down to how the Vertex Shaders and Fragment Shaders are implemented in hardware. Fixed Function A fixed function architecture is kind of old school.  It designates a fixed number of vertex shaders and a fixed number of fragment shaders.  Fixed means unchanging,  meaning that vertex shaders can’t do anything besides vertex shading. They are stuck in life doing nothing but running vertex programs.  You will find fixed function architectures in mobile devices because they can be made more power efficient. An example of a fixed function architecture is the Tegra 3 which has: 4 Vertex Shaders 8 Fragment Shaders Unified Unified shaders are what you find on desktop GPUs.  In this architecture, there is a pool of shaders that can be used as either vertex shaders or fragment shaders based on whichever is required.  So if there are a ton of vertices that need processing, the GPU can use all the shaders as vertex shaders. My laptop’s Nvidia GTX 650M has 384 Unified Shaders.  Ever wonder why the divide between Mobile and Desktop gaming performance is so big?  This is why, a ton of unified shaders vs a handful of fixed function shaders. Shaders If you don’t write your own shaders then you are most likely working with materials.  You might be setting a texture map, turning on per vertex or per pixel lighting, setting diffuse and ambient colors etc. In the end, all of these material settings are transformed into 2 programs that run on the GPU.  These 2 programs are then run on the vertex and fragment shaders to produce the desired look of your material. Some material options will generate vertex and fragment programs that are more expensive than others.  For instance, the fresnel effect is very expensive!  Towards the end of this article there are some helpful charts to help you decide which material options you might want to avoid on mobile devices. Vertex Shader What does this guy do?  Things. . . magical things!  Even if you don’t write your own shaders, you should understand some basics about vertex shaders:  A vertex shader is a program that runs on the GPU The program runs once for every vertex in your model A vertex program’s runtime is measured in cycles One vertex shader cycle directly corresponds to a single GPU clock cycle All vertex programs for a single draw call are run in parallel if there are enough slots Huh? Why would you how do I Let’s break this down into game developer english! A Simple material with no lighting =  12 GPU cycles A Simple material with Per Vertex lighting =  23 GPU cycles 12 GPU cycles means that the vertex shader program for the material takes 12 GPU clock cycles to run.  So, let’s take our yellow cube as an example. The cube has 24 vertices and is using Per Vertex lighting.  This means that for the cube’s draw call, we are going to: Send 24 vertices to the Vertex Shader(s) For each vertex, a program taking 23 cycles is going to run The 24 programs will run in parallel.  Obviously if we have only 4 vertex shaders on the GPU, then only 4 programs can run at a time while the remaining programs wait for an empty slot. Whoah Whoah Whoah! Bottleneck Alert?! I know what you’re thinking!  You’re looking at the Tegra 3 architecture, you see the 4 little vertex shaders vs the 8 fragment shaders and are thinking, these 4 little dudes are obviously going to be my bottleneck!  Hmm, let’s just take a peek at the math and find out! Sticking with Tegra 3, we look up his GPU specs  and see that the GPU runs at 520 MHz, cool!  Let’s focus our thoughts around a single frame.  We can calculate the number of Vertex Shader clock cycles that are available to us in a single frame at 60 FPS. OK, let’s make this even more useful.  Let’s say we’ve decided we want to use Per Vertex lighting on all our objects. Knowing that Per Vertex lighting costs 23 cycles, we can actually calculate a ballpark for the maximum number of vertices per frame. Santa came early this year because that's a ton of vertices!  Just looking at the number of vertex shaders alone can be quite deceiving as you can see.  Now, these calculations involve a little bit of hand-waving because things just don't quite work like this, see the reality check section below.  However, this still serves as a pretty good baseline for things. I would be negligent not to mention the official recommendations by nvidia for Tegra 3.   Peak throughput is achieved at 10 cycles per vertex . Fragment Shader (also called pixel shaders) Like vertex shaders, fragment shaders are just another program that runs on the GPU.  Here are their highlights. A fragment shader is a program that runs on the GPU The program runs once for every fragment generated by your model At least 1 fragment is generated for every pixel of your model, more complicated materials may generate more fragments One fragment shader cycle directly corresponds to a single GPU clock cycle All fragment programs for a single draw call are run in parallel if there are enough slots The main thing to understand about fragment shaders is the number of fragment programs that are generated.  In most cases, you are talking 1 fragment per pixel. To the right you can see a faked example.  Basically, every pixel that is occupied on the screen is turned into a fragment.  In reality, this cube is about 120 x 120 pixels and would therefore generate 120 * 120 = 14,400 fragments.  What’s most important to remember is: The larger a model is on screen, the more fragments it will generate More complex materials require more cycles Knowing this, you can see how you might want to be wary of materials that eat lots of fragment shader cycles, especially if they occupy a large portion of the screen! Bottleneck Check Let’s do some off the cuff math to estimate the power of our fragment shaders on the Tegra 3.  First, how many pixel shader cycles do we have per frame? Twice as many as we do fragment cycles. How many full 1080p frames can we render if each fragment program were 10 cycles? Well, how many cycles does it take to calculate a single 1080p frame using a material that costs 10 cycles? OK, how many of those can we do per frame? Again, a bit of hand-waving going on, but the main point here is that at 1080p, the fragment shaders will almost always be your bottleneck.  You can see how post-processing is basically out of the question at full frame resolution since enabling bloom, gamma and contrast filtering would consume all your cycles. Now you can see why using off-screen rendering can give a huge boost to your game.  Your game can render out at 720p and then upscale to 1080p like the big boys . Reality Check For all of my efforts above where I try to give some ballpark numbers, you should know the truth.  The numbers are a lie (very much like the cake).  You see, the numbers I gave would only ever be valid if you were drawing a single piece of geometry with a really snazzy material. Most of our games consist of quite a bit more than a single draw call.  Here’s the rest of the story.  Before the next draw call can happen, the GPU has to wait for the fragment shaders to finish processing the previous draw call.  So something like this: Draw Call 1: Vertex Shader -> Fragment Shader -> Done Draw Call 2: Vertex Shader -> Fragment Shader -> Done Draw Call 3: Vertex Shader -> Fragment Shader -> Done Think about what this might mean in terms of performance.  Let’s say you were drawing  50 cubes with all the bells and whistles, real-time lighting, bloom, camera blur, gamma adjustment, contrast filter, SSAO.  In this situation, your vertex shaders will be twiddling their thumbs while the fragment shaders get rocked.  In fact, you could increase the geometry of your cubes by a TON and your performance would stay exactly the same! This is an important concept to remember, because if you know that your bottleneck is the fragment shaders you can often get some free visual improvements by feeding starving vertex shaders. Putting it all together Thanks to nvidia perfHUD ES, I was able to look at the cost of shaders for Shiva 3D.  It’s pretty darn simple, perfHUD shows you a list of all your shaders and which draw call they are associated with.  You can click on a shader and see how many vertex and fragment cycles it consumes. Below I present the results of my findings organized by lighting and material type.  I also include a special case for the fresnel effect. Because fresnel can be applied to any material, I show the results without fresnel (Normal) and with. No Lighting Vertex Lighting There is a special caveat with fresnel based lighting here.  When using the fresnel effect with vertex based lighting, you incur 1 extra draw call on your material.  The fresnel numbers below are arrived at by adding the vertex and fragment cycles of both draw calls. Pixel Lighting I had to run the fresnel test several times to double check these results.  You will note that with fresnel enabled for some material types that you incur less fragment cycles!  I’m only reporting the numbers here, I don’t have a great explanation for them. It could be that the difference in cost is due to the fresnel affect visually dominating what we see, for instance think of the fresnel effect as the Sun and the original material as a flashlight.  No point in turning a flashlight on when it’s competing with the sun, right?  On the other hand, why isn’t this true for the vertex/no light cases?  PURE speculation here that could be resolved if I went back and stepped through both shaders to see what operations they’re performing. That’s all!  If you found this useful, please leave a comment! "},{"title":"Stormtrooper's Guide to Droid JTAG'ing","baseurl":"","url":"/2013/12/19/stormtroopers-guide-to-droid-jtaging/","date":"2013-12-19 00:00:00 -0800","categories":["android","hacking","interesting problems at work"],"body":" TOP SECRET EMPIRE PERSONNEL ONLY You’ve done it!  You’ve finally found the droids you were looking for.  But hey, wait a minute, they don’t even power on?!? How are you supposed to re-purpose them for your evil schemes and recover all those juicy princess holocrons? Buckle-up soldier! It’s time to JTAG! ### What’s JTAG? JTAG is a debugging interface that pretty much every CPU has on it, even droids!  Once you hook up, you can read/write to flash memory and change out those bad motivators! Step 1. Gather Tools There are some necessary tools to get this job done right. Torx screwdriver (usually t5 or t6) Solder (no larger than 0.022) Flux Soldering station Wet sponge It’s important to use a temperature controlled soldering station.  We recommend a temperature of ~ 700° F. Step 2. Remove the Case Don’t be fooled by this step.  The Rebel Scums have put plenty of traps in your path to make you fail.  Follow these guidelines and keep your blaster ready! Keep screws organized If the plastic isn’t budging, look for a missed screw Every time a plastic piece is removed, look for more screws Use a small flat-head screwdriver to help slide the case open Be warned!  When the case comes off, 2 plastic pieces will fall out as if something has broken.  Tell your armed escort about this ahead of time to avoid any potential confusion.  Then put the pieces aside until needed for reassembly.  If by accident you do break a piece of the case, you may want to google:  Stormtrooper helmet with anti force choke . Step 3 Disconnect Motherboard Cables With the case off, you can now disconnect the motherboard cables.  There are usually 2 or 3 cables that need to be removed.  Just flip that little white lever and the cable slides right out.  Keep limbs and weapons free and clear of the USB port. Once all ribbon cables are removed, you can detach the motherboard from the screen. Use proper lifting techniques as illustrated below.  Optionally, you may equip utility belt UP-37 for more lifting support. Step 4. Reveal JTAG Pins Once you flip the motherboard over, don’t be fooled by the deception of the rebellion!  Fire a few blaster shots into any mysterious adhesives you find.  If you are unable to find the JTAG pins, load training module JTAG Holosearch . Step 5. Connect to JTAG Pins OK trooper, there are a lot of options here.  Depending on how much training you have, you could go one of several routes.  Be sure to check with our Intergalactic Provider regarding some of the pre-made JTAG units available. Maybe you’re using a raspberry PI or maybe you have a custom JTAG box like Medusa or RIFF.  Worst case, you’ll have to solder all the JTAG pins yourself.  If you’re soldering yourself, be sure to always flux the pads and then tin them.  The power of the flux rivals the powers of the light and dark-side combined! If you haven’t yet completed module: The Zen of Force Based Soldering  then here are some helpful tips: Use flux! Use a small amount of solder on the tip of the iron to conduct heat to the pin pad Tin the pad before trying to solder a wire to it Heat a tinned pad with a small amount of solder on the tip and then push the wire into the solder You should never use large gauge wire to solder to the phone PCB, doing so will put too much strain on the joint and will result in a lifted pad.  You will eventually lift a pad, it happens, better to practice your skills on some old R1 units before doing the real deal. If you only ever have to JTAG a single unit, then soldering wires will get the job done.  But, most of us vets use JTAG jigs.  They’re a specially made PCB that is pressed onto the JTAG contacts using a clip.  These units are safer (no lifted pads), involve little or no soldering and they make JTAG’ing multiple units a snap! You could make your own if you have time or you can buy them pre-made especially for your device and JTAG hardware combination.  They cost slightly less than a stein of grog at your local cantina. When JTAG units are connected and working, do not bump, breath or look at them wrong! Step 6 Cleanup Clean up the extra flux with some rubbing alcohol before you power up. Step 7 Commence with Evil Plans Looks like you’re all set to go.  Good luck with that R2 unit :) "},{"title":"ShiVa 1.9.2  New API Calls","baseurl":"","url":"/2013/12/24/shiva-1-9-2-new-api-calls/","date":"2013-12-24 00:00:00 -0800","categories":["shiva 3d"],"body":"With the newly released ShiVa 1.9.2 Game Engine  came a bunch of new API calls.  Not all of these calls have been documented, until now!  Head over to my github repo for a comprehensive list of new API calls that showed up between 1.9.1 and 1.9.2. The biggest highlights in my mind are. ## Average Frame Time application.setUseAverageFrameTime ( bUse ) This should be a default.  It makes all the internal engine calls use average frame time.  This could mean smoothing out jitters in the dynamics system and is something everyone should try. Collider Create/Destroy collider.create ( hObject ) collider.destroy ( hObject ) Being able to create and destroy colliders at runtime solves many problems, particularly if your meshes are being combined. Offscreen Output, Clipping Children, List Item Children Offscreen output lets you easily render a HUD to a rendermap. hud.enableOffscreenOutput ( hUser, sRenderMap, bEnabled ) Enabling children clipping will clip children that spill out of their parent container. hud.setContainerClipChildren ( hComponent, bClip ) List item children allows you to use a HUD template inside of a list! hud.setListItemChildAt ( hComponent, nItem, nColumn, hChild ) Disabling Logging Possibly one of my absolute favorites!  Shuts down those noisy logs: if system.getClientType ( ) == system.kClientTypeEditor then log.enable ( false ) end Toggling AI Models Now instead of adding/removing AI models, you can disable them. This provides a very clean solution for toggling sets of functionality! object.enableAIModel ( hObject, sAIModel, bEnable ) user.enableAIModel ( hUser, sAIModel, bEnable ) Attractors, Vortex Fields and Turbulence The particle system gets better with every release!  There are simply too many new functions here.  Here’s a sample that creates a vortex field. local hBox = application.getCurrentUserSceneTaggedObject ( \"box\" ) sfx.addParticleVortexField ( hBox ) sfx.setParticleVortexFieldAxialDrop ( hBox, 0, 0.1 ) sfx.setParticleVortexFieldAxialDropDamping ( hBox, 0, 0 ) sfx.setParticleVortexFieldOrbitalSpeed ( hBox, 0, 1 ) sfx.setParticleVortexFieldPosition ( hBox, 0, 0, 0, 0, object.kLocalSpace ) sfx.setParticleVortexFieldRadialPull ( hBox, 0, 3 ) sfx.setParticleVortexFieldRadialPullDamping ( hBox, 0, 0 ) sfx.setParticleVortexFieldStrength ( hBox, 0, 1 ) Everything Else You’ll find the rest of the API changes on my github page , feel free to submit changes or samples. "},{"title":"Obfuscating ShiVa Games with Proguard","baseurl":"","url":"/2014/01/16/obfuscating-shiva-games-with-proguard/","date":"2014-01-16 00:00:00 -0800","categories":["android","hacking","shiva 3d"],"body":" If you are worried about people hacking your ShiVa based game then obfuscation is a good place to start.  Obfuscation is very simple to implement in your Android project and is a good starting point for anti-hacking measures. Nothing will stop a truly determined and experienced hacker, period.  If you rate the skill of hackers from 1-10, you might think of obfuscation as preventing those of skill 5 or less.  When it comes to fighting people that are decompiling your code, you won’t win all the time.  Obfuscation is easy to implement and makes a confusing mess of your code, so it’s a great low-effort way to prevent the script-kiddies from screwing with your purchase code. To successfully work with obfuscation, this tutorial will show you: How to modify your build to enable obfuscation What to save after each build to allow de-obfuscating stack dumps How to de-obfuscate stack dumps Modify Your Build By build  I mean an Android Project UAT build.  In other words, you’ve gone through the Authoring Tool, built an android project from your STK and have unzipped the resulting zip file to access the build files. Step 1 project.properties Edit project.properties and uncomment the line to read: proguard.config=proguard-project.txt Step 2 proguard-project.txt Replace the contents of proguard-project.txt with: Step 3 modify line 17 of proguard-project.txt You need to change line 17 and replace the bracketed variables.  Here is a before/after example showing the replacement of the package and class name. Before -keep class [_PACKAGE].[_CLASSNAME]{ After -keep class com.error454.example.Main{ Now you can build with ant. What to save Every time you build your app, proguard will generate files under bin/proguard.   It is important that you save the mapping.txt file so that you can associate it with a specific build of your app.  Once you’ve uploaded an obfuscated build to the android market, the stack traces that are generated in user reports are going to look like non-sense.  The mapping.txt file will allow you to transform them into the helpful stack traces that you remember. Remember, every time you do a final build for the app store, save your mapping.txt, preferably as a versioned file in a source repository. How to Deobfuscate The android SDK includes a tool called retrace  that you’ll find under  tools\\proguard\\bin .  To deobfuscate your dump, you run: retrace mapping.txt obfuscated_dump.txt Conclusion & Caveats With just a few simple steps, you’ve obfuscated your game.  Keep in mind that many SDKs have specific changes that need to be added to proguard-project.txt, so be sure to check their instructions for any required additions.  I hope this tutorial was helpful.  If you are interested in some penetration testing against your game, please let me know. "},{"title":"3D Platformer Character Animation Notes","baseurl":"","url":"/2014/01/28/3d-platformer-character-animation-notes/","date":"2014-01-28 00:00:00 -0800","categories":["game theory","graphics","shiva 3d"],"body":" We made a 3D platformer with a constrained 2D view not long ago, obviously in our engine of choice, ShiVa .  The main character fired a number of different ranged weapons and generally made a huge mess of some unfriendly boxes that were invading his planet. I faced a number of challenges when implementing the character running, aiming and shooting animations and I thought it would be useful to share my solutions for the next indie.  In this post is a web demo, you’ll need to let your browser load the shiva plugin to see it. The primary topics that I’ll discuss are: Syncing character run speed and movement rate Aiming the gun Shooting the gun Animation track mixing Run Speed & Movement Rate I remember when the first Unreal Engine came out, there was an article with one of the ID dudes (I want to say it was Carmack) and he was kind of slamming Unreal Engine because their monsters slid across the ground.  Basically, the rate that the character was being translated across the ground and the playback rate of the walk cycle were not synced up to look good.  I like to think of this relationship between animation speed and movement rate as a zone that will from now on be called the Zone of Awesomeness. The Zone of Awesomeness (ZoA) is where you want to be, it’s the fine green line running down the center of the graph.  If the animation speed becomes faster than the movement rate and goes above the line, the character begins to look like s/he’s running on ice.  If the animation speed drops below the movement rate, the character looks like s/he is sliding.  There are times when you may want to have the running on ice effect, (you know like when you’re actually running on ice) but you’ll rarely ever want the sliding effect (unless your game is about ice skating). Staying in the ZoA Our game had a speed boost which increases the movement rate of the character.  I wanted the animation to automatically adapt to the character’s velocity so that we stayed in the ZoA.  Below is the method that I used to do this along with the prototype I made to plug values in. Like most prototypes, the concept was simple enough, it required: A scene where the character can run left/right and allow me to clearly see his feet Ability to modify character movement rate Ability to modify animation speed Controls 1/2: Decrease/Increase Movement Rate 3/4: Decrease/Increase Animation Speed A/D: Move left/right Mouse: Aim/fire 9: Toggle auto-animation speed calculation stkobject( \"800\" , \"600\" , \"/assets/shiva/charanim.stk\" , null, null , null , null , null , null , 0 , 1 , \"<V t='2' n='S3DStartUpOptions.BackgroundColor'>034,034,034</V>\" , 0 , 0 , 0 , 0 , 1, null , null , \".png\", 0 , 222222,1); Finding an Equation for the ZoA The goal then is to have a function where you pass in movement velocity and you get back the correct animation speed: Animation Speed = f(Velocity) To arrive at this equation for animation speed, I could have done a bunch of math, but I took the lab approach. I decided to collect data and then interpret the results.  Here was my Character Animation Lab 101: Pick a velocity for the player Adjust animation speed until it looks good Log the player velocity and animation speed Perform steps 1-3 for several different velocities that will be used in the game This gave me the following set of data values.  As you can see, I charted the values and then using the cheating powers of excel, I added a trendline to the chart and selected linear regression.   It was pretty clear that this data was linear and not a polynomial, exponential or quadratic!  Notice that my R² value is 0.9997 which indicates that this is a good fit. If my R² value was < 0.9 then I may have used a series of linear equations based on the velocity range that I was in.  Perhaps switching between 2 or 3 equations based on velocity ranges.  It really depends on what the line looks like. To wrap up, my equation for animation speed was: animation speed = 2.9 * velocity * 0.56 To see it in action in the demo, press the 9 key to turn on the animation calculation and then change the run velocity, magic! Aiming the Gun Aiming the gun was one of the more conceptually difficult problems to solve. How do we animate the character across a full 180 degree arc (facing one direction) so that he aims exactly where the player is pointing? Now how do we do this while he’s running/jumping? We considered manually transforming bones in code to move arms, gun and head where the mouse or analog stick was aimed.  The problem with this method is obviously that a ton of bones move when you’re aiming, pretty much the entire upper body.  So things looked pretty robotic doing this and it turns out that my code animation skills are not the best. I scoured the internet for ideas on how to approach what would seem like a common problem aiming the gun of a character but surprisingly came up empty. I studied the rogue in Trine 2 for quite some time, being about as close as I could get to a well implemented reference. After a lot of scribbling and head scratching, I came up with my own solution.  To the reader that’s just now typing in the comments Why didn’t you use XYZ Gamasutra article from 1992? /shakesfist where were you 6 months ago :( I told my brother to make an animation 360 frames long where the character start position was aiming straight up and the ending position was the character aiming straight down at his feet.  This animation only involved the upper body, so I deleted the un-used animation tracks so as not to interfere with say running.  Also it was important that the animation moved at a constant speed with no tweening at the start or end. This animation gave me a full 180 degree range of motion, every possible angle I needed to aim! I wanted it 360 frames long so that I would have double resolution for every degree of animation, although in retrospect this may have been overkill. Now, instead of triggering this animation and letting it play out, I manually controlled the exact keyframe it was on based on the angle that the character was aiming. In practice, the whole thing was one simple interpolation, with the most difficult part being converting the incoming angles so that the origin was 0 degrees when aiming straight up. Doing this made the remaining math much easier because then I could easily interpolate between the minimum and maximum keyframes by the number of degrees / 180: local nCurrentKeyFrame = math.interpolate ( 0, 360, nDegrees / 180 ) Shooting the Gun I was hoping that our gun positioning would be so precise that all I had to do was spawn a projectile at the tip of the gun and let it rip across the screen. Maybe this is just noob wishful thinking because this wasn’t the case, although I tried it at first. What ended up happening was that projectiles couldn’t hit anything, they were off by just a fraction of a degree and would go too far into the screen depth wise.  Also, when running, the gun bobs and pitches ever so slightly and this makes using the gun as a reference point inaccurate. I settled on a method that I’m calling the virtual aiming cursor. The goal is to position a helper object where the projectile will be spawned and have it look at the destination. Doing this makes it easy to use a mouse or analog stick to do the aiming, much of the logic is the same. For the mouse, I find the vector from the camera to the mouse cursor.  I then do a ray-plane intersection to find the actual point on the 2D plane where my character should be aiming, you have to do this to make sure your projectiles stay in the correct plane of existance!  I then used the head bone on the character and find the vector from the head to the intersection point on the 2D plane.  Once I have this, I calculate a vector offset to compensate for the length of the gun, so instead of spawning the bullet on my character’s head, it spawns near the tip of the gun. From here, position the helper, have it look at the intersection point and we’re ready to fire! This method was close enough and most importantly prioritizes accuracy.  The projectile may not be perfectly matched to the barrel, but it always goes exactly where you are pointing. Animation Tracks The final piece of the puzzle was mixing animation tracks.  Or more accurately, deleting animation tracks.  I had to hand delete a number of conflicting animation tracks.  For instance in the run animation, I deleted all the tracks that the aim animation uses.  In the aiming animation, I deleted all of the lower body tracks so that the run animation could take over. "},{"title":"Quicksort for ShiVa3D","baseurl":"","url":"/2014/02/10/quicksort-for-shiva3d/","date":"2014-02-10 00:00:00 -0800","categories":["shiva 3d"],"body":"If you ever need to sort a table of numbers in shiva, I adapted this Rosetta Code entry  for shiva. The following code: local tTest = table.newInstance ( ) table.add ( tTest, 1) table.add ( tTest, 9) table.add ( tTest, 7) table.add ( tTest, 3) table.add ( tTest, 5) table.add ( tTest, 10) table.add ( tTest, 6) table.add ( tTest, -5) table.add ( tTest, -9) table.add ( tTest, -15) table.add ( tTest, 33) table.add ( tTest, 4) table.add ( tTest, 8) this.quicksort (tTest) for i = 0, table.getSize ( tTest) - 1 do log.message ( table.getAt ( tTest, i ) ) end Produces the output: -15 -9 -5 1 3 4 5 6 7 8 9 10 33 "},{"title":"Game Design Template for Galaxy Note","baseurl":"","url":"/2014/03/12/game-design-template-for-galaxy-note/","date":"2014-03-12 00:00:00 -0700","categories":["game theory"],"body":"As a game developer, I love the Note 3. It has taken the place of my small moleskin notebook. The included S Note app includes many templates, but you can also create your own. Here is the Gamestorming template I’ve made and an example of how I use it. Click to download the full size png. Example of use. "},{"title":"ShiVa Localization","baseurl":"","url":"/2014/04/11/shiva-localization/","date":"2014-04-11 00:00:00 -0700","categories":["interesting problems at work","marketplace  publishing","shiva 3d"],"body":"As we are nearing our first Steam release, we wanted to make a strong effort to localize our game in as many different languages as possible. This brought up an interesting problem involving workflow and tools that we’ve now solved and wanted to share. We wanted a solution that would allow: * Multiple translators to work on a shared document at the same time * Automated conversion from the spreadsheet to the XML files used by the game engine * Easy usage inside the game engine * Forward compatibility with ShiVa 2.0 and its ability to create in-engine add-ons in LUA The solution I came up with is very simple and has 3 components to it. Only one of these components is specific to my favored game engine, the remaining components are generic and can be modified to suite any project. Quick Links Google Spreadsheet Template Github Project (parser + ShiVa AI) ShiVa ste Overview The workflow of this solution looks like this: Make a spreadsheet with all of your translations Export the spreadsheet as CSV Parse the CSV document into multiple XML documents, one for each language and file Read XML documents into the game engine and use the desired strings The Spreadsheet Check out and make a copy of  the spreadsheet template  to get started on your own translations. The format of the spreadsheet is important. If it changes, the parser also needs to change. How the spreadsheet works is fairly intuitive and is probably easier done than said. This spreadsheet is automatically translating english to all other languages using the function: GoogleTranslate( text, sourceLanguage, destinationLanguage) Look at the formula bar for any translated cell and you can see that I’ve defaulted the source language to english while pulling the destination language from the language code field on the top row. We all know how good/bad auto-translation is, love it or hate it, you decide on the quality of your translations. Spreadsheet Instructions Each filename entry must be preceded by an empty cell! If not, the parser will fail. Notice how the example has an empty cell before each filename. I’m fighting with the decision to make this text red and blinking to catch your attention. To add a new file, create an entry for the filename in column A (make sure there is an empty cell above the filename!). In the example, there are two files defined ( MenuScreen  and KeyBind ). The bold font is cosmetic only, only the text is preserved when exported to CSV. To add an entry to a file, define the desired string identifier below the filename. Examples for MenuScreen are title, start, options credits . These identifiers must be unique for each file. For example, it’s ok if every file has a title  identifier, but there can’t be two title identifiers in the same file. To set a translation, move right from each identifier, filling in the translation for the current column as you go. The Parser The parser is a small LUA script that lives in the csv-xml-i18n repo . The parser takes a CSV file as input. To generate a CSV file from Google Docs, select File -> Download As -> Comma-separated values. If you don’t have LUA installed on your system, you’ll of course need that to run the code. You can then feed the parser your CSV file by passing the filename as the first argument: lua csvToXml.lua test.csv The result of running the script is that language files will be generated for each file and language defined. The naming convention is [filename]-[country code].xml. The Game Engine Download my ShiVa AI to easily implement localization in your game. The I18N AI has the following features to make ShiVa localization a breeze: A Simple Interface for Fetching Strings getString (Filename.identifier) Examples: log.message ( I18N.getString ( \"MenuScreen.title\" ) -- prints \"Awesome Game!\" log.message ( I18N.getString ( \"KeyBind.jump\" ) ) -- prints \"Jump\" log.message ( I18N.getString ( \"KeyBind.attack\" ) ) -- prints \"Attack\" Automatic XML Loading The AI automatically loads the XML file matching the OS language. If your OS is set to french and you request a string from MenuScreen , it will first try to load MenuScreen-fr.xml . If it can’t find that file, it will fall back to MenuScreen-en.xml . The fallback language is english. ShiVa Integration To use the AI and XML files in your project, follow these simple steps: Add the XML files to your ShiVa project’s resources/XML folder (be careful of editing these files in the shiva editor, it may destroy the UTF-8ness) Drag the XML files into the resources tab for your project Import the I18N AI and add it as the first UserAI Use the getString function! There is also a cleanup function that can be called <pre>I18N.cleanUp()</pre>  that empties out the hash table used to store the XML file contents. Use it when you want to clean up strings that won’t be used anymore. Conclusion This system is simple and it works. Follow the rules and it will work for you too. If you use this system, please let me know in the comments! "},{"title":"Using STK Packs in ShiVa","baseurl":"","url":"/2014/05/14/using-stk-packs-in-shiva/","date":"2014-05-14 00:00:00 -0700","categories":["shiva 3d"],"body":"Most ShiVa games have a single STK pack file. The reason why is because it’s easy to hit the export button and dump everything into one giant file when you publish. On many PC platforms like Desura and Steam, the client does a proper diff and then only downloads chunks of files that are different than what he already has.  In Rage Runner, our pack file is about 250 MB. In an update where the only thing we’ve changed is 2 kb of XML, the player still has to download the entire 250 MB pack! This is obviously wasteful. In this tutorial I am going to detail: * How to create STK files that contain your content * How to use STK files in your game * How to integrate this into your editor workflow * Caveats for publishing on individual platforms Get the Sample Project There is a sample project available below. All of the screenshots and code samples in this article come from this project. Download Sample Project Creating STK Content The first step is to create STK files that contain only your content. We are going to make an STK file that has all of our models and materials. To do so: Select content you want to export Right-Click and add to export Continue with step 1 for all assets Finally click export Placing Pack Files When packaging STK files with your game, they need to be placed in the root of your project folder. This is true for exported projects as well as when testing your game in the editor. Loading packs To load a pack file, we need to add it to the cache: cache.addFile ( \"MatsAndModels.stk\", \"file://\" .. application.getPackDirectory ( ) .. \"/MatsAndModels.stk\" ) If the pack is successfully loaded, you will see the log message. Packfile : MatsAndModels.stk loaded from cache If you don’t see that message, then there is either a typo in your code or the STK file isn’t in your root project folder. Referencing Resources To use a model or resource that is in an STK file, you add the prefix of the STK name but without the .stk extension. scene.createRuntimeObject ( hScene, box ) becomes scene.createRuntimeObject ( hScene, MatsAndModels/box ) This is the same for sound, materials, xml, etc. Simply add the STK prefix with a slash and your resource(s) will be found. NOTE. The models and resources should NOT be referenced in your game editor! This can be hard to get used to! Workflow Considerations One complication with this way of organizing assets is that whenever a resource is updated, the STK file must also be updated. This can make for a terrible workflow when you want to do rapid prototyping. A simple work around is to first check if the resource is referenced by the project, if it isn’t, check your list of known pack files. The idea is that you can prioritize referenced resources so that during rapid prototyping you can drag your resource(s) into the game editor so they’ll override the contents of the pack files. Here are two simple functions that do this. -------------------------------------------------------------------------------- function ExternalSTK.getModel ( sModel ) -------------------------------------------------------------------------------- -- -- Checks to see if a model is referenced in the project and tries to fall -- back to known pack files if it isn't. -- if application.isModelReferenced ( sModel ) then return sModel end -- -- If you have multiple pack files, you may need to add more checks after this. -- if application.isModelReferenced ( \"MatsAndModels/\" .. sModel ) then return \"MatsAndModels/\" .. sModel end -------------------------------------------------------------------------------- end -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- function ExternalSTK.getResource ( sResource, kType ) -------------------------------------------------------------------------------- -- -- Checks to see if a resource is referenced in the project and tries to fall -- back to known pack files if it isn't. -- if application.isResourceReferenced ( sResource, kType ) then return sResource end -- -- If you have multiple pack files, you may need to add more checks after this. -- if application.isResourceReferenced ( \"MatsAndModels/\" .. sResource, kType ) then return \"MatsAndModels/\" .. sResource end -------------------------------------------------------------------------------- end -------------------------------------------------------------------------------- Here’s how I’d use these functions. local hBox = scene.createRuntimeObject ( hScene, this.getModel ( \"box\" ) ) shape.setMeshMaterial ( hBox, this.getResource ( \"red\", application.kResourceTypeMaterial ) ) Packaging Considerations As of ShiVa 1.9.2, there are some considerations you should be aware of when packaging a game that has multiple STK files. Windows On Windows, when a game is launched, the STK to load is chosen based on alphabetical order. You will need to: Modify the name of the STK file so that it is first in alphabetical order ( game.stk  becomes  agame.stk). Mac On Mac, when a game is launched, the STK to load is chosen based on alphabetical order. You will need to: Modify the name of the STK file so that it is first in alphabetical order ( game.stk  becomes  agame.stk).  For a Mac app, the STK files are in YourGame.app/Contents/Resources/ Linux Linux also has some STK load order issues but the best solution here is to make a bash file that deliberately calls the STK file to load. When you export a linux build, you get an executable baring the same name as your STK file. For YourGame.stk you will get an executable called YourGame . So the bash file would be: #!/bin/bash ./YourGame YourGame.stk Conclusion The obvious application for STK files is that they can help deliver incremental updates to players in small manageable pieces. But there are other applications as well, like allowing players to mod your game by simply replacing your STK textures pack! Or customizing aspects of your game by loading different packs at runtime! If you’ve used STK files in unique ways that aren’t mentioned here, please share in the comments! "},{"title":"PhysX 3.3.1 Integration Notebook","baseurl":"","url":"/2014/06/29/physx-3-3-1-integration-notebook/","date":"2014-06-29 00:00:00 -0700","categories":["graphics","physx","shiva 3d"],"body":"Changelog July 29, 2014 Updated Rigid Body Section July 24, 2014 Added Rigid Body Physics July 9, 2014 Added details on collision spheres July 2, 2014 Added more cloth tearing info and cloth scale section. June 29, 2014 First entry Dear Zach, In case of time machine, read this before you start implementing the PhysX SDK. Sincerely, Zach # Getting Error Messages & Feedback There are 2 things that would have saved me a few weeks of work had I enabled them from the start. PhysX Visual Debugger PhysX Visual Debugger (PVD) is #1. It’s dead simple to get setup following the Nvidia example and it provides you invaluable feedback. Mostly its value is to make sure that  what you think is happening is actually what’s happening . Look at the physics objects in PVD alongside with your game, compare the cloth particle count and placement in your game engine along with the PVD, they should sync up. Run the Checked Build You should always run the checked build. If you’re working with PhysX in a plugin system, you’ll need to attach the debugger to your main process. Doing so will let you see the assert messages that pop up in the Visual Studio console. They will tell you when you do stupid things, like pass bogus (but not invalid) parameters to class instantiations. You might otherwise spend a lot of time figuring out why your physics objects are acting oddly. Cloth PxClothMeshDesc Weights Of course you understand that these weights determine whether the cloth particle is simulated or static. 0 means the particle is static, > 0 means that the particle is simulated. Usually we just use 0.0f and 1.0f to distinguish these two. A key part of setting up the cloth correctly is making lists of the vertices that need to be frozen in place vs simulated.  You’ll need to write a few custom tools that allow you to select these vertices and save them out as a list so that you can populate these weights at runtime. Don’t bother trying to split up your vertices and cloth weights into separate variables. I originally tried stuffing the weights into their own array and would continuously get the error: ..\\..\\PhysX\\src\\NpFactory.cpp (427) : invalid parameter : PxPhysics::createCloth: particle values must be finite and inverse weight must not be negative Despite the fact that my particle values were finite in length/value and all my inverse weights were non-zero. Eventually I gave up and followed the nvidia sample code where they stuff the vertices and weights into a single PxVec4 like a thanksgiving day turkey. Vertices go in (x,y,z) and weight goes in (w). Cloth Is Tearing You may get excited that you’ve found a new feature to allow tearing of cloth when half your flag just up and jumps off the flagpole. If your cloth is falling apart like this. It probably means that your mesh has duplicate vertices. In my case, the tessellated plane had duplicate vertices that needed to be welded. There is at least 1 other case that causes tearing. The problem is that not all duplicate vertices are geometric. In my case, some duplicate vertices in other meshes were part of the UV map. If any of these vertices are set to simulated, the cloth will have issues. Since I wasn’t able to separate out the UV from the mesh vertices, I wrote a quick check for duplicate vertices and made sure that they were forced to a weight of 0.0f. Cloth Scale The scale of the cloth can be tricky. When you are slurping down the vertice translations info for  PxClothMeshDesc , you need to multiply each vertice by the relevant scale factor of the object so that the space representation in PhysX is the same as in the engine. The problem comes in during the update phase when you’re matching the vertices in engine with the vertices as PhysX delivers them. I soon discovered that after updating the vertices in engine, the engine was applying the original scale factor to them! This results in a double scaling. It’s pretty obvious what’s going on here. The coordinates in the engine are local space and get a scale factor applied. One option is to apply a reverse scale factor in the vertice update function. This has a huge downside in that to do this you have to transform every individual vertice each frame. In my case, I’m using a single function call to update my mesh vertices which has speed advantages. So, the simple solution is to: Create PxClothMeshDesc with scaled vertices Set the object uniform scale to 1 This workaround gives PhysX the correctly scaled vertices and then relies on the PhysX representation for future updates. Since the uniform scale is then essentially zero’d out (at 1), no further scale factor is applied to mesh vertice updates! Cloth Collision Spheres Collision sphere positions are defined in the Local Space of the cloth object! When defining the collision spheres that make up each capsule for a character, it made more sense to define those in terms of local space offsets from each bone. Then once you know the offset from the character root to the cloth root, you can apply the necessary transforms to update the sphere positions each frame. A small sidenote here is that using a boxy model with spherical colliders does not look very accurate when simulated. Rigid Body Physics Filter Shader for Rigid Static Collision When using a custom filter shader, filter masks seem to be completely ignored on rigid static objects. I’ve asked why . Here is my current filter shader that handles rigid static collisions correctly. PxFilterFlags MyFilterShader(PxFilterObjectAttributes attributes0, PxFilterData filterData0, PxFilterObjectAttributes attributes1, PxFilterData filterData1, PxPairFlags& pairFlags, const void* constantBlock, PxU32 constantBlockSize) { // let triggers through if(PxFilterObjectIsTrigger(attributes0) || PxFilterObjectIsTrigger(attributes1)) { pairFlags = PxPairFlag::eTRIGGER_DEFAULT; return PxFilterFlag::eDEFAULT; } // generate contacts for all that were not filtered above pairFlags = PxPairFlag::eCONTACT_DEFAULT; // Check for static object collision if( PxGetFilterObjectType(attributes0) == PxFilterObjectType::eRIGID_STATIC || PxGetFilterObjectType(attributes1) == PxFilterObjectType::eRIGID_STATIC ) { pairFlags |= PxPairFlag::eNOTIFY_TOUCH_FOUND | PxPairFlag::eNOTIFY_CONTACT_POINTS ; return PxFilterFlags(); } // trigger the contact callback for pairs (A,B) where // the filtermask of A contains the ID of B and vice versa. if((filterData0.word0 & filterData1.word1) && (filterData1.word0 & filterData0.word1)) { pairFlags |= PxPairFlag::eNOTIFY_TOUCH_FOUND | PxPairFlag::eNOTIFY_CONTACT_POINTS ; return PxFilterFlags(); } // If the mask didn't match, then suppress the collision return PxFilterFlag::eSUPPRESS; } Reducing Simulation Jitter I found sub-stepping to greatly reduce jitter in my simulations involving kinematic actors. Sub-Stepping int substeps = 3; // Modify number of substeps based on your needs float subStepSize = mStepSize / substeps; for ( PxU32 subStepI = 0; subStepI substeps; subStepI++ ) { gScene-simulate(subStepSize); gScene-fetchResults(true); } Also, thanks to Gordon at Nvidia, I discovered that I was not setting the moment of inertia on my actors. This was the main factor causing jitter and is also very noticeable because it causes objects to have very high moments of inertia which means it takes far less torque to get them spinning. "},{"title":"Packaging Unreal Engine Editor","baseurl":"","url":"/2014/10/16/ue4-packaging/","date":"2014-10-16 00:00:00 -0700","categories":["ue4"],"body":"Packaging Unreal Engine After building the latest 4.5 editor, I needed to package it up and move it to another machine. Figuring out which folders were required took me awhile, so here is to saving time in the future. Once the build has completed, pull the following folders out of the Engine folder to arrive at the folder structure below: /Root Samples (Optional) Templates (Optional) Engine Binaries DotNET ThirdParty Win64 Config Content DerivedDataCache Documentation (Optional) Extras (Optional) Intermediate Plugins Programs Saved Shaders I should mention that I ommitted the Android and IOS folders from the Binaries folder. This would be relevant if you are doing mobile. Also, I was building for 64-bit only, so you may need the Win32 folder or other build binaries for your OS. "},{"title":"1,000,000 Stupid Questions for UE4 Developers","baseurl":"","url":"/2014/10/17/UE4-Crash-Course/","date":"2014-10-17 00:00:00 -0700","categories":["ue4"],"body":"My dumpster of random questions and answers for UE4. Let’s say you have a blueprint and you want to get access to the particle system and the audio file sub-components. What do you do? Here’s what I’ve been doing. In the .h file of my actor: UParticleSystemComponent* JetpackParticle; UAudioComponent* JetpackSound; In the BeginPlay callback: // Get all the particles in this actor, find the named instances. TArray<UParticleSystemComponent*> particles; GetComponents(particles); for (UParticleSystemComponent* particle : particles) { if (particle->GetName().Equals(\"Jetpack\")) { JetpackParticle = particle; JetpackParticle->SetHiddenInGame(true); break; } } // Get all the sounds in this actor, find the named instances TArray<UAudioComponent*> sounds; GetComponents(sounds); for (UAudioComponent* sound : sounds) { if (sound->GetName().Equals(\"JetpackSound\")) { JetpackSound = sound; break; } } Finally at the end of BeginPlay, do a sanity check to make sure we don’t have dangling pointers: check(JetpackParticle); check(JetpackSound); Logging to the Screen GEngine->AddOnScreenDebugMessage(-1, -1, FColor::Red, FString::SanitizeFloat(Val)); GEngine->AddOnScreenDebugMessage(-1, -1, FColor::Red, TEXT(\"YO\")); UPROPERTY “Missing variable type” If you get the error Missing variable type on your UPROPERTY line, it’s probably because you’ve put a semicolon at the end of the line. WRONG UPROPERTY(EditAnywhere, BlueprintReadOnly, Category = \"RobotActor\"); RIGHT UPROPERTY(EditAnywhere, BlueprintReadOnly, Category = \"RobotActor\") How to get a reference to an actor in the level blueprint? https://docs.unrealengine.com/latest/INT/Engine/Blueprints/UserGuide/Types/LevelBlueprint/index.html#referencingactors You can store a blueprint in your class as: UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"Tether\") UBlueprint* RopeSegment; You can spawn an instance of this blueprint: AActor* segment = GetWorld()->SpawnActor(RopeSegment->GeneratedClass, &actorLocation, &actorRotation, spawnParams); How to get delta time? GetWorld()->GetDeltaSeconds(); Math functions Found in FMath namespace How to draw hud? Make a blueprint that inherits from HUD. Use the event receive draw hud in the blueprint Make sure your GameMode is set to use that HUD blueprint Don’t try to draw hud from player controller How to get the current camera? From inside HUD: APlayerController* pc = GetOwningPlayerController(); AActor *vt = pc->GetViewTarget(); ACameraActor* camera = Cast<ACameraActor>(vt); if (camera) { //do stuff } How to get the player camera manager? AMyCustomCameraType* camera = Cast<AMyCustomCameraType>(UGameplayStatics::GetPlayerCameraManager(GetWorld(), 0)); if (camera) { //... } What is ACharacter ? Why are there camera controls in there? Characters are Pawns that have a mesh, collision, and built-in movement logic. They are responsible for all physical interaction between the player or AI and the world, and also implement basic networking and input models. They are designed for a vertically-oriented player representation that can walk, jump, fly, and swim through the world using CharacterMovementComponent. There are camera controls in there because the example you’re looking at (Top Down Camera) has added a camera and a camera boom to the character: What is APlayerController? PlayerControllers are used by human players to control Pawns. ControlRotation (accessed via GetControlRotation()), determines the aiming orientation of the controlled Pawn. In networked games, PlayerControllers exist on the server for every player-controlled pawn, and also on the controlling client’s machine. They do NOT exist on a client’s machine for pawns controlled by remote players elsewhere on the network. What does USpringArmComponent do? This component tried to maintain its children at a fixed distance from the parent, but will retract the children if there is a collision, and spring back when there is no collision. In the examples, the Camera is attached to the Spring Arm so that the heirarchy is: Parent: Camera Boom Child: Camera How should I store components in my class? Many of the example use TSubobjectPtr to do this. The caveat with this is that TSubobjectPtr can only ever be set in your constructor using the post construct initialize properties pointer In the .h: UPROPERTY() TSubobjectPtr<USphereComponent> CollisionComp; In the .cpp constructor: CollisionComp = PCIP.CreateDefaultSubobject<USphereComponent>(this, TEXT(\"SphereComp\")); What is a AGameMode ? The AGameMode class defines the game being played, and enforces the game rules. Some of the default functionality in AGameMode includes: Any new functions or variables that set game rules should be added in a subclass of the AGameMode class. Anything from what inventory items a player starts with or how many lives are available to time limits and the score needed to end the game belongs to GameMode. A subclass of the AGameMode class may be created for each gametype the game should include. A game may have any number of gametypes, and thus subclasses of the AGameMode class; however, only one gametype may be in use at any given time. A GameMode Actor is instantiated each time a level is initialized for play via the UGameEngine::LoadMap() function. The gametype this Actor defines will be used for the duration of the level. The Game Mode also sets the default pawn class and the player controller class. How do I define an Enum in C++? From the wiki . Create a namespace with the enum in it, usually above your class so that others can see it: UENUM(BlueprintType) namespace EGameState { enum Type { VE_TitleScreen UMETA(DisplayName = \"TitleScreen\"), VE_CharacterCreation UMETA(DisplayName = \"CharacterCreation\"), VE_Overworld UMETA(DisplayName = \"Overworld\"), VE_Battle UMETA(DisplayName = \"Battle\") }; } UCLASS(minimalapi) class ARPGSimulatorGameMode : public AGameMode { GENERATED_UCLASS_BODY() UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = GameState) TEnumAsByte<EGameState::Type> GameStateEnum; }; Quick Reference for U* Properties: UPROPERTY(EditAnywhere, BlueprintReadWrite, Category = \"Tether\") UPROPERTY(VisibleAnywhere, BlueprintReadOnly, Category = \"Tether\") Functions: UFUNCTION(BlueprintCallable, Category = \"Tether\") UFUNCTION(BlueprintNativeEvent, Category = \"Tether\") Who spawns the default pawn and takes control of it? The GameMode does, the interesting stuff is in GameMode.cpp in the function StartNewPlayer() RestartPlayer() is where the magic happens. This code: Tries to find a good player start location Tries to spawn the player using the configured DefaultPawn So if you’re trying to prevent the spawn of a DefaultPawn, once simple way is to simply not set the DefaultPawnClass variable in your custom Game Mode. Is it ok to use flat Classes? Yes, but only if you don’t plan on revealing their variables/functions to blueprints. All of the UPROPERTY and UFUNCTION stuff requires some special stuff in the header. So it’s easier to just create your classes as a sub-object of UObject. This means that: UCharacterContainer* me = new UCharacterContainer(); Becomes UCharacterContainer* me = NewObject<UCharacterContainer>(); Timers From the docs . How to override hit events in c++? Following this example . UCLASS() class ROB1E_API AProjectile : public AActor { GENERATED_UCLASS_BODY() UFUNCTION(BlueprintCallable, Category = \"Projectile\") void LaunchProjectile(FRotator direction, float impulsePower); UPROPERTY(VisibleDefaultsOnly, Category = Projectile) TSubobjectPtr<USphereComponent> CollisionComp; UFUNCTION() void OnHit(AActor* OtherActor, UPrimitiveComponent* OtherComp, FVector NormalImpulse, const FHitResult& Hit); }; AProjectile::AProjectile(const class FPostConstructInitializeProperties& PCIP) : Super(PCIP) { CollisionComp = PCIP.CreateDefaultSubobject<USphereComponent>(this, TEXT(\"SphereComp\")); CollisionComp->InitSphereRadius(0.5f); CollisionComp->BodyInstance.SetCollisionProfileName(\"Projectile\"); CollisionComp->OnComponentHit.AddDynamic(this, &AProjectile::OnHit); RootComponent = CollisionComp; } Beware of Gamepad Axis Events They are called every frame even if you have no gamepad connected. If you’re sharing variables with other input methods, be sure to guard them: void ARob1ePawn::AimGamepadX(float Val) { if (UseGamepad) { LastAimX = Val; } } PhysX Rope Notes My rope is not behaving as expected. If I wrap the rope around an asteroid and try to pull, the simulation goes crazy with the asteroid translation jumping all over the place like vampire bill trying to rescue sookie from a dozen attackers. I noticed that the Unreal implementation uses PX6DOF joints for all joint types in Unreal. In my own physx implementation in another engine, I created this same simulation but using PXSphericalJoints instead and don’t have these types of simulation jitters. I’ve tried the usual stuff. Enabling sub-stepping (16 at 0.0013). Increasing position and velocity iterations for my rope segments, asteroid and actor (15,15). Rope Position Iteration count >= 13 seem to make a big difference. By disabling rope physics collisions and enabling substepping (5 at 0.00666) with Rope iterations at 13,4 the simulation looks good. Reducing projection to 0.2 seems to make the simulation less likely to explode. How to start physx visual debugger? Run pvd connect/disconnect in the console Fixed Axis Mode To update fixed axis mode, you need to call the initializer: CollisionComp->BodyInstance.LockedAxisMode = ELockedAxis::Y; CollisionComp->BodyInstance.CreateDOFLock(); There is a patch coming in 4.6 possibly that will change this to: CollisionComp->BodyInstance.SetLockedAxis(ELockedAxis::Y); It’s also interesting to note that the DOF lock is nothing more than a physics constraint, not some special physics engine setting or state. "},{"title":"Geeking Out with Philips Hue","baseurl":"","url":"/2014/11/02/geeking-out-with-philips-hue/","date":"2014-11-02 00:00:00 -0700","categories":["geek"],"body":"I’ve been wanting to jump into smart lighting for a few years now. I finally decided that the Philips Hue system had enough of my desired features to warrant a starter kit. What follows is how I’ve been using the kit in various locations around my house. The Theater I use an OUYA to power my XBMC experience in my theater room. My primary desire in this room is lighting that automatically turns on and off based on whether content is playing. So if I pause a movie and need to get up, the lights go on, if I unpause then the lights go off. You can see a video of this below. This solution is working at about 95%, there are still some edge case bugs to fix where sometimes I lose connection with XBMC. I’m leveraging a few open source projects to accomplish this: phue is a python library for Philips Hue node-xbmc is an XBMC nodejs controller How it works All of this runs on my server which happens to be a linux box, but anything with python and node can do the job. Turning Lights On/Off The core of the system is a set of simple python scripts for turning lights on and off. I leveraged the phue library to make my own helper library that turns most of my on/off scripts into about 3 lines of code. Here is an example from my helper library: #!/usr/bin/python from phue import Bridge b = Bridge('127.0.0.1') lights = b.get_light_objects('name') office = lights['Lamp'] theater = lights['Theater'] bedroom = lights['Bedroom'] def On(lamp, hue, brightness): lamp.on = True lamp.effect = 'none' lamp.transitiontime = 0 lamp.hue = hue lamp.brightness = brightness lamp.saturation = 254 Now to turn a light on, I can write a simple script: #!/usr/bin/python import zachhue zachhue.On(zachhue.theater, 46920, 200) Detecting XBMC Events The 2nd piece to this is responding to XBMC pause/play/stop events. This is where the node-xbmc projects comes in. The node-xbmc project is a nodejs library that allows you to connect to your XBMC instance like a remote control. You can then register for events, for instance here is how I register for the pause and play events in javascript: xbmcApi.on('notification:pause', function(){ console.log(Date.now() + ': onPause'); mainLightsOn(); }); xbmcApi.on('notification:play', function(){ console.log(Date.now() + ': onPlay'); allLightsOff(); }); mainLightsOn() is just a function which uses PythonShell to run one of my simple on/off scripts: function mainLightsOn(){ PythonShell.run('lightson.py', function (err) { console.log(Date.now() + ': LIGHTS ON'); }); }; Future Enhancements What I’d like in the end is to replace the rope light with accent lighting that shines on our movie posters. We also have rope lighting around the ceiling but are still waiting on our smart outlet to tie it into the system. The hardest part of this project is remembering whether I’m writing python or javascript at any given time ;) The Office I was really impressed with the Philips Amibilight and some of the open source knockoffs. So I thought I’d try to copy this with what I had available. The first step was to throw a lamp with one of the Hue bulbs behind my monitors. As you can see I have a corner desk, so it gives me some nice close planes to cast light onto. Here is a shot that has my python script running, it is matching the color of the bulb to the average color of the content on my right monitor. Here is a video showing it in action. Just in case anyone asks, some of the geek things in the shot are: Laser cut Zelda Box by BurntPixels Link pixel art by 8-Bit Babe How it Works The workflow of this solution is simple. Capture screenshot Find average color in screenshot Set Hue bulb to color found in #2 Capturing the Image Since this is running on windows, I’m using the ImageGrab function from the PIL library. There are other cross-platform python routines for grabbing screenshots. After capturing the image, I resize it to a width of 128 while matching the height to the aspect ratio of my monitor. This is to make the stats calculations have an easier time. To be honest, 128 was a totally arbitrary number that I picked only because 2^7 is always a good starting point. from PIL import ImageGrab from PIL import ImageStat width = 1920 height = 1080 # Grab the image im = ImageGrab.grab() # Resize to width of 128 im = im.resize((128, int(128 / (width/height)))) Analyzing the Image Now to find the average color of all the pixels captured in the screenshot. At first I was using ImageStat from PIL to do something like this: stats = ImageStat.Stat(im) # now stats.mean[0], stats.mean[1], stats.mean[2] are the r,g,b values I noticed that the resulting average color would often be a color that wasn’t even visible on the screen. I found an old post from the python mailing list and found that using the quantize method would be a good alternative since it would only return actual colors that exist in the image. So this is what I’m using now: qstats = im.quantize(1).convert(\"RGB\").getpixel((0, 0)) # now qstats[0], qstats[1], qstats[2] are the average r,g,b values Color Space Conversion Now that we have an average color in RGB, we need to convert from RGB to HSL. Here is the code snippet I’m using to do that. At first I tried using the colorsys library to do a conversion straight from RGB to HSB, but the hues didn’t seem to match the Hue system very well. So I later found a stackoverflow post that linked over to the philips source with a good comment on how things work. I converted the code found there into these 2 python functions that allow converting an rgb value to the CIE 1931 color space that the Hue uses. Because the phue library has functions for setting CIE 1931 colors, we’re done with this code: xy = zachhue.RGBtoXY(qstats[0], qstats[1], qstats[2]) zachhue.OnXY(zachhue.office, xy, 255, 0.7) Where OnXY is: def OnXY(lamp, xy, brightness, time): lamp.on = True lamp.effect = 'none' lamp.transitiontime = time lamp.xy = xy lamp.brightness = brightness The one thing I’m still playing with is the brightness value. For now you can see above that I’m hardcoding it to 255. But I’ve also tried grabbing the actual brightness by converting the rgb -> hsv. The main issue with this is that during the day, the light isn’t bright enough for me, hence the hardcoding of 255. Future Improvements Unfortunately, for my standard workflow, the average color of my screen is often pretty close to white :( Thanks every app in the world :/ The most obvious improvement is to replace the single bulb with 3 or 4 LED strips. Then you could capture select portions of the screen to drive the individual strips. Burning CPU to analyze images continuously is not a great solution. It would be better for game devs to implement the Hue api directly into their game. I started hacking around with this in Rage Runner and it’s pretty cool. Honestly the only thing scaring me away from a full integration is the thought of writing the necessary UI and logic to make a solid experience pairing the game to the Hue bridge :( The Bedroom I also have a Hue light in my bedroom. The only smarts on this one is a daily alarm that turns on the light at a certain time. My XBMC script also turns the bedroom light on between the hours of 9PM and 2AM if my XBMC box has just been turned off. "},{"title":"UE4 Platformer Game Analysis","baseurl":"","url":"/2014/11/10/platformer-game-analysis/","date":"2014-11-10 00:00:00 -0800","categories":["ue4"],"body":"My journal from the disection of the platformer game sample. PlatformerGameMode.h Contains a custom state enum for tracking the states of the game. Also exposes this through a getter but there’s no setter, the state gets modified based on the flow of the game. Has private variables related to scoring Manages the time remaining in the game as well as adding/subtracting from that time based on picking up things in the world Has functions that manage saving best checkpoint times Pauses/Unpauses the game Starts/restarts the game Positions the player at the start location PlatformerBlueprintLibrary This code is basically a huge set of wrappers to expose functions that live in PlatformerGameMode to blueprints. Not all of the exposed functions are a 1:1 mapping. For instance the FinishRace function, notice that there isn’t a 1:1 function to call, instead it finishes the round and then asks if the round was won: bool UPlatformerBlueprintLibrary::FinishRace(class UObject* WorldContextObject) { bool bHasWon = false; APlatformerGameMode* MyGame = GetGameFromContextObject(WorldContextObject); if (MyGame) { MyGame->FinishRound(); bHasWon = MyGame->IsRoundWon(); } return bHasWon; } I don’t really understand the motivation behind this organization. If I were writing something from scratch, I would have the instinct to expose the Game Mode functions to blueprint and have my blueprints calling the same functions 1:1. On the other hand, not every function in here is a wrapper for PlatformerGameMode functions. There is also stuff that calls through to the HUD, sorts scores and formats time strings. I guess that in a way, it is quite nice to have all of your available blueprints in one place rather than making your designer get a handle to your Game Mode to call Blueprint A or a handle to your HUD to call Blueprint B. Also, you can then treat the blueprints like a traditional getter/setter where you do more careful type checking to make sure inputs and outputs are dealt with. PlatformerGameUserSettings This class handles applying and modifying settings in the game: Sound volume Fullscreen mode Of interest is the UPROPERTY(config) for the sound volume variable. More docs . PlatformerPlayerMovementComponent The CharacterMovementComponent allows avatars not using rigid body physics to move by walking, running, jumping, flying, falling, and swimming. It is specific to Characters, and cannot be implemented by any other class. Properties that can be set in the CharacterMovementComponent include values for falling and walking friction, speeds for travel through air and water and across land, buoyancy, gravity scale, and the physics forces the Character can exert on Physics objects. The CharacterMovementComponent also includes root motion parameters that come from the animation and are already transformed in world space, ready for use by physics. The default CharacterMovementComponent is overriden in the PlatformerCharacter constructor. The main new things that I see this class doing are: Implement sliding - this includes functions to query the sliding state Expose a bunch of variables to tune various aspects of the movement mechanics Overriding StartFalling PhysWalking and ScaleInputAcceleration PlatformerPlayerCameraManager A subclass of APlayerCameraManager with an override for: UpdateViewTargetInternal And new functionality for: Setting a fixed camera Z offset Settings/Getting camera zoom PlatformerPlayerController Subclass of APlayerController : The PlayerController implements functionality for taking the input data from the player and translating that into actions such as movement, using items, firing weapons, etc. These actions are generally passed on to other components of the system; most notably, the Pawn and Camera. Provides overrides for: PostInitializeComponents Creates the in-game menu SetupInputComponent Binds the InGameMenu action to the public function OnToggleInGameMenu() Special functions: Enables click and touch events TryStartingGame() exposed to blueprints. Toggle in game menu Handle to an in-game menu In the constructor is where the custom Player Camera Manager is set: PlayerCameraManagerClass = APlatformerPlayerCameraManager::StaticClass(); It turns out that UE4 has their own implementation of Smart Pointers, TSharedPtr, TSharedRef, TWeakPtr . You can see one used for the menu instantiation: PlatformerIngameMenu = MakeShareable(new FPlatformerIngameMenu()); Input Handling The player controller only handles the ESC button to toggle the menu. The other input bindings are handled directly in the pawn. "},{"title":"Setting up Perforce on Azure for UE4","baseurl":"","url":"/2014/11/14/azure/perforce/","date":"2014-11-14 00:00:00 -0800","categories":["perforce","azure","ue4"],"body":"Today is one of those fun days where I get to wear a few different hats in my independent game development career. We’re bringing some interns on and we need a better way of collaborating on projects together. It’s actually amazing that we’ve hobbled along using duct-tape (git) and bailing wire (dropbox) for so long. This is a little embarrassing, but I’m going to outline what we’ve been using for the last year. The Poor Man’s Free Dropbox Git Server Start with a shared dropbox folder for a project. On one end, initialize a git repo Use Dropbox selective sync to ignore the .git folder as well as intermediate build folders Note that to do this, you usually have to copy your .git folder out, disable selective sync (which deletes the folder) and then copy the .git folder back in. The advantages of this method are: With a local git server, the method costs nothing You don’t have to open your source repo to the world You have an extra level of redundancy Changes are instantaneous The downsides are: Changes are instantaneous 1 person ends up being in charge of all of the revisioning Not all applications enjoy files being ripped out from under them and this can create occasional sync issues Does not scale to 3 people Time to Grow Up I’m a serious person and now it’s time to get serious about team collaboration. Today I’m setting up a linux server on Azure that will be used to host a Perforce repository for our team that’s using Unreal Engine 4. Under the Bizspark program, we get $150.00 of free server credit each month. So it seems silly to pay for hosting without first exhausting this resource. Azure is pretty darn cool. If you like geek panels, there are plenty to be had: Azure Choices There are a lot of choices when setting up a virtual machine. Here is a record of some of the choices I made. My overall goal was to get the minimum viable server configuration so that all of my free credits would go towards bandwidth and storage. My biggest nightmare is for production to stop because we’ve exhausted our free credits :o As an overplanner, I hope that I have greatly over-estimated our usage (fingers crossed). Server Tier For server tier, I only need basic, not standard. The higher tier includes load balancing and scaling, not something we need for a code repository shared among 4 people. We’re all in the same hemisphere anyways, but for that matter if there was a team member in china, I’d just see if they could tough out the latency ;) Virtual Machine Size Perforce does better with more memory, but then again, what doesn’t? How much is enough? Their kb article has the equation: Estimated Memory = Number of files * 1.5 KB Ok, let’s estimate this. Content Estimation There are a lot of files in a game. You’ve got static meshes, textures, materials, blueprints, skeletons, animations, sound clips and more. All of these are separate files. If I estimate that our game will contain: 1000 unique static meshes 1/4th of those will contain a skeleton Each has ~ 3 texture maps Each has ~ 2 materials Each has ~ 3 animations Each has ~ 2 blueprints 1000 * (3 + 2 + 3 + 2) + 1000 * 0.25 = 10,250 files 10250 * 1.5 KB = 15 MB For the Unreal Engine binary distribution alone we have 13,612 files: 13612 * 1.5 KB = 20 MB So all said, 35 MB is all we need to satisfy perforce? Pffff. This is far more modest than I thought. The lowest tier on azure has 768 MB of memory which is about 22X more than what we need. But that’s actually the amount of memory allocated to the entire VM, so after the OS is up and running… let’s see, Ubuntu Server requires 192 MB these days: (768-192) / 35 = 16X I can live with 16X headroom, this allows for slop in my estimate and also unforseen memory usage on the server side. So I’m starting out on the lowest server tier and will monitor our memory usage. If we start getting paged out AND we actually notice it AND it’s hindering our productivity then we’ll look at bumping up to the next memory tier. But as the original goal is to go spartan, we’ll stick with the lowest tier. Storage Replication Geo-Redundant is $5 / 100 GB Locally Redundant is $4 / 100 GB An extra dollar per 100 GB gives me peace of mind if my local data center is destroyed. For 500 GB of content, I’ll pay an extra $5 dollars per month. If it hits the fan, I know where to find a few extra dollars, for now I’m leaving this at Geo-Redundant. Disk Size On Azure, you are billed based on used storage and the threshold is 1 TB. So any unused disk space that is laying around isn’t billed. 500 GB sounds plenty for our repo. Our largest project so far has been around 5 GB with all the assets and code. Disk Host Caching Going to leave this at None, but honestly didn’t spend a lot of time researching it. Misc Perforce Config Notes I see that as part of the install perforce package installs, it created a perforce group/user and that it also created a default folder to place repositories: Creating home directory `/opt/perforce/servers' ... usermod: no changes Thank you for choosing Perforce. To create and configure a Perforce Server, on this host, run: sudo /opt/perforce/sbin/configure-perforce-server.sh server-name Since I just setup a new disk specifically for perforce, why not just mount this directly to /opt/perforce/servers and be done. After fdisk, mke2fs, and editing fstab we’re ready to rock. The first time I mount, I notice that it changes permissions on the servers folder to root:root 755, so we need to chown back to perforce:perforce and chmod back to 700. A quick reboot test to see if things worked: # df Filesystem 1K-blocks Used Available Use% Mounted on /dev/sda1 30202916 1094896 27841988 4% / none 4 0 4 0% /sys/fs/cgroup udev 338020 8 338012 1% /dev tmpfs 68644 344 68300 1% /run none 5120 0 5120 0% /run/lock none 343220 0 343220 0% /run/shm none 102400 0 102400 0% /run/user /dev/sdb1 20509308 45000 19399452 1% /mnt /dev/sdc1 515929528 71448 510598828 1% /opt/perforce/servers # ls -l /opt/perforce/ total 16 drwxr-xr-x 2 root root 4096 Nov 14 11:33 bin drwxr-xr-x 2 root root 4096 Nov 14 11:33 sbin drwx------ 3 perforce perforce 4096 Nov 14 11:42 servers drwxr-xr-x 3 root root 4096 Nov 14 11:33 usr Cha-Ching! Ports I’ve setup perforce to run using SSL. I’ve also required passwords for users, prevented automatic user creation, created some users and set their passwords. I’ve decided to start by grouping users into 2 groups: artists developers I then set depot permissions based on group. Future note, this is how you create or administer groups: p4 group groupname Depots are created using: p4 depot depotname Protection is configured using protect: p4 protect Unreal Engine 4 Organization & Workflows My end goal is to host two depots in perforce: A binary engine distribution built from latest stable git release Our current work in progress. Artist Workflow For a new artist, I want the workflow to be: Sync perforce depots Run UnrealVersionSelector to register engine directory and setup shell extensions Double click .uproject file and they’re off to the races Artists will then use the built-in perforce support to manage source controlling assets. When a new version of the engine is compiled, they’ll need to repeat the new artist workflow. My primary concern here is that artists will forget how to use P4V to sync the engine because they’ll do it so infrequently. Developer Workflow Developers will use git to manage the C++ side of the project and will push binary versions of the engine and project to perforce. I feel deceptive using the word developers plural because I’m the only developer. Here is how the repos overlap for developers i.e. me. Note that project name is Rob1E . The crossed out items are things that are ignored in both repos. You can see that git would ignore all perforce hosted folders in addition to the crossed out items and vice versa for perforce. To share the compiled pieces of a game project, I simply compile the development solution and then push the game dll/pdb file found in the Binaries/Win64 folder to perforce. Why use 2 repos? Why am I using 2 repos? Why not just go all-in on perforce? Or conversely all-in on git? The last question is easy. Git would be a difficult workflow for artists. I want to spend my day coding games, not solving merge conflicts. Why not go all in on perforce? My primary reasons are: I like the git workflow My git repo disaster recovery plan is tested and proven I don’t feel like the artists need access to the code, not even read access I don’t want to lose my commit history I’m sure some of these issues are surmountable and I’d love to be convinced of a better solution by someone. If I ever get to the point where I spend more time managing the system than I am using it, then it’s simple enough to dump my code into perforce. One potential downside that I foresee is that version tagging might get out of sync between the code and asset repos. But time will tell. That’s It! Time to take my IT hat off and put my gamedev hat back on :) "},{"title":"11 Random ShiVa Projects","baseurl":"","url":"/2014/11/15/shiva/poc/flush/","date":"2014-11-15 00:00:00 -0800","categories":["shiva"],"body":"I needed to free up some disk space so I started going through my project folder of about 85 projects. I’ll be the first to say that I make a lot of junk and most of it gets thrown away. The good stuff gets thrown into repos. In my cleaning frenzy, I stumbled upon 11 old shiva projects that sit somewhere between junk and repo status. So I figured, why not release them to the world for dissection, study and ridicule :) These are raw proofs of concept, so expect the code to be fast and loose! The 11 projects can be found in my POC repository , here is a brief introduction to each of these awkward unfinished things. hello-space-invaders There are 2 game projects in here, lander and space . lander lander does basically nothing and I’m not sure what I was thinking when I made it. 2 player space invaders? space space is a pretty standard start to space invaders. You can move left/right and shoot but the enemies have no AI. FortunousWheel At one point, the wife and I were doing a motivational chore thing and at the end of the week we would get a prize if we did all of our chores. I made this project so that we could have a Wheel of Fortune style prize picker. So we’d spin the wheel and pull out a numbered piece of paper from a jar telling us what we’d won! I was really proud of this project because the code to generate the wheel was kind of ridiculous. I do remember the biggest drawback was that the wheel took a TON of draw calls. But the wife had just gotten the HTC One and that thing just handled it so I shrugged and called it good. Platformer Physics A familiar face for a familiar problem. Jumping physics, early jump termination and stuff. shiva-pong An incomplete pong implementation. Paddle moves, ball moves but the physics need work. Spacebar launches ball, mouse moves paddle. random This project was made to try and visualize the distribution of gaussian random numbers vs random numbers. Try changing the initial state from Gaussian to Random to see for yourself. flocking This may have had visions of glory to be a flocking simulator but it ended up just being a steering implementation. Move the mouse around and the box follows it. simplex This is a shiva plugin and project. The plugin is a simplex noise generator and the project uses it to make random nebulas. The nebulas are rendered to a rendermap and in the makeRandomNebula case they are saved to D:\\nebula. There was a stretch of time where I was letting this run overnight and I’d come back to a bunch of nebulas in the morning. I even saved some of my favorite generation parameters to make the nebulas Cotton Candy and Purple Thunder :) We used this at one point to make backdrops for Rage Runner. letterRender Hmmm, I can’t really say what my motivation behind this was. I think I was inspired by the Wii news browser because they rendered each letter individually and had them animate onto the page. I wanted to try something similar. spatialization A simple test with dials for sound spatialization. This also contains code for what I call DebugHUD which is a quick way of getting dials on the screen to tweak AIs. HorseTest The original goal of this project was to see if I could take a free unity asset and import it into Shiva. The answer was yes. The next question was how difficult it was to manually modify the horse skeletal system to improve the horse turning animation. Things got progressively worse as I started playing with a day/night system and then gave up. The horse controller is pretty solid and I’m happy with that small piece of work. lua-performance I like for equations to make sense when you read them in code. In most cases, I prioritize readability over imaginary performance. Someone made the comment that dividing is really slow in LUA and I didn’t believe them. So I wrote this small test to try and benchmark multiplication and table pre-allocation. The numbers are so small in my test that it didn’t matter. I mean, technically we know that multiplication is faster than division on the CPU. But can we even perceive it and does it even matter for most of our code? I’d say no for 99% of our code. As a game designer, I wanted to answer this question for LUA because for me, division vs multiplication is a matter of readability. So here we go down the rabbit hole. The first step was to see what the LUA VM instructions for multiplication and division were in the version of LUA that shiva was using (5.0.2). I start with the following lua app: local a = 123452.245 / 2.0 local b = 123452.245 * 0.5 I then compile this and list the LUA machine code: error454@olympos:~$ luac -l test.lua main <test.lua:0> (3 instructions, 24 bytes at 0x144f910) 0 params, 2 stacks, 0 upvalues, 2 locals, 2 constants, 0 functions 1 [1] DIV 0 250 250 ; 2 2 2 [2] MUL 1 250 251 ; 2 0.5 3 [2] RETURN 0 1 0 You can see that both divide (DIV) and multiply (MUL) are a single LUA instruction. Let’s dive down to the next layer and pop open the LUA source. In the LUA virtual machine code (lvm.c) we find the implementations for the 4 primary arithmetic operations: static void Arith (lua_State *L, StkId ra, const TObject *rb, const TObject *rc, TMS op) { TObject tempb, tempc; const TObject *b, *c; if ((b = luaV_tonumber(rb, &tempb)) != NULL && (c = luaV_tonumber(rc, &tempc)) != NULL) { switch (op) { case TM_ADD: setnvalue(ra, nvalue(b) + nvalue(c)); break; case TM_SUB: setnvalue(ra, nvalue(b) - nvalue(c)); break; case TM_MUL: setnvalue(ra, nvalue(b) * nvalue(c)); break; case TM_DIV: setnvalue(ra, nvalue(b) / nvalue(c)); break; //...ommitted for brevity } We’ll work from the outside in. There is an important struct for TObject , it stores a LUA type (number, string, etc?) and the value of that type. typedef struct lua_TObject { int tt; // Type Value value; // Value } TObject; The value of TObject has some house-keeping items along with the actual numeric value of the LUA variable: typedef union { GCObject *gc; void *p; lua_Number n; // number value int b; } Value; Now we can see that setnvalue simply sets the type and value of a TObject #define setnvalue(obj,x) { TObject *i_o=(obj); //interpret object passed in as pointer to TObject i_o->tt=LUA_TNUMBER; //Set type to LUA_TNUMBER i_o->value.n=(x); //Set value to x (arg) } OK, so setnvalue is simply going to take the stack register passed to it and store a number. In our case either: nvalue(b) * nvalue(c) nvalue(b) / nvalue(c) That’s it? Better find out what nvalue does first to keep our noses clean. We find the macro to be this pile of junk: #define nvalue(o) check_exp(ttisnumber(o), (o)->value.n) // returns the number value from lua_TObject->value #define check_exp(c,e) (e) //returns e ?! #define ttisnumber(o) (ttype(o) == LUA_TNUMBER) //TRUE if lua_TObject->tt is LUA_TNUMBER #define ttype(o) ((o)->tt) //returns type from TObject->tt In chess, this would be notated with an exclamation point. Was this an implementation error? This macro is a complete waste of space, I can only guess that it is unfinished. Maybe a place-holder for eventually checking whether the argument is a number and maybe throwing an assert. We’ve reached the end. We can now say with certainty that multiply and divide operations in LUA are straight-up multiply and divide operations in C. So at least we can move the argument from LUA to C. "},{"title":"How to use Perforce in UE4","baseurl":"","url":"/2014/11/20/perforce/ue4/workflow","date":"2014-11-20 00:00:00 -0800","categories":["perforce","ue4"],"body":"Intro Perforce is a source control repository used to track and control changes to files. You should read the official perforce docs for UE4 first. Then come back here for more details. The benefits of perforce are: Tracking Helps us keep tabs on changes to assets. For instance if an asset breaks, we can look at the history of an asset. Here is the history of our player character: If needed, we can revert back to an older version to fix issues. We also have a good list of who modified something and what they modified. Control Changes Perforce makes sure that only a single person can make changes to an asset at a time. Imagine what would happen if two people were modifying the same asset simultaneously and then both saved. Whoever saved last would overwrite all the changes of the first person! Working in Perforce Perforce has essentially 5 actions that you can perform inside the Unreal Engine Editor. These actions are accessed by right clicking on an asset, folder or project root and looking in the Source Control menu: Check Out Locks the file and allows you to make changes to the file. Mark for Add Marks a new file to be added to the repository. You can only mark files that don’t already exist in the repo. Check In Submits all of your changes to the server and unlocks the file. You can check in: A single asset at a time All new/changed assets at once (accessed from File->Submit To Source Control ) Always look through the files that are about to be submitted and ask yourself the question Question : Did I really change this asset? If the answer is no then don’t submit that asset. Revert it instead. Notice below that I’ve only selected to submit the files that were deliberately changed. Revert Reset the asset to the state it was in before it was checked out. Also unlocks the file if it was previously checked out. Sync Pulls down the latest version of the file from the server. This gives you all of the latest changes from other collaborators. You can sync: A single asset An entire folder Perforce Workflows Workflow 1: Updating Code You will typically do this first thing in the morning or when your coder says you need to update code. You need to close down UE4, open the P4V client and Get Latest. Make sure you’ve selected the top level folder for the project first. Once syncing is complete, you can launch the editor again. Workflow 2: Adding New Files Workflow 3: Making Changes to Existing Files Keep in mind that once an asset is checked out, nobody else is allowed to make changes until you either check in or revert. So don’t leave important assets checked out over the weekend! Workflow 4: Syncing Changes Sometimes when syncing content, the Yellow Exclamation will not go away and it will appear that your content isn’t syncing. This is called the Sync of Death . Never check out a file that is in the SoD state (if you’ve already checked out the file, revert it). When you have files in the SoD state, you will need to restart Unreal Engine and sync again before working with those files. Conclusion These are the basics of working with perforce in UE4. At first the workflow may seem counterproductive, but you’ll get the hang of it and eventually it will be second nature. The areas where you need to exercise the most caution are the buggy Sync of Death and the accidental Check-In of files that you did not intend to edit. Keep in mind that it’s ok to make new test assets, maps etc if you need to do some testing. You can leave these test assets in your project, just remember to not mark them for add. Just be careful that the assets you are checking in don’t depend on any of your test assets! Keep those check in descriptions good and your entire team will be happy! "},{"title":"GameDev Barbie","baseurl":"","url":"/2014/11/22/gamedev/barbie","date":"2014-11-22 00:00:00 -0800","categories":["silly"],"body":"My own contribution to the Feminist Hacker Barbie Movement . "},{"title":"Nebulas, Stars, Random Numbers and UE4","baseurl":"","url":"/2014/12/06/nebulas/stars/random","date":"2014-12-06 00:00:00 -0800","categories":["nebula","star","gamedev","ue4"],"body":"For the last week I’ve been catching up on ways to render stars, nebulas and space in general. The learning curve is quite steep and I’ve absorbed enough to finally feel like the firehose has been turned off. Here is a reflection on what I’ve learned, some things I’ve tried and where I might look next when I revisit this. Stars The first thing I studied about stars was how photoshop people were making cool space scenes. All of these compositing tricks seem very similar: Make a bunch of dots using noise Mask out sections of the noise to make the scene more interesting (less is more) Adjust brightness/contrast so that stars have varying brightness levels I thought this would be pretty simple to do with a combination of material and blueprint, so here we start with the brute force solution. Step 1 - Spawn Stars I thought it would be nice for level designers to be able to drag a volume into the scene, set the number of stars they desire and see stars pop-up. So I made a blueprint that lets you do this. I’m using Bluetilities to do this, otherwise you’d have to wire in all your creation on the start of the level. You can even set a seed value so that you get the same star pattern every time. Below you can see that the components of the StarVolume are the volume itself and an instanced mesh that uses the EditorPlane for a static mesh. When you run the Respawn function, the blueprint spawns X stars at random points contained inside the box extent. The stars are spawned as instanced static meshes so that the end result is 1 draw call. Here you can see a volume with stars spawned in it. Step 2 - Material The material could probably use some work, here are the things that are working well. Meshes Face the Camera I had an issue with the instanced static meshes and getting their actual world position, resulting in some pretty odd stuff. This below is Math Hall example 2.21 from the Content Examples, the only difference is the red outline where I swapped out ActorPosition for ObjectPivotPoint . Stars Fade as they Approach Camera The second piece of this is fading the stars out as they approach the camera. Star Shape You’ll notice that for my star shape I’m having a hard time deciding between a DiamondGradient and a RadialGradientExponential. Here’s the two side by side, enlarged for example. Basically, use DiamondGradient to get a pointy looking star. The diamonds need to be about twice as large as the radial to get the same effect. Also, things seem a bit dim, so I’d recommend multiplying a bit to get a true white in order to negate the dimming effect of the gradient. Step 3 - Material Misc This is a Translucent material right now and I’ve found that it’s important to enable adaptive anti-aliasing. Without this turned on, you kind of lose the stars when the camera moves. Final Thoughts on Stars Things I’m happy with 3D stars that I can fly through w/free parallaxing and all that 1 draw call Easy for level designers to use Things I’m not happy with They look like crap compared to artistic space shots that I’ve seen :( No subtle twinkling Random distribution is too random-number generator-ish, I’d like to use perlin noise for a more natural star distribution Stars in the same volume can only be 1 color. I haven’t spent the time on this, but when you create an instanced static mesh, the creation call doesn’t pass back that instance. So I’m not sure how to update the dynamic material parameter per-instance… maybe you can’t, but that’s the first thing to look into. We can always overlay 2-3 layers for multiple colors. Nebulas A week ago I thought I’d naively go where game developers weren’t going by making some great looking volumetric nebulas. Lol! Here’s what I can tell you. The Big Boys This paper , written in 2000 helped lay a solid foundation for Emission Nebulas. I’d like to give these guys a standing ovation and an invitation to dinner. The problem with the paper as it applies to games is that they use volumetric rendering. Here is what I understand of their process: Create a surface model of the nebula Calculate a distance field for the surface model - this is basically voxelizing the surface model of the nebula so that they can render not just the surface but some distance (the thickness of the ionization layer of the nebula) under the surface Apply turbulence to the distance field Render top voxels fully transparent and vary the transparency with distance into the ionization layer As a future note, the formula for procedural turbulence that they reference is: function turbulence(p) t = 0 scale = 1 while ( scale > pixelsize ) t += abs(Noise(p/scale) * scale) scale /= 2 return t Before I fully understood how they were rendering these nebulas, I got prematurely excited because I knew that UE4 used distance fields for a couple features. You can even turn the visualization of distance fields on in the editor. The key to understanding here is surface vs volumes, we render surfaces, the big boys render volumes. For future use, here is my Nebula 101. Nebula 101 A cloud of ionized gas that emits light of various colors. 1 or more nearby stars do the ionizing. Most Common Gas Types 90% Hydrogen 10% (Helium, Oxygen, Nitrogen) Color The color of a nebula is based on the gas type that it is composed of. Light works the way it always does with visible light being emitted by electrons as they jump between different orbitals. Since emission nebulas are composed mostly of hydrogen, we can use the Balmer (not the Microsoft dude) series to calculate the visible spectrum of light. For Hydrogen, this is only 4 visible wavelengths - red, aqua, violet and… violet which are also known by their much cooler nicknames of H (for hydrogen) followed by a greek letter of the alphabet (alpha,beta,gamma,delta) going from right to left in the photo: These are wavelengths in nm: Red (H-Alpha): 656.3 Aqua (H-Beta): 486.1 Violet (H-Gamma) 434.1 Violet (H-Delta) 410.2 Converting these wavelengths to RGB using this code , we get the following 4 colors. I’ve set the colors up here with their layers screened to see all the possible interactions between them. We can see that hydrogen nebulas would be dominated by red at the lowest energy state and shades of blue and a bit of violet. We can easily find the visible spectra for the other most common gas types, Helium and Nitrogen are interesting because they both have an organgish color component. The bottom line is that if you stick with this color spectra, the nebulas will be believable. A rainbow nebula could be cool but it would be much cooler if it had a story behind it with gasses that made sense. Nebula Attempts in UE4 Middleware My first port of call was to check out the middleware TrueSky . From a conversation with them, it sounds like nebulas would be possible, but the functionality isn’t built in right now. Since I’m not making Nebula: The Game , I’m not interested in spending time hacking around with their source at this point. Maybe they’ll surprise me in the new year with an implementation. Building On The Cloud Example There’s a nice looking cloud particle in the Content Examples Effects map. I started with this, got rid of the motion, maxed out the lifetime and replaced the cloud texture with some perlin noise. The thing about perlin noise is that you can play with it forever and get lots of interesting stuff. It’s a slow process too because every time you change noise values you have to wait for shaders to recompile. At the points where the noise actually looks good in game, the node preview window doesn’t show you anything useful. In the end, my best result is still kind of meh, clouds in space. Here is a starting point for this method. The additive material. Here is what I call my Perlin base. It’s what I start with. Want more fluffy clouds? Decrease the scale, here’s 0.002 with a level scale of 1.5. Other Tests I tried switching the material to lit and creating light/dark areas through lighting. I like this workflow for artists and think that the ideal workflow would be the ability to drag out some sort of static meshes into the scene and then light them statically. Particles sound like a good fit but they give you very little artistic control due to the particle spawns being random. Going forward I’d like to move towards a more artist friendly workflow that doesn’t involve tuning perlin noise and particle emitter counts. That said, this was a fun diversion, back to making games! "},{"title":"Using Animation Blueprints with Matinee","baseurl":"","url":"/2015/01/30/matinee/animation/blueprint","date":"2015-01-30 00:00:00 -0800","categories":["ue4"],"body":"I recently did some matinee scenes in UE4 and came away feeling like I was using the tools wrong. That’s probably because I’m a programmer with no animation skills trying to use tools meant for artists with no programming skills :o Add on top of this a time constraint and you have a somewhat willy nilly solution that I’m about to describe. What follows is a summary of how I worked around the tool limitations to arrive at a solution that worked for my skillset. One of the end results can be seen below: What Matinee Wants Matinee seems to want a raw skeletal mesh in your level. It then lets you move this mesh and trigger animations. The intent seems to be that you’re either going to provide a big custom animation for your entire sequence, or your animation clips are done in such a way that they already blend between one another. Matinee does not blend ! I can’t emphasize this enough, you cannot blend between animation tracks in Matinee! This whole not blending thing may come as a huge surprise to many people. The challenge for me was that I didn’t have a bunch of custom animations. The assets I had to work with were: 1 Animation track (idle) 3 Aim poses for each arm (used for aim blendspace) 3 Look poses for the head (used for head look blendspace) A handful of custom poses (1 frame, no animation) Why Not use an Animation Blueprint? I thought to myself, hmm, what lets me trigger animations and also does animation blending, IK and stuff? Obviously the answer was animation blueprints! This excited me because I had already solved the puzzle of animation blueprints and I knew they did exactly what I wanted, I could move my characters head at a target and move his arms up and down to aim at stuff. Matinee Destroys Animation Instances Any skeletal mesh that has a matinee track will have their animation instance ripped out and thrown away when the matinee starts. This means that you cannot have a skeletal mesh that is both using an animation blueprint AND part of a matinee track. I wrote-up an answerhub issue on this although I think the limitation is probably by design. How to Use Animation BP in Matinee This setup may seem odd at first, so here is a high-level overview of how it works. There are 3 total components here: The Star The Protege The Animation Blueprint The Visible Animated Actor (The Star) This can be a blueprint or a skeletal mesh actor, it doesn’t matter. This is going to be the visible actor in your matinee sequence. Every time you want to play an animation, you’ll instead fire an event from matinee, that event will enable a state in your animation blueprint to enable the desired animation. To hook up the star, when the matinee starts you need to attach him to the protege via some code or blueprint like: AttachRootComponentToActor(MatineeActorToMatch, NAME_None, EAttachLocation::SnapToTarget, false); If you aren’t a coder you can skip the rest of this section and move on to the next heading. One thing I wanted is smooth entry into a matinee sequence. When a matinee begins, I don’t know where the player character is, they might be facing the wrong direction or slightly off target from where my matinee actor begins. So here is really kind of a full peak at how I’m trying to manage this. First I enter matinee mode with an optional time until the transition is locked: void ARob1ePawn::EnterMatineeMode(float TimeUntilTranslationLock, AActor* ActorToMatch) { DisableInput(UGameplayStatics::GetPlayerController(GetWorld(), 0)); bIsInMatineeMode = true; MatineeTimeUntilTranslationLock = MatineeTimeMax = TimeUntilTranslationLock; MatineeActorToMatch = ActorToMatch; } Now when my pawn ticks, I lerp the rotation and location over the specified time period and when the time expires, I attach to the matinee actor: void ARob1ePawn::Tick(float DeltaSeconds) { Super::Tick(DeltaSeconds); /*** * __ __ ______ ______ __ __ __ ______ ______ * /\\ \"-./ \\ /\\ __ \\ /\\__ _\\ /\\ \\ /\\ \"-.\\ \\ /\\ ___\\ /\\ ___\\ * \\ \\ \\-./\\ \\ \\ \\ __ \\ \\/_/\\ \\/ \\ \\ \\ \\ \\ \\-. \\ \\ \\ __\\ \\ \\ __\\ * \\ \\_\\ \\ \\_\\ \\ \\_\\ \\_\\ \\ \\_\\ \\ \\_\\ \\ \\_\\\\\"\\_\\ \\ \\_____\\ \\ \\_____\\ * \\/_/ \\/_/ \\/_/\\/_/ \\/_/ \\/_/ \\/_/ \\/_/ \\/_____/ \\/_____/ * */ if (bIsInMatineeMode) { if (MatineeTimeUntilTranslationLock >= 0.f && MatineeActorToMatch) { MatineeTimeUntilTranslationLock -= DeltaSeconds; FVector MatchLocation = MatineeActorToMatch->GetActorLocation(); FRotator MatchRotation = MatineeActorToMatch->GetActorRotation(); // Attach actor if (MatineeTimeUntilTranslationLock <= 0) { SetActorLocationAndRotation(MatchLocation, MatchRotation); AttachRootComponentToActor(MatineeActorToMatch, NAME_None, EAttachLocation::SnapToTarget, false); } else { // Match Translations over time period FVector Location = GetActorLocation(); FRotator Rotation = GetActorRotation(); Location = FMath::Lerp(Location, MatchLocation, 1 - MatineeTimeUntilTranslationLock / MatineeTimeMax); Rotation = FMath::Lerp(Rotation, MatchRotation, 1 - MatineeTimeUntilTranslationLock / MatineeTimeMax); SetActorLocationAndRotation(Location, Rotation); } } return; } ... Of course when you’re all done you should detach from the matinee actor and re-enable input! The Matinee Skeletal Mesh Actor (The Protege) This skeletal mesh will be the actor that you move around in matinee. The actor will be set to hidden in the game because he’s really just around as a placeholder. Your visible actor will be attached to this actor so that he follows the location and rotation that you are animating. The problem with triggering events that play animations on your animation blueprint is that now matinee scrubbing doesn’t show the animations in the scene. For this reason I recommend triggering the animations on this matinee track just for your own reference while scrubbing the matinee sequence. Custom Animation Blueprint You will probably want to make a custom state for your matinee work as I’ve done here. My main goal was to be able to blend my idle animation with basically everything and to trigger specific states with very specific in/out transition times. The anim bp itself is mostly a bunch of Blend Poses by bool and a few additive nodes. Putting it all Together Your matinee sequences will start to look like this. Notice how I fire the RobGunCheck event and the Rob_gun_tap animation at the same time. I didn’t have to fire that animation, but it sure helps because now I can scrub matinee and also have a perfect timeline for when the animation will end. Continuing forward, you can see I fire RobLookLeft and Lookatdog at the same time etc. At the end of the day, your matinee controller is going to look like this. Where each of these events in a nutshell only needs to set a bool value in your animation blueprint like so. Summary You can combine matinee and animation blueprints using the above workflow. Since events don’t fire when scrubbing matinee, you will need to also add the skeleton animation track for reference even though it’s the animation blueprint that fires the animation in the final sequence. Ideally in the future, we’d be able to blend between animation tracks in matinee and also get more granular skeletal control, for instance an IK rig. I’ll admit, this workflow may seem odd, but it works for my skills. If you have a similar workflow that you think might work better, I’d love to hear it! "},{"title":"Character Movement Replication in UE4","baseurl":"","url":"/2015/03/20/ue4/movement/replication","date":"2015-03-20 00:00:00 -0700","categories":["ue4"],"body":" There’s a piece of documentation on the official UE4 site that briefly discusses different methods for replicating Character Movement over the network. It explains 4 different methods for replicating new movement abilities and goes on to detail why 3 of the methods basically just kind of suck. The short explanation is that the preferred method not only keeps the client synced but can also replay moves that would pile up during a lag spike. This way, any movement based abilities triggered by the client during the spike won’t get lost. What follows is a concrete example of Approach 4 that is mentioned in the document. This took me several days to nail down, most of the major structure was discovered by looking at the Unreal Tournament source. There you will find an implementation for sliding, dodging and sprinting. The downside of the UT source is how huge it is! I hope this serves as a good basis to build your movement based replication with! Resources Original Documentation My Sample Project When to use this Method? It should be clear when to use this method vs running authoritative methods on the Server. This method is only used for abilities that somehow affect character movement. By default, the engine handles Jumping and Crouching using this method. You wouldn’t shoot a gun or activate an airstrike using this method, instead you’d call those directly on the server. But if you’re adding a double jump, sprint or teleport (all abilities that directly affect the movement of the character) then this is the right method to use. The Sample Project I wanted to make my additions to the original Side Scroller sample crystal clear. To do so I’ve split up my code into separate commits. The first commit is the original Side Scroller C++ template, another commit illustrates the minimum necessary code needed to implement a sprint ability. You can look at the commit log and see all of the changes made quite easily. Time Caveats One thing that the UE4 character movement component does is regularly reset client timestamps to maintain a higher accuracy. The function responsible for updating these timestamps is part of the network prediction class that we’ve overridden and is called UpdateTimeStampAndDeltaTime which is called to by a CharacterMovement component function of the same name. I noticed that the UT code adjusts all of the saved move movement timers whenever client time is adjusted in this manner. However, the UE4 codebase does not. Looking at the CharacterMovementComponent , it doesn’t adjust any timers as a result of a client time change. JumpKeyHoldTime is an example of a timer that is stuffed into the saved moves list and not adjusted when a client timestamp is reset. I also want to call out that in UT code, depending on whether you are client or server, different values are used to represent the current world time (see GetCurrentMovementTime() ). The two choices are: A variable called CurrentServerMoveTime CharacterOwner->GetWorld()->GetTimeSeconds() If you trace who sets CurrentServerMoveTime and what it is set to you’ll eventually find that it either ends up taking the client timestamp from the network prediction data or GetWorld()->GetTimeSeconds() . TLDR; I didn’t implement the timer corrections seen in UT because I don’t yet fully understand the significance. It seems to me that the delta between time corrections would be sub-millisecond. This area requires more research and probably some debug logging in UT to really nail down. Testing Replays To trigger network replays, I found it helpful to introduce some packet loss into the simulation. This article explains the flags you need to do so. I used the following console commands to simulate 500 ms (+/- 100 ms) with 15% packet loss: p.NetShowCorrections 1 Net PktLag=500 Net PktLagVariance=100 Net PktLoss=15 Conclusion Character based movement replication and ability replay is, well, kind of involved! Hopefully you get a better start than I did :) "},{"title":"Mr. Robot Episode 1 Analysis","baseurl":"","url":"/2015/08/03/mr/robot/episode/1/analysis","date":"2015-08-03 00:00:00 -0700","categories":["tv"],"body":"Mr. Robot was recommended to me and I finally got around to checking out the pilot episode. I dig the show, I really dig that they made an effort to use real stuff for the computer scenes. As I was watching the screens fly by I was thinking “wow, I think they got it right, this is the real deal!”. I thought it would be interesting to freeze frame some of these scenes and take a closer look at what’s going on, will I come away more or less impressed? Let’s find out! But before I launch into this I want to say that I know how difficult it is making creative content and entertainment. Regardless of what I find, I’ll thumbs up the content creators as I think they did a good enough job pulling the wool over my eyes in the heat of the moment. Frame A root@elliot I’m about to read a lot into this little prefix: The format for this prefix is username@machine_name . First off, he’s running as root… this could be a clue that he’s a fast and loose kind of guy that prefers root to sudo. Does he really need to be root? Perhaps! You see, we know that Elliot is running ping to find some machine on the network, it’s possible that he regularly uses various ping switches that would require root: Flood ping Wait interval < 0.2 seconds Preload > 3 We also see that he named his computer after himself, interesting! Maybe this could be an insight into his frequent conversations with himself, perhaps the voice in his head is personified by his computer. Setting your machine name to your own first name seems a little kindergarten at first glance, especially for a bright security hacker, but I could be giving too much street cred to cool and arguably less obvious computer names. I can imagine that if I asked Elliot what’s up with his lame computer name he’d probably reply that if someone got his computer, it’s already too late, it doesn’t matter what the name is. The Prefix Problem Honestly, the ping command output is where I start to notice things falling apart a bit. Ping is used to locate machines on a network if you didn’t know already. Here’s what an actual ping command looks like on linux or bsd: root@computer:~$ ping -c 1 127.0.0.1 PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data. 64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.087 ms --- 127.0.0.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.087/0.087/0.087/0.000 ms root@computer:~$ What’s the difference? In the screenshot above, they’re replicating the user prompt ( root@elliot ) onto every line of output. On normal boxes, the user prompt is not displayed when a program prints output. I don’t know of a way to replicate this output without piping every command to some other command, I also don’t think most people would want their prompt appended to their output. Please tell me if you’ve seen this in the real world, I’d really like to know! Ping Most people run ping and then hit Ctrl + C to stop it once they’re satisfied, however since we never see a ^C appear on Elliot’s prompt, we must assume that he is invoking ping directly with the parameter -n 1 . Also, the ping statistics section is totally missing and there isn’t an invocation I could find that would squelch that output. Perhaps Elliot was just sick and tired of ping -q (supposedly the quiet mode) spitting out a bunch of extra lines and so one night while rolling on his pain-killers he compiled a personal version of ping with the brevity he desired. Moving beyond the issues, we see that Elliot has found the machine he’s looking for, what’s next? elpscrk -list pswList.list-add Dylan; June 3rd, Stonehenge My guess is that this fictional program stands for E lliot’s L eet P a S sword CR ac K er. The first invocation is cut off but reads: elpscrk -list pswList.list-add Dylan; June 3rd, Stonehenge The intent seems clear, Elliot is trying to crack a password and is adding some dates and words to help his dictionary attack. Here’s my problem with this, the semicolon… in linux, the semicolon is literally the end of command character, so this command would be split into two commands: elpscrk -list pswList.list-add Dylan June 3rd, Stonehenge Last I checked there is no program called June :/ Moving on, I’m guessing that the the list count of 9,875,894 is the number of permutations that will be attempted. It seems like a small number considering the password length is 10 characters. It doesn’t say Permutation Count, so it could be that each list is a unique dictionary by itself… that would be a lot of dictionaries! The 2nd invocation of elpscrk has Elliot specifying the IP address of the machine that he pinged earlier. He’s also specifying what looks like a user mich05654. What machine is this exactly? Is he trying to crack the password by attempting to login to this server with user mich05654? Typically there are timeouts that make this difficult. Maybe this is a server at his work and maybe the password he’s hacking is his co-workers. If so, interesting naming convention you got for your employees! Hmm, this will remain a mystery. Frame B Map sectionus34567 This is a fictional command and also highly suspicious as it uses a capital M. It’s really unclear what it would do. One guess is that it’s a command that loads a specific set of environment variables, similar to something like: source build/envsetup.sh Heck let’s go with it. The M is probably capital so that it doesn’t conflict with the other map command that maps to and from unicode. The parameter sectionus34567 is the section of the data center that we’re interested in… I’ll admit that sectionus34567 is a very unwieldly name (better would be han or chewy). But once decoded, it makes perfect sense: Section US, Row 34, Column 56, Cage 7 That’s one hell of a big data center, we would have to infer from the above that there are 10 cages per grid unit. Locate server WBKUW300PS345672 Again with the capital letter commands! Clearly this is so the Locate command doesn’t interfere with the existing lowercase locate command (the one that helps you find files). I won’t try to decode the first part of this machine naming scheme other than to say that it appears the server location is also reflected in his name, with 1 additional detail. 345672 CLEARLY indicates Row 34, Column 56, Cage 7, Blade Chassis 2 . What’s interesting is that after running the locate command, the command prompt changes to indicate that Elliot is now either connected to machine WBKUW300PS345672 or perhaps has more environment variables setup so that future alias’s will use this machine. It’s entirely possible/probable that the data center is using key-based passwordless login to ssh between various local machines. So the Locate command could have fired of an ssh in the background and you know, suppressed a bunch of the normal login output. astsu - info -backup -short The astsu command is another fictional command that had me puzzled for awhile. The space between the first dash and the word info is what made me pause. I thought at first that it could be a custom sudo command, similar to doing “ sudo su - ”. But there are other invocations where they do `astsu -close” so I had to throw that theory away. The author did some pretty amazing parsing of input parameters. My guess is that this is another utility that Elliot wrote. The command probably stands for A S erver s T atu S U tility. The commands seem pretty obvious so far: info - gives you info on the server backup - prints the backup server short - keeps the details short The output isn’t anything special. It’s an interesting design decision to force the user to specify the full -backup (probably -b or –backup would be more realistic). The more I think about this the more it seems out of character. Here’s a programmer that has spent an absurdly huge amount of time parsing input parameters to allow for spaces between dashes and a disproportional amount of time on the output layer where they abbreviate backup to bkup . This is highly suspect when the words server and backup are both 6 characters long and would align nicely in a printed output with a fixed width font. Let’s be honest, so far Elliot seems like a lazy coder with little to no stylish sensibility, if I had to guess, he wrote the input layer while high and the output layer the day after his anti-addiction drugs ran out. Frame C This frame has a few different invocations of astsu . astsu -close port: * -persistent We really see the power of astsu here as Elliot shuts down all the ports on the server. I wonder if astsu just turns around and uses iptables? astsu - ifconfig - disable More magical spaces! At least ifconfig is something that we’re all familiar with. Here it looks like Elliot is disabling the ethernet interfaces. It makes you wonder why he closed the ports first if he was just going to yank the whole rug out from under things. This could be a best practice as there’s a slight hiccup bringing interfaces back up later on. Locate BKUW300PS345672 Here Elliot switches to the backup server. He runs his usual astsu wank to get details on the server. This one comes back as offline and has no default gateway! It also has a domain controller… set waneth0* : * 23.234.45.1 255.255.255.0 [45.85.123.10; 45.85.124.10; 45.85.125.10] This command appears to be bringing up all of the ethernet devices that were brought down earlier. Elliot is specifying the default gateway explicitly (23.234.45.1) as well as the 3 DNS servers in square brackets. All of the ethernet interfaces come up except for waneth04 which says failed , so Elliot man handles it: set -force -ovr02 waneth04 : 23.234.45.62:441 23.234.45.1 255.255.255.0 [45.85.123.10; 45.85.124.10; Right, the good ole -force -ovr02 trick, seems to have worked. Tuck that one in your pocket sys admins. astsu -open port: * -persistent Elliot opens up all the ports. Frame D ps aux|grep root Hey hey! This looks legit…. at first… except that there should be about several dozen other processes on a normally running system. I’m gonna let it slide and move right to the bigger issue here, is Elliot really an “ aux ” type of guy instead of an “ -ef ” type of guy? Can we infer from this that Elliot cut his teeth on the BSD side of things rather than POSIX? Hmm, deep thoughts here, very deep thoughts. This output is interesting, clearly we see the evilscorpwb* processes, those probably shouldn’t be running. But what’s going on with PID 24? cpuset ./01dat. Also to note is the -20 on the command line, I’m guessing this is a nice level since those range from -20 to 20. What nice does is change the priority of your process. So here we see that someone has given their process maximum priority! Also cpuset is a utility that allows you to assign an application to a specific set of CPUs. This could be the intent here but it’s hard to say. astu trace -pid 244 -cmd Lots of issues here. First of all, what the heck is astu , did Elliot mean to type astsu ? Does Elliot have yet another custom program called astu? Wow, thanks for the great naming convention Elliot. Furthermore, there is no pid 244 in the output, does he mean pid 24 (the suspiciously niced cpuset command). Presumably what Elliot meant to type was: astsu trace -pid 24 -cmd astsu already does everything else, may as well add an strace like feature too. We see an inconspicuous “trace placed” message after this and presumably Elliot watches the fopen calls fly by to find where on the filesystem this process is reading/writing. Frame E ps aux| grep root|cpuset Well, looks like cpuset is another piece of custom work, because you can’t just pipe stuff to it. Perhaps the intent was to also grep for cpuset . Regardless, a process was returned with PID 4, it has a nice level of -20 so I’m guessing this is the cpuset we saw in Frame D. astu -ls ./root/fsociety/ -a Presumably a typo that was meant to be astsu . Here Elliot lists the contents of the root/fsociety directory. Frame F The file listing comes back followed by arguably the single most disappointing command in this entire series. more readme.txt Elliot uses more ? Whaaaaat? I would have pegged him for a less kind of guy. Let’s talk about the contents of readme.txt. This was Elliot’s immediate reaction. His reaction was not to the message itself, but to the sloppy asymmetric header that was missing a dash on the left side. Was this intentional? Did they know that this would bother Elliot? Frame G sudo kill 4 Elliot kills the setcpu process that was first PID 24, typed as PID 244 and then mysteriously changed to PID 4. astu -rm -norecycle /root/ fsociety/ Whoah whoah whoah there tiger. First of all, the dreaded astu command returns, we may need to refactor our entire theory on astsu and its many functions. The willy nilly Elliot is deleting some files and skipping the recycle bin. Unfortunately he didn’t escape his space and is lucky he canceled the delete otherwise he would have blown away his entire root directory. chmod -R ER280652 600 Elliot does a recursive change mod to presumably change the permissions of the fsociety folder so that only the owner can read/write it. The ER280652 is presumably a user name or group. Unfortunately chmod doesn’t let you specify user/group, you use chown for that and chown doesn’t let you specify a new permission, you use chmod for that. This is a combination of both, so I’m guessing that Elliot wrote his own chmod . Busy guy! Conclusion This was fun, but the ping-ponging between astsu and astu has me a bit bewildered. I’d like to see someone do an in-depth analysis of the two tools and what their individual functions are. "},{"title":"The 5 Golden Rules of Input","baseurl":"","url":"/2015/10/18/5/golden/rules/of/input","date":"2015-10-18 00:00:00 -0700","categories":["game-theory","gamedev"],"body":"Imagine for a moment that you are out car shopping. As you reach out to open a car door you realize there’s no handle?!? The salesman says “oh yeah, you have to crawl through the window in this one”… Doors that open, this is a car feature that everyone has come to expect in consumer vehicles. Similarly, there is a base feature set that players are going to expect from your game. If you don’t hit this base feature set, players could exit your game early out of frustration before they even see the cool stuff. This article will give you easy to follow rules on how to support gamepad and mouse/keyboard like a world boss. To do this we’re gonna do 3 things, first we’re going to talk about the high-level logic of detecting which input device the player is using. Next we’re going to cover the 5 rules that will provide a great player input experience. Finally, we’re going to see how a few AAA games hold up against these rules. This article is primarily aimed at single player PC games that support gamepads, so if you’re doing couch co-op or splitscreen you will need to take this with a grain of common sense. The Input Selection Engine Let’s state the problem we’re trying to solve. The goal is to support both gamepads and mouse/keyboard control in our game. The player should be able to control the game using any input device by simply using that input device. This means no getting off the couch or hunting through menus to toggle a gamepad option. To achieve this goal, we need an engine (a piece of code that runs in a loop) that will tell us which input device the player wants to use. The engine logic can be broken down by answering two simple questions: Does the player want to use the gamepad? Does the player want to use the mouse and keyboard? Does the Player want to use the Gamepad? Consider this chart, where the green check mark is a 100% indicator, the red X is a 0% indicator and the yellow exclamation is somewhere in-between. Remember that many people have hard-wired gamepads that are always connected to the PC. So seeing a detected gamepad isn’t a conclusive indicator of the player’s desire to use a gamepad. The reason that the analog stick is not a definite yes is that it’s possible that the player has a floaty analog stick. So while you might completely ignore this and just treat it as an absolute, it’s a good idea to define an analog noise floor and ignore values below it. Your engine might be doing this already. Does the Player want to use the Keyboard/Mouse? Consider this chart. The mouse movement falls into the same cautionary category as analog joystick movement. It’s a good indicator that the player wants to use the mouse but there’s also a small chance of jitter (like when a cat hair gets stuck under the laser). The easy solution is setting a minimum distance threshold for mouse movement. Engine Logic Based on the charts above, we can write an engine that toggles between gamepad and keyboard/mouse control on the fly. This will always be running in the background and will automatically switch to the preferred input device without missing a beat. I call this the Timestamp Input Engine. The main idea is that you store 2 timestamps that indicate the time when the last input event was received for both gamepad and keyboard/mouse. Every time you detect a valid input event, you update the appropriate timestamp variable with the current game time. Make sure to use elapsed game time, not delta-time or a time that might get paused or dilated. Now that you have timestamps indicating the last gamepad and keyboard events, the actual input detection is as easy as seeing which timestamp is greater. The 5 Rules Now that our engine is defined, let’s focus on satisfying what I’m calling The 5 Golden Rules of Input. I feel that these rules are key to allowing the player to have a non-frustrating single player experience. Rule 1 - Icons Match the Input Device Whenever the input device is changed, all on-screen button icons should change to match the corresponding device. You can see an example of this in geometry wars by popping the battery out of your gamepad. Rule 2 - Mouse Cursor Matches the Input Device This should go without saying that if the player is using a gamepad, they don’t want a mouse cursor in the middle of the screen, you know, just sitting there ruining their life. If they are using a mouse then they probably want to see the mouse cursor. With Steam machines and Steam big picture mode, many players with gamepads are on a couch. Don’t make them stand up and walk over to the PC to drag the mouse cursor to the bottom of the screen. Also, when you hide the mouse, make sure that you aren’t just changing the visibility of the cursor but are also disabling the ability to generate selection/hover events in menus. Rule 3 - All Devices Work 100% of the Time At all times, the player must be able to navigate all menus and control the game with any connected input device. And without explicitly selecting the device in a menu. Rule 4 - DPAD, Analog Stick & Mouse can Navigate Menus The player must be able to navigate menus using the mouse and both the DPAD and analog stick. Not only is this a good player experience but it will also allow you to market your game as having Full Controller Support on Steam. Rule 5 - A Disconnected Gamepad Pauses the Game When the player is using a gamepad and the gamepad gets disconnected, the game should pause. Probably the only caveat to this is if the player is in a menu, in which case the input device should simply toggle. Be sure that the pause screen follows Rules 1-4. Child of Light is a good example of this in action, you can see them following Rules 1 & 5 here. How Games Measure Up Now it’s time to check out a few of the gamepad enabled games in my steam account and see how they measure up to these 5 rules. Geometry Wars 3 The main issue with GW3 is a design issue that many games suffer from. The input detection engine pretty much works like this: The result is that icons never match the input device if you’ve got a hard-wired gamepad and aren’t using it. Another interesting design decision was their mouse timer, even if you are using keyboard/mouse and have the mouse button held down, the mouse cursor will always vanish after 5 seconds of zero mouse travel. The menu system in GW3 is clearly designed for a gamepad and it does support both analog and dpad for navigation but does not support mouse. The game pauses when the gamepad connection is lost, however it does this even when you’ve never even touched the gamepad. Most of these issues could be resolved just by making the input detection engine a bit smarter. Pacman CE DX+ Pacman uses the same input detection scheme as GW3, so icons never match unless you unplug your gamepad. The mouse cursor is never hidden, even with a gamepad detected. In a game where every little twitch matters, it’s punishing that it does not pause when the gamepad gets unplugged. Tomb Raider The input detection engine doesn’t trigger off of mouse movement, it requires a physical button press. The game does not pause when gamepad connection is lost. Child of Light This is a good example of input done right! Bioshock Infinite Great input detection, they trigger off of mouse movement! Unfortunately no pause on gamepad disconnect. Summary That’s all I’ve got, a high-level input detection engine based on timestamps and 5 simple rules for input. Again, this is targeted at single player games, so keep in mind that if your game supports couch co-op, you’ll probably be deliberately breaking a few of these. If you have any questions or comments please leave them below. "}]}